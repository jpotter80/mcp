{"chunk_id": "layout-tensors-000", "document_id": "layout-tensors", "content": "A [`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor/)\nprovides a view of multi-dimensional data stored in a linear\narray. `LayoutTensor` abstracts the logical organization of multi-dimensional\ndata from its actual arrangement in memory. You can generate new tensor \"views\"\nof the same data without copying the underlying data.\nThis facilitates essential patterns for writing performant computational\nalgorithms, such as:\n\n- Extracting tiles (sub-tensors) from existing tensors. This is especially\n valuable on the GPU, allowing a thread block to load a tile into shared\n memory, for faster access and more efficient caching.\n- Vectorizing tensors—reorganizing them into multi-element vectors for more\n performant memory loads and stores.\n- Partitioning a tensor into thread-local fragments to distribute work across a\n thread block.\n\n`LayoutTensor` is especially valuable for writing GPU kernels, and a number of\nits APIs are GPU-specific. However, `LayoutTensor` can also be used for\nCPU-based algorithms.\n\nA `LayoutTensor` consists of three main properties:", "position": 0, "token_count": 220, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-000", "document_id": "layout-tensors", "position": 0, "token_count": 220, "has_code": false, "overlap_with_previous": false, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
{"chunk_id": "layout-tensors-001", "document_id": "layout-tensors", "content": "`LayoutTensor` is especially valuable for writing GPU kernels, and a number of\nits APIs are GPU-specific. However, `LayoutTensor` can also be used for\nCPU-based algorithms.\n\nA `LayoutTensor` consists of three main properties:\n\n* A [layout](/mojo/manual/layout/layouts), defining how the elements are\n laid out in memory.\n* A `DType`, defining the data type stored in the tensor.\n* A pointer to memory where the data is stored.\n\nFigure 1 shows the relationship between the layout and the storage.\n\n<figure>\n\n![](../images/layout/tensors/layout-tensor-indexing-simple.png#light)\n![](../images/layout/tensors/layout-tensor-indexing-simple-dark.png#dark)\n\n<figcaption><b>Figure 1.</b> Layout and storage for a 2D tensor</figcaption>\n\n</figure>\n\nFigure 1 shows a 2D column-major layout, and the corresponding linear array of\nstorage. The values shown inside the layout are offsets into the storage: so the\ncoordinates (0, 1) correspond to offset 2 in the storage.", "position": 1, "token_count": 228, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-001", "document_id": "layout-tensors", "position": 1, "token_count": 228, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
{"chunk_id": "layout-tensors-002", "document_id": "layout-tensors", "content": "Because `LayoutTensor` is a view, creating a new tensor based on an existing\ntensor doesn't require copying the underlying data. So you can easily create a\nnew view, representing a tile (sub-tensor), or accessing the elements in a\ndifferent order. These views all access the same data, so changing the stored\ndata in one view changes the data seen by all of the views.\n\nEach element in a tensor can be either a single (scalar) value or a SIMD vector\nof values. For a vectorized layout, you can specify an *element layout* that\ndetermines how the vector elements are laid out in memory. For more information,\nsee [Vectorizing tensors](#vectorizing-tensors).\n\n## Ac", "position": 2, "token_count": 151, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-002", "document_id": "layout-tensors", "position": 2, "token_count": 151, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
{"chunk_id": "layout-tensors-003", "document_id": "layout-tensors", "content": "cessing tensor elements\n\nFor tensors with simple row-major or column-major layouts, you can address a\nlayout tensor like a multidimensional array to access elements:\n\n```mojo\nelement = tensor2d[x, y]\ntensor2d[x, y] = z\n```\n\nThe number of indices passed to the subscript operator must match the number of\ncoordinates required by the tensor. For simple layouts, this is the same as the\nlayout's *rank*: two for a 2D tensor, three for a 3D tensor, and so on. If the\nnumber of indices is incorrect, you may see a cryptic runtime error.\n\n```mojo\n# Ind", "position": 3, "token_count": 138, "has_code": true, "section_hierarchy": ["Accessing tensor elements"], "metadata": {"chunk_id": "layout-tensors-003", "document_id": "layout-tensors", "position": 3, "token_count": 138, "has_code": true, "overlap_with_previous": false, "section_hierarchy": ["Accessing tensor elements"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#accessing-tensor-elements"}}
{"chunk_id": "layout-tensors-004", "document_id": "layout-tensors", "content": "exing into a 2D tensor requires two indices\nel1 = tensor2d[x, y] # Works\nel2 = tensor2d[x] # Runtime error\n```\n\nFor more complicated \"nested\" layouts, such as tiled layouts, the\nnumber of indices **doesn't** match the rank of the tensor. For details, see\n[Tensor indexing and nested layouts](#tensor-indexing-and-nested-layouts).\n\nThe `__getitem__()` method returns a SIMD vector of elements, and the compiler\ncan't statically determine the size of the vector (which is the size of the\ntensor's element layout). This can cause type checking errors at compile time,\nbecause some APIs can only accept scalar values (SIMD vectors of width 1). For\nexample, consider the following code:\n\n```mojo\ni: Int = SIMD[DType.int32, width](15)\n```\n\nIf `width` is 1, the vector can be implicitly converted to an `Int`, but if\n`width` is any other value, the vector can't be implicitly converted. If `width`\nisn't known at compile time, this produces an error.", "position": 4, "token_count": 241, "has_code": true, "section_hierarchy": ["Indexing into a 2D tensor requires two indices"], "metadata": {"chunk_id": "layout-tensors-004", "document_id": "layout-tensors", "position": 4, "token_count": 241, "has_code": true, "overlap_with_previous": false, "section_hierarchy": ["Indexing into a 2D tensor requires two indices"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#indexing-into-a-2d-tensor-requires-two-indices"}}
{"chunk_id": "layout-tensors-005", "document_id": "layout-tensors", "content": "If your tensor stores scalar values, you can work around this by explicitly\ntaking the first item in the vector:\n\n```mojo\nelement = tensor[x, y][0] # element is guaranteed to be a scalar value\n```\n\nYou can also access elements using the `load()` and `store()` methods, which\nlet you specify the vector width explicitly:\n\n```mojo\nvar elements: SIMD[DType.float32, 4]\nelements = tensor.load[4](x, y)\nelements = elements * 2\ntensor.store(x, y, elements)\n```\n\n### Tens", "position": 5, "token_count": 121, "has_code": true, "section_hierarchy": ["Indexing into a 2D tensor requires two indices"], "metadata": {"chunk_id": "layout-tensors-005", "document_id": "layout-tensors", "position": 5, "token_count": 121, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Indexing into a 2D tensor requires two indices"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#indexing-into-a-2d-tensor-requires-two-indices"}}
{"chunk_id": "layout-tensors-006", "document_id": "layout-tensors", "content": "or indexing and nested layouts\n\nA tensor's layout may have nested modes (or sub-layouts), as described in\n[Introduction to layouts](/mojo/manual/layout/layouts#modes). These layouts have\none or more of their dimensions divided into sub-layouts. For example, Figure 2\nshows a tensor with a nested layout:\n\n<figure>\n\n![](../images/layout/tensors/layout-tensor-indexing-nested.png#light)\n![](../images/layout/tensors/layout-tensor-indexing-nested-dark.png#dark)\n\n<figcaption><b>Figure 2.</b> Tensor with nested layout</figcaption>\n\n</figure>\n\nFigure 2 shows a tensor with a tile-major nested layout. Instead of being\naddressed with a single coordinate on each axis, it has a pair of coordinates\nper axis. For example, the coordinates `((1, 0), (0, 1))` map to the offset 6.\n\nYou can't pass nested coordinates to the subscript operator `[]`, but you can\npass a flattened version of the coordinates. For example:\n\n```mojo\n# retrie", "position": 6, "token_count": 227, "has_code": true, "section_hierarchy": ["Indexing into a 2D tensor requires two indices", "Tensor indexing and nested layouts"], "metadata": {"chunk_id": "layout-tensors-006", "document_id": "layout-tensors", "position": 6, "token_count": 227, "has_code": true, "overlap_with_previous": false, "section_hierarchy": ["Indexing into a 2D tensor requires two indices", "Tensor indexing and nested layouts"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#tensor-indexing-and-nested-layouts"}}
{"chunk_id": "layout-tensors-007", "document_id": "layout-tensors", "content": "ve the value at ((1, 0), (0, 1))\nelement = nested_tensor[1, 0, 0, 1][0]\n```\n\nThe number of indices passed to the subscript operator must match the *flattened\nrank* of the tensor.\n\nYou can't currently use the `load()` and `store()` methods for tensors with\nnested layouts. However, these methods are usually used on tensors that have\nbeen [*tiled*](#tiling-tensors), which yields a tensor with a simple layout.\n\n## Creat", "position": 7, "token_count": 115, "has_code": true, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))"], "metadata": {"chunk_id": "layout-tensors-007", "document_id": "layout-tensors", "position": 7, "token_count": 115, "has_code": true, "overlap_with_previous": false, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#retrieve-the-value-at-1-0-0-1"}}
{"chunk_id": "layout-tensors-008", "document_id": "layout-tensors", "content": "ing a LayoutTensor\n\nThere are several ways to create a `LayoutTensor`, depending on where the tensor\ndata resides:\n\n* On the CPU.\n* In GPU global memory.\n* In GPU shared or local memory.\n\nIn addition to methods for creating a tensor from scratch, `LayoutTensor`\nprovides a number of methods for producing a new view of an existing tensor.\n\nnote No bounds checking\n\nThe `LayoutTensor` constructors don't do any bounds-checking to verify\nthat the allocated memory is large enough to hold all of the elements specified\nin the layout. It's up to the user to ensure that the proper amount of space is\nallocated.\n\n### Creating a `", "position": 8, "token_count": 137, "has_code": false, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))", "Creating a LayoutTensor"], "metadata": {"chunk_id": "layout-tensors-008", "document_id": "layout-tensors", "position": 8, "token_count": 137, "has_code": false, "overlap_with_previous": false, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))", "Creating a LayoutTensor"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#creating-a-layouttensor"}}
{"chunk_id": "layout-tensors-009", "document_id": "layout-tensors", "content": "LayoutTensor` on the CPU\n\nWhile `LayoutTensor` is often used on the GPU, you can also use it to create\ntensors for use on the CPU.\n\nTo create a `LayoutTensor` for use on the CPU, you need a `Layout` and a block of\nmemory to store the tensor data. A common way to allocate memory for a\n`LayoutTensor` is to use an\n[`InlineArray`](/mojo/stdlib/collections/inline_array/InlineArray/):\n\n```mojo\nalias rows = 8\nalias columns = 16\nalias layout = Layout.row_major(rows, columns)\nvar storage = InlineArray[Float32, rows * columns](uninitialized=True)\nvar tensor = LayoutTensor[DType.float32, layout](storage).fill(0)\n```\n\n`InlineArray` is a statically-sized, stack-allocated array, so it's a fast and\nefficient way to allocate storage for most kinds of `LayoutTensor`. There are\ntarget-dependent limits on how much memory can be allocated this way, however.\n\nYou can also create a `LayoutTensor` using an `UnsafePointer`. This may be\npreferable for very large tensors.", "position": 9, "token_count": 244, "has_code": true, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))", "Creating a LayoutTensor", "Creating a `LayoutTensor` on the CPU"], "metadata": {"chunk_id": "layout-tensors-009", "document_id": "layout-tensors", "position": 9, "token_count": 244, "has_code": true, "overlap_with_previous": false, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))", "Creating a LayoutTensor", "Creating a `LayoutTensor` on the CPU"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#creating-a-layouttensor-on-the-cpu"}}
{"chunk_id": "layout-tensors-010", "document_id": "layout-tensors", "content": "You can also create a `LayoutTensor` using an `UnsafePointer`. This may be\npreferable for very large tensors.\n\n```mojo\nalias rows = 1024\nalias columns = 1024\nalias buf_size = rows * columns\nalias layout = Layout.row_major(rows, columns)\nvar ptr = UnsafePointer[Float32].alloc(buf_size)\nmemset(ptr, 0, buf_size)\nvar tensor = LayoutTensor[DType.float32, layout](storage)\n```\n\nNote that this example uses [`memset()`](/mojo/stdlib/memory/memory/memset/)\ninstead of the `LayoutTensor` `fill()` method. The `fill()` method performs\nelementwise initialization of the tensor, so it may be slow for large tensors.\n\n### Creating a `Lay", "position": 10, "token_count": 161, "has_code": true, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))", "Creating a LayoutTensor", "Creating a `LayoutTensor` on the CPU"], "metadata": {"chunk_id": "layout-tensors-010", "document_id": "layout-tensors", "position": 10, "token_count": 161, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))", "Creating a LayoutTensor", "Creating a `LayoutTensor` on the CPU"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#creating-a-layouttensor-on-the-cpu"}}
{"chunk_id": "layout-tensors-011", "document_id": "layout-tensors", "content": "outTensor` on the GPU\n\nWhen creating a `LayoutTensor` for use on the GPU, you need to consider which\nmemory space the tensor data will be stored in:\n\n* GPU global memory can only be allocated from the host (CPU), as a\n `DeviceBuffer`.\n* GPU shared or local memory can be statically allocated on the GPU.\n\n#### Creating a `Lay", "position": 11, "token_count": 77, "has_code": false, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))", "Creating a LayoutTensor", "Creating a `LayoutTensor` on the GPU"], "metadata": {"chunk_id": "layout-tensors-011", "document_id": "layout-tensors", "position": 11, "token_count": 77, "has_code": false, "overlap_with_previous": false, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))", "Creating a LayoutTensor", "Creating a `LayoutTensor` on the GPU"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#creating-a-layouttensor-on-the-gpu"}}
{"chunk_id": "layout-tensors-012", "document_id": "layout-tensors", "content": "outTensor` in global memory\n\nYou must allocate global memory from the host side, by allocating a\n[`DeviceBuffer`](/mojo/stdlib/gpu/host/device_context/DeviceBuffer/). You can\neither construct a `LayoutTensor` using this memory on the host side, before\ninvoking a GPU kernel, or you can construct a LayoutTensor inside the kernel\nitself:\n\n* On the CPU, you can construct a `LayoutTensor` using a `DeviceBuffer` as its\n storage. Although you can create this tensor on the CPU and pass it in to a\n kernel function, you can't directly modify its values on the CPU, since the\n memory is on the GPU.\n* On the GPU: When a `DeviceBuffer` is passed in to `enqueue_function()`, the\n kernel receives a corresponding `UnsafePointer` in place of the\n `DeviceBuffer.` The kernel can then create a `LayoutTensor` using the\n `pointer`.", "position": 12, "token_count": 198, "has_code": false, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))", "Creating a LayoutTensor", "Creating a `LayoutTensor` on the GPU", "Creating a `LayoutTensor` in global memory"], "metadata": {"chunk_id": "layout-tensors-012", "document_id": "layout-tensors", "position": 12, "token_count": 198, "has_code": false, "overlap_with_previous": false, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))", "Creating a LayoutTensor", "Creating a `LayoutTensor` on the GPU", "Creating a `LayoutTensor` in global memory"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#creating-a-layouttensor-in-global-memory"}}
{"chunk_id": "layout-tensors-013", "document_id": "layout-tensors", "content": "In both cases, if you want to initialize data for the tensor from the CPU, you\ncan call\n[`enqueue_copy()`](/mojo/stdlib/gpu/host/device_context/DeviceContext#enqueue_copy)\nor\n[`enqueue_memset()`](/mojo/stdlib/gpu/host/device_context/DeviceContext#enqueue_memset)\non the buffer prior to invoking the kernel. The following example shows\ninitializing a `LayoutTensor` from the CPU and passing it to a GPU kernel.\n\n```mojo\nfn initialize_tensor_from_cpu_example():\n alias dtype = DType.float32\n alias rows = 32\n alias cols = 8\n alias block_size = 8\n num_blocks = rows * cols // (block_size * block_size)\n alias input_layout = Layout.row_major(rows, cols)\n\n fn kernel(tensor: LayoutTensor[dtype, input_layout, MutableAnyOrigin]):\n if (global_idx.x < tensor.shape[0]() and global_idx.y < tensor.shape[1]()):\n tensor[global_idx.x, global_idx.y] = (\n tensor[global_idx.x, global_idx.y] + 1\n )", "position": 13, "token_count": 230, "has_code": true, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))", "Creating a LayoutTensor", "Creating a `LayoutTensor` on the GPU", "Creating a `LayoutTensor` in global memory"], "metadata": {"chunk_id": "layout-tensors-013", "document_id": "layout-tensors", "position": 13, "token_count": 230, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))", "Creating a LayoutTensor", "Creating a `LayoutTensor` on the GPU", "Creating a `LayoutTensor` in global memory"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#creating-a-layouttensor-in-global-memory"}}
{"chunk_id": "layout-tensors-014", "document_id": "layout-tensors", "content": " try:\n var ctx = DeviceContext()\n var host_buf = ctx.enqueue_create_host_buffer[dtype](rows * cols)\n ctx.synchronize()\n\n var expected_values = List[Scalar[dtype]](rows * cols)\n for i in range(rows * cols):\n host_buf[i] = i\n expected_values[i] = i + 1\n var dev_buf = ctx.enqueue_create_buffer[dtype](rows * cols)\n ctx.enqueue_copy(dev_buf, host_buf)\n var tensor = LayoutTensor[dtype, input_layout](dev_buf)\n\n ctx.enqueue_function[kernel](\n tensor,\n grid_dim=(num_blocks, num_blocks),\n block_dim=(block_size, block_size),\n )\n ctx.enqueue_copy(host_buf, dev_buf)\n ctx.synchronize()\n\n for i in range(rows * cols):\n if host_buf[i] != expected_values[i]:\n raise Error(\n String(\"Unexpected value \", host_buf[i], \" at position \", i)\n )\n print(\"Success\")\n except error:\n print(error)\n```\n\n#### Creating a `LayoutTensor` in shared or local memory\n\nTo create a tensor on the GPU in shared memory or local memory, use the\n`LayoutTensor.stack_allocation()` static method to create a tensor with backing\nmemory in the appropriate memory space.", "position": 14, "token_count": 247, "has_code": true, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))", "Creating a LayoutTensor", "Creating a `LayoutTensor` on the GPU", "Creating a `LayoutTensor` in global memory"], "metadata": {"chunk_id": "layout-tensors-014", "document_id": "layout-tensors", "position": 14, "token_count": 247, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))", "Creating a LayoutTensor", "Creating a `LayoutTensor` on the GPU", "Creating a `LayoutTensor` in global memory"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#creating-a-layouttensor-in-global-memory"}}
{"chunk_id": "layout-tensors-015", "document_id": "layout-tensors", "content": "n pattern\nis to copy a small tile of a larger tensor into shared memory or local memory to\nreduce memory access time.\n\n```mojo\nalias tile_layout = Layout.row_major(16, 16)\n\nvar shared_tile = LayoutTensor[\n dtype,\n tile_layout,\n MutableAnyOrigin,\n address_space = AddressSpace.SHARED,\n].stack_allocation()\n```\n\n[note]\nThe name `stack_allocation()` is misleading. It is a *static* allocation,\nmeaning the allocation is processed at compile time. The allocation is like a\nC/C++ stack allocation in that its lifetime ends when the function in which it\nwas allocated returns. This API may be subject to change in the near future.\n\nIn the case of shared memory, all threads in a thread block see the same\nallocation. For local memory, each thread gets a separate allocation.\n\n## Tiling tensors\n\nA fundamental pattern for using a layout tensor is to divide the tensor into\nsmaller tiles to achieve better data locality and cache efficiency. In a GPU\nkernel you may want to select a tile that corresponds to the size of a thread\nblock. For example, given a 2D thread block of 16x16 threads, you could use a\n16x16 til", "position": 15, "token_count": 252, "has_code": true, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))", "Creating a LayoutTensor", "Creating a `LayoutTensor` on the GPU", "Creating a `LayoutTensor` in shared or local memory"], "metadata": {"chunk_id": "layout-tensors-015", "document_id": "layout-tensors", "position": 15, "token_count": 252, "has_code": true, "overlap_with_previous": false, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))", "Creating a LayoutTensor", "Creating a `LayoutTensor` on the GPU", "Creating a `LayoutTensor` in shared or local memory"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#creating-a-layouttensor-in-shared-or-local-memory"}}
{"chunk_id": "layout-tensors-016", "document_id": "layout-tensors", "content": "e (with each thread handling one element in the tile) or a 64x16 tile\n(with each thread handling 4 elements from the tensor).\n\nTiles are most commonly 1D or 2D. For element-wise calculations, where the\noutput value for a given tensor element depends on only one input value, 1D\ntiles are easy to reason about. For calculations that involve neighboring\nelements, 2D tiles can help maintain data locality. For example, matrix\nmultiplication or 2D convolution operations usually use 2D tiles.\n\n`LayoutTensor` provides a `tile()` method for extracting a single tile. You can\nalso iterate through tiles using the `LayoutTensorIter` type.\n\nWhen tiling a tensor that isn't an exact multiple of the tile size, you can\ncreate the tensor as a *masked tensor* (with the optional `masked` parameter set\nto True). When tiling a masked tensor, the tile operations will return partial\ntiles at the edges of the tensor. These tiles will be smaller than the requested\ntile size. You can use the `tensor.dim(axis)` method to query the tile\ndimensions at runtime.\n\n### Extracting a tile", "position": 16, "token_count": 245, "has_code": false, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))", "Tiling tensors"], "metadata": {"chunk_id": "layout-tensors-016", "document_id": "layout-tensors", "position": 16, "token_count": 245, "has_code": false, "overlap_with_previous": false, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))", "Tiling tensors"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#tiling-tensors"}}
{"chunk_id": "layout-tensors-017", "document_id": "layout-tensors", "content": "mages/layout/tensors/layout-tensor-tile.png#light)\n![](../images/layout/tensors/layout-tensor-tile-dark.png#dark)\n\n<figcaption><b>Figure 3.</b> Extracting a tile from a tensor</figcaption>\n\n</figure>\n\nNote that the coordinates are specified in *tiles*.\n\nThe layout of the extracted tile depends on the layout of the parent tensor. For\nexample, if the parent tensor has a column-major layout, the extract tile has a\ncolumn-major layout.\n\nIf you're extracting a tile from a tensor with a tiled layout, the extracted\ntile must match the tile boundaries of the parent tensor. For example, if the\nparent tensor is composed of 8x8 row-major tiles, a tile size of 8x8 yields an\nextracted tile with an 8x8 row-major layout.\n\n[note]\nTrying to extract a tile that's not an even multiple of the parent tile size\nusually results in an error.\n\nIf you need to know the type of the tile (to declare a variable, for example),\nyou can use the `tile_type()` method, with the same tile size parameters. Only\nuse `tile_type()` inside the `__type_of()` operator.", "position": 17, "token_count": 254, "has_code": false, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))", "Tiling tensors", "Extracting a tile"], "metadata": {"chunk_id": "layout-tensors-017", "document_id": "layout-tensors", "position": 17, "token_count": 254, "has_code": false, "overlap_with_previous": false, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))", "Tiling tensors", "Extracting a tile"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#extracting-a-tile"}}
{"chunk_id": "layout-tensors-018", "document_id": "layout-tensors", "content": "```mojo\nalias MyTileType = __type_of(tensor.tile_type[32, 32]())\nvar my_tile: MyTileType\n```\n\n### Tiled iterators\n\nThe [`LayoutTensorIter`](/mojo/kernels/layout/layout_tensor/LayoutTensorIter/)\nstruct provides a way to iterate through a block of memory, generating a layout\ntensor for each position. There are two ways to use `LayoutTensorIter`:\n\n* Starting with a memory buffer, you can generate a series of tiles.\n* Given an existing lay", "position": 18, "token_count": 108, "has_code": true, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))", "Tiling tensors", "Extracting a tile"], "metadata": {"chunk_id": "layout-tensors-018", "document_id": "layout-tensors", "position": 18, "token_count": 108, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))", "Tiling tensors", "Extracting a tile"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#extracting-a-tile"}}
{"chunk_id": "layout-tensors-019", "document_id": "layout-tensors", "content": "out tensor, you can extract a set of tiles along a given\n axis.\n\n#### Tiling a memory buffer\n\nWhen you start with a memory buffer, `LayoutTensorIter` iterates through the\nmemory one tile at a time. This essentially treats the memory as a flat array of\ntiles.\n\n```mojo\nalias buf_size = 16\nvar storage = InlineArray[Int16, buf_size](uninitialized=True)\nfor i in range(buf_size):\n storage[i] = i\nalias tile_layout =", "position": 19, "token_count": 104, "has_code": true, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))", "Tiling tensors", "Tiled iterators"], "metadata": {"chunk_id": "layout-tensors-019", "document_id": "layout-tensors", "position": 19, "token_count": 104, "has_code": true, "overlap_with_previous": false, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))", "Tiling tensors", "Tiled iterators"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#tiled-iterators"}}
{"chunk_id": "layout-tensors-020", "document_id": "layout-tensors", "content": "Layout.row_major(2, 2)\nvar iter = LayoutTensorIter[DType.int16, tile_layout, MutableAnyOrigin](\n storage.unsafe_ptr(), buf_size\n)\n\nfor i in range(buf_size // tile_layout.size()):\n tile = iter[]\n print(tile)\n print(\" - \")\n iter += 1\n```\n\nThe iterator constructor takes all of the parameters you'd use to construct a\n`LayoutTensor`—a `DType`, layout, and an origin—and as arguments it takes a\npointer and the size of the memory buffer.\n\nNote that the iterator doesn't work like a standard iterator, and you can't use\nit directly in a `for` statement like you would use a collection. Instead, you\ncan use either the dereference operator (`iter[]`) or the `get()` method to\nretrieve a `LayoutTensor` representing the tile at the current position.\n\nYou can advance the iterator by incrementing it, as shown above. The iterator\nalso supports `next()` and `next_unsafe()` methods, which return a copy of the\niterator incremented by a specified offset (default 1). This means you can also\nuse a pattern like this:", "position": 20, "token_count": 241, "has_code": true, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))", "Tiling tensors", "Tiled iterators", "Tiling a memory buffer"], "metadata": {"chunk_id": "layout-tensors-020", "document_id": "layout-tensors", "position": 20, "token_count": 241, "has_code": true, "overlap_with_previous": false, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))", "Tiling tensors", "Tiled iterators", "Tiling a memory buffer"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#tiling-a-memory-buffer"}}
{"chunk_id": "layout-tensors-021", "document_id": "layout-tensors", "content": "```mojo\nfor i in range(num_tiles):\n current_tile = iter.next(i)[]\n …\n```\n\n`LayoutTensorIter` also has an optional boolean `circular` parameter. A\n`LayoutTensorIter `created with `circular=True` treats the memory buffer as\ncircular; when it hits the end of the buffer, it starts over again at the\nbeginning.\n\n### Tiling a `LayoutTensor`\n\nTo iterate over a tensor, call the\n[`tiled_iterator()`](/mojo/kernels/layout/layout_tensor/LayoutTensor/#tiled_iterator)\nmethod, which produces a `LayoutTensorIter`:\n\n```mojo\nfrom layout import LayoutTensor\nfrom math import ceildiv\n\n# given a tensor of size rows x cols\nalias num_row_tiles = ceildiv(rows, tile_size)\nalias num_col_tiles = cei", "position": 21, "token_count": 175, "has_code": true, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))", "Tiling tensors", "Tiled iterators", "Tiling a memory buffer"], "metadata": {"chunk_id": "layout-tensors-021", "document_id": "layout-tensors", "position": 21, "token_count": 175, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))", "Tiling tensors", "Tiled iterators", "Tiling a memory buffer"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#tiling-a-memory-buffer"}}
{"chunk_id": "layout-tensors-022", "document_id": "layout-tensors", "content": "ldiv(cols, tile_size)\n\nfor i in range(num_row_tiles):\n var iter = tensor.tiled_iterator[tile_size, tile_size, axis=1](i, 0)\n\n for _ in range(num_col_tiles):\n tile = iter[]\n # … do something with the tile\n iter += 1\n```\n\n## Vectorizing tensors\n\nWhen working with", "position": 22, "token_count": 73, "has_code": true, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))", "Tiling tensors", "Tiling a `LayoutTensor`"], "metadata": {"chunk_id": "layout-tensors-022", "document_id": "layout-tensors", "position": 22, "token_count": 73, "has_code": true, "overlap_with_previous": false, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))", "Tiling tensors", "Tiling a `LayoutTensor`"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#tiling-a-layouttensor"}}
{"chunk_id": "layout-tensors-023", "document_id": "layout-tensors", "content": "tensors, it's frequently efficient to access more than one\nvalue at a time. For example, having a single GPU thread calculate multiple\noutput values (\"thread coarsening\") can frequently improve performance.\nLikewise, when copying data from one memory space to another, it's often helpful\nfor each thread to copy a SIMD vector worth of values, instead of a", "position": 23, "token_count": 75, "has_code": false, "section_hierarchy": ["given a tensor of size rows x cols"], "metadata": {"chunk_id": "layout-tensors-023", "document_id": "layout-tensors", "position": 23, "token_count": 75, "has_code": false, "overlap_with_previous": false, "section_hierarchy": ["given a tensor of size rows x cols"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#given-a-tensor-of-size-rows-x-cols"}}
{"chunk_id": "layout-tensors-024", "document_id": "layout-tensors", "content": "single\nvalue. Many GPUs have vectorized copy instructions that can make copying more\nefficient.\n\nTo choose the optimum vector size, you need to know the vector operations\nsupported for your current GPU for the data type you're working with. (For\nexample, if you're working with 32-bit values and the GPU supports 128-bit copy\noperations, you can use a vector width of 4.) You can use the\n[`simd_width_of()`](/mojo/stdlib/sys/info/simd_width_of/) method to find the\nappropriate vector width:\n\n```mojo\nfrom sys.info import simd_width_of\nfrom gpu.host.compile import get_gpu_target\n\nalias simd_width = simd_width_of[DType.float32, get_gpu_target()]\n```\n\nThe [`vectorize()`](/mojo/kernels/layout/layout_tensor/LayoutTensor/#vectorize)\nmethod creates a new view of the tensor where each element of the tensor is a\nvector of values.\n\n`v_tensor = tensor.vectorize[1, simd_width]()`\n\nThe vectorized tensor is a view of the original tensor, pointing to the same\ndata. The underlying number of scalar values remains the same, but the tensor\nlayout and element layout changes, as shown in Figure 4.\n\n<figure>", "position": 24, "token_count": 258, "has_code": true, "section_hierarchy": ["given a tensor of size rows x cols", "Vectorizing tensors"], "metadata": {"chunk_id": "layout-tensors-024", "document_id": "layout-tensors", "position": 24, "token_count": 258, "has_code": true, "overlap_with_previous": false, "section_hierarchy": ["given a tensor of size rows x cols", "Vectorizing tensors"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#vectorizing-tensors"}}
{"chunk_id": "layout-tensors-025", "document_id": "layout-tensors", "content": "The vectorized tensor is a view of the original tensor, pointing to the same\ndata. The underlying number of scalar values remains the same, but the tensor\nlayout and element layout changes, as shown in Figure 4.\n\n<figure>\n\n![](../images/layout/tensors/vectorized-tensor.png#light)\n![](../images/layout/tensors/vectorized-tensor-dark.png#dark)\n\n<figcaption><b>Figure 4.</b> Vectorizing a tensor</figcaption>\n\n</figure>\n\n## Partitioning a tensor across threads\n\nWhen working with tensors on the GPU, it's sometimes desirable to distribute the\nelements of a tensor across the threads in a thread block. The\n[`distribute()`](/mojo/kernels/layout/layout_tensor/LayoutTensor/#distribute)\nmethod takes a thread layout and a thread ID and returns a thread-specific\n*fragment* of the tensor.\n\nThe thread layout is tiled across", "position": 25, "token_count": 186, "has_code": false, "section_hierarchy": ["given a tensor of size rows x cols", "Vectorizing tensors"], "metadata": {"chunk_id": "layout-tensors-025", "document_id": "layout-tensors", "position": 25, "token_count": 186, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["given a tensor of size rows x cols", "Vectorizing tensors"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#vectorizing-tensors"}}
{"chunk_id": "layout-tensors-026", "document_id": "layout-tensors", "content": "the tensor. The *N*th thread receives a\nfragment consisting of the *N*th value from each tile. For example, Figure 5\nshows how `distribute()` forms fragments given a 4x4, row-major tensor and a\n2x2, column-major thread layout:\n\n<figure>\n\n![](../images/layout/tensors/distribute-layout.png#light)\n![](../images/layout/tensors/distribute-layout-dark.png#dark)\n\n<figcaption><b>Figure 5.</b> Partitioning a tensor into fragments</figcaption>\n\n</figure>\n\nIn Figure 5, the numbers in the data layout represent offsets into storage, as\nusual. The numbers in the thread layout represent thread IDs.\n\nThe example in Figure 5 uses a small thread layout for illustration purposes. In\npractice, it's usually optimal to use a thread layout that's the same size as\nthe warp size of your GPU, so the work is divided across all available threads.\nFor example, the following code vectorizes and partitions a tensor over a full\nwarp worth of threads:", "position": 26, "token_count": 221, "has_code": false, "section_hierarchy": ["given a tensor of size rows x cols", "Partitioning a tensor across threads"], "metadata": {"chunk_id": "layout-tensors-026", "document_id": "layout-tensors", "position": 26, "token_count": 221, "has_code": false, "overlap_with_previous": false, "section_hierarchy": ["given a tensor of size rows x cols", "Partitioning a tensor across threads"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#partitioning-a-tensor-across-threads"}}
{"chunk_id": "layout-tensors-027", "document_id": "layout-tensors", "content": "```mojo\nalias thread_layout = Layout.row_major(WARP_SIZE // simd_size, simd_size)\nvar fragment = tile.vectorize[1, simd_size]().distribute[thread_layout](lane_id())\n```\n\nGiven a 16x16 tile size, a warp size of 32 and a `simd_size` of 4, this code\nproduces a 4x16 tensor of 1x4 vectors. The thread layout is an 8x4 row major\nlayout.\n\n## Copying tensors\n\nThe `layout-tensor` package provides a large set of utilities for copying\ntensors. A number of these are specialized for copying between various GPU\nmemory spaces. All of the layout tensor copy methods respect the layouts—so you\ncan transform a tensor by copying it to a tensor with a different layout.\n\n`LayoutTensor` itself provides two methods for copying tensor data:\n\n* [`copy", "position": 27, "token_count": 185, "has_code": true, "section_hierarchy": ["given a tensor of size rows x cols", "Partitioning a tensor across threads"], "metadata": {"chunk_id": "layout-tensors-027", "document_id": "layout-tensors", "position": 27, "token_count": 185, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["given a tensor of size rows x cols", "Partitioning a tensor across threads"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#partitioning-a-tensor-across-threads"}}
{"chunk_id": "layout-tensors-028", "document_id": "layout-tensors", "content": "_from()`](/mojo/kernels/layout/layout_tensor/LayoutTensor/#copy_from)\n copies data from a source tensor to the current tensor, which may be in a\n different memory space.\n* [`copy_from_async()`](/mojo/kernels/layout/layout_tensor/LayoutTensor/#copy_from_async)\n is an optimized copy mechanism for asynchronously copying from GPU global\n memory to shared memory.\n\nBoth of these methods copy the entire source tensor. To divide the copying work\namong multiple threads, you need to use `distribute()` to create thread-local\ntensor fragments, as described in\n[Partitioning a tensor across threads](#partitioning-a-tensor-across-threads).\n\nThe following code sample demonstrates using both copy methods to copy data to\nand from shared memory.\n\n```mojo\nfn copy_from_async_example():\n alias dtype = DType.float32\n alias in_size = 128\n alias block_size = 16\n alias num_blocks = in_size // block_size # number of block in one dimension\n alias input_layout = Layout.row_major(in_size, in_size)\n alias simd_width_gpu = simd_width_of[dtype, get_gpu_target()]()", "position": 28, "token_count": 233, "has_code": true, "section_hierarchy": ["given a tensor of size rows x cols", "Copying tensors"], "metadata": {"chunk_id": "layout-tensors-028", "document_id": "layout-tensors", "position": 28, "token_count": 233, "has_code": true, "overlap_with_previous": false, "section_hierarchy": ["given a tensor of size rows x cols", "Copying tensors"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#copying-tensors"}}
{"chunk_id": "layout-tensors-029", "document_id": "layout-tensors", "content": " fn kernel(tensor: LayoutTensor[dtype, input_layout, MutableAnyOrigin]):\n # extract a tile from the input tensor.\n var global_tile = tensor.tile[block_size, block_size](block_idx.x, block_idx.y)\n alias tile_layout = Layout.row_major(block_size, block_size)\n var shared_tile = LayoutTensor[\n dtype,\n tile_layout,\n MutableAnyOrigin,\n address_space = AddressSpace.SHARED,\n ].stack_allocation()\n\n # Create thread layouts for copying\n alias thread_layout = Layout.row_major(\n WARP_SIZE // simd_width_gpu, simd_width_gpu\n )\n var global_fragment = global_tile.vectorize[\n 1, simd_width_gpu\n ]().distribute[thread_layout](lane_id())\n var shared_fragment = shared_tile.vectorize[\n 1, simd_width_gpu\n ]().distribute[thread_layout](lane_id())\n\n shared_fragment.copy_from_async(global_fragment)\n async_copy_wait_all()\n\n # Put some data into the shared tile that we can verify on the host.\n if (global_idx.x < in_size and global_idx.y < in_size):\n shared_tile[thread_idx.x, thread_idx.y] = (\n global_idx.x * in_size + global_idx.y\n )\n\n barrier()\n global_fragment.copy_from(shared_fragment)", "position": 29, "token_count": 244, "has_code": false, "section_hierarchy": ["given a tensor of size rows x cols", "Copying tensors"], "metadata": {"chunk_id": "layout-tensors-029", "document_id": "layout-tensors", "position": 29, "token_count": 244, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["given a tensor of size rows x cols", "Copying tensors"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#copying-tensors"}}
{"chunk_id": "layout-tensors-030", "document_id": "layout-tensors", "content": " barrier()\n global_fragment.copy_from(shared_fragment)\n\n try:\n var ctx = DeviceContext()\n var host_buf = ctx.enqueue_create_host_buffer[dtype](in_size * in_size)\n var dev_buf = ctx.enqueue_create_buffer[dtype](in_size * in_size)\n ctx.enqueue_memset(dev_buf, 0.0)\n var tensor = LayoutTensor[dtype, input_layout](dev_buf)\n\n ctx.enqueue_function[kernel](\n tensor,\n grid_dim=(num_blocks, num_blocks),\n block_dim=(block_size, block_size),\n )\n ctx.enqueue_copy(host_buf, dev_buf)\n ctx.synchronize()\n for i in range(in_size * in_size):\n if host_buf[i] != i:\n raise Error(\n String(\"Unexpected value \", host_buf[i], \" at position \", i)\n )\n print(\"Success!\")\n except error:\n print(error)\n```\n\n### Thread-aware copy functions\n\nThe [`layout_tensor` package](/mojo/kernels/layout/layout_tensor/) also includes\na number of specialized copy functions for different scenarios, such as copying\nfrom shared memory to local memory. These functions are all *thread-aware*:\ninstead of passing in tensor fragments, you pass in a thread layout which the\nfunction uses to partition the work.", "position": 30, "token_count": 245, "has_code": true, "section_hierarchy": ["given a tensor of size rows x cols", "Copying tensors"], "metadata": {"chunk_id": "layout-tensors-030", "document_id": "layout-tensors", "position": 30, "token_count": 245, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["given a tensor of size rows x cols", "Copying tensors"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#copying-tensors"}}
{"chunk_id": "layout-tensors-031", "document_id": "layout-tensors", "content": "ensor` provides\na powerful abstraction for working with multi-dimensional data.\nBy combining a layout (which defines memory organization), a data type, and a\nmemory pointer, `LayoutTensor` enables flexible and efficient data manipulation\nwithout unnecessary copying of the underlying data.\n\nWe covered several essential tensor operations that form the\nfoundation of working with `LayoutTensor`, including creating tensors,\naccessing tensor elements, and copying data between tensors.\n\nWe also covered key patterns for optimizing data access:\n\n- Tiling tensors for data locality. Accessing tensors one tile at a time can\n improve cache efficiency. On the GPU, tiling can allow the threads of a\n thread block to share", "position": 31, "token_count": 135, "has_code": false, "section_hierarchy": ["given a tensor of size rows x cols", "Copying tensors", "Thread-aware copy functions"], "metadata": {"chunk_id": "layout-tensors-031", "document_id": "layout-tensors", "position": 31, "token_count": 135, "has_code": false, "overlap_with_previous": false, "section_hierarchy": ["given a tensor of size rows x cols", "Copying tensors", "Thread-aware copy functions"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#thread-aware-copy-functions"}}
{"chunk_id": "layout-tensors-032", "document_id": "layout-tensors", "content": "high-speed access to a subset of a tensor.\n- Vectorizing tensors for more efficient data loads and stores.\n- Partitioning or distributing tensors into thread-local fragments for\n processing.\n\nThese patterns provide the building blocks for writing efficient kernels in Mojo\nwhile maintaining clean, readable code.\n\nTo see some practical examples of `LayoutTensor` in use, see [Optimize custom\nops for GPUs with Mojo ](/max/tutorials/custom-ops-matmul/).", "position": 32, "token_count": 93, "has_code": false, "section_hierarchy": ["given a tensor of size rows x cols", "Summary"], "metadata": {"chunk_id": "layout-tensors-032", "document_id": "layout-tensors", "position": 32, "token_count": 93, "has_code": false, "overlap_with_previous": false, "section_hierarchy": ["given a tensor of size rows x cols", "Summary"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#summary"}}
