{"chunk_id": "gpu-intro-tutorial-000", "document_id": "gpu-intro-tutorial", "content": "This tutorial introduces you to GPU programming with Mojo. You'll learn how to\nwrite a simple program that performs vector addition on a GPU, exploring\nfundamental concepts of GPU programming along the way.\n\nBy the end of this tutorial, you will:\n\n- Understand basic GPU programming concepts like grids and thread blocks.\n- Learn how to move data between CPU and GPU memory.\n- Write and compile a simple GPU kernel function.\n- Execute parallel computations on the GPU.\n- Understand the asynchronous nature of GPU programming.\n\nWe'll build everything step-by-step, starting with the basics and gradually\nadding more complexity. The concepts you learn here will serve as a foundation\nfor more advanced GPU programming with Mojo. If you just want to see the\nfinished code, you can [get it on\nGitHub](https://github.com/modular/modular/tree/main/examples/mojo/gpu-intro).\n\n[tip]\n**Tip:** To use AI coding assistants with Mojo, see our guide for\n[using AI coding assistants](/max/coding-assistants/).\n\nSystem requirements:\n\n## 1. Create a Mojo project\n\nTo install Mojo, we recommend using [`pixi`](https://pixi.sh/latest/) (for\nother options, see the [install guide](/mojo/manual/install)).\n\n1. If you don't have `pixi`, you can install it with this command:\n\n ```s", "position": 0, "token_count": 282, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "gpu-intro-tutorial-000", "document_id": "gpu-intro-tutorial", "position": 0, "token_count": 282, "has_code": true, "overlap_with_previous": false, "section_hierarchy": [], "file_path": "gpu/intro-tutorial.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial", "title": "Get started with GPU programming", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial"}}
{"chunk_id": "gpu-intro-tutorial-001", "document_id": "gpu-intro-tutorial", "content": "h\n curl -fsSL https://pixi.sh/install.sh | sh\n ```\n\n2. Navigate to the directory in which you want to create the project\n and execute:\n\n ```bash\n pixi init gpu-intro \\\n -c https://conda.modular.com/max-nightly/ -c conda-forge \\\n && cd gpu-intro\n ```\n\n This creates a project directory named `gpu-intro`, adds the Modular conda\n package channel, and enters the directory.\n\n [tip]\n **Tip:**\n You can skip the `-c` options if you [add these channels as\n defaults](/pixi#create-a-project-and-virtual-environment).\n\n\n\n3. Install the `mojo` package:\n\n ```bash\n pixi add mojo\n ```\n\n3. Let's verify the project is configured\n correctly by checking the version of Mojo that's installed within our\n project's virtual environment:\n\n ```bash\n pixi run mojo --version\n ```\n\n You should see a version string indicating the version of Mojo installed,\n which by default should be the latest nightly version.\n\n4. Activate the project's virtual environment:\n\n ```bash\n pixi shell\n ```\n\n Later on, when you want to exit the virtual environment, just type `exit`.\n\n## 2. Get a reference to the GPU device\n\nThe [`DeviceContext`](/mojo/stdlib/gpu/host/device_context/DeviceContext/) type\nrepresents a logical instance of a GPU device. It provides methods for\nallocating memory on the device, copying data between the host CPU and the GPU,\nand compiling and running functions (also known as *kernels*) on the", "position": 1, "token_count": 333, "has_code": true, "section_hierarchy": ["1. Create a Mojo project"], "metadata": {"chunk_id": "gpu-intro-tutorial-001", "document_id": "gpu-intro-tutorial", "position": 1, "token_count": 333, "has_code": true, "overlap_with_previous": false, "section_hierarchy": ["1. Create a Mojo project"], "file_path": "gpu/intro-tutorial.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial", "title": "Get started with GPU programming", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial#1-create-a-mojo-project"}}
{"chunk_id": "gpu-intro-tutorial-002", "document_id": "gpu-intro-tutorial", "content": "device.\n\nUse the\n[`DeviceContext()`](/mojo/stdlib/gpu/host/device_context/DeviceContext#__init__)\nconstructor to get a reference to the GPU device. The constructor raises an\nerror if no compatible GPU is available. You can use the\n[`has_accelerator()`](/mojo/stdlib/sys/info/has_accelerator/) function to check\nif a compatible GPU is available.\n\nSo let's start by writing a program that checks if a GPU is available and then\nobtains a reference to the GPU device. Using any editor, create a file named\n`vector_addition.mojo` with the following code:\n\n```mojo title=\"vector_addition.mojo\"\nfrom gpu.host import DeviceContext\nfrom sys import has_accelerator\n\ndef main():\n @parameter\n if not has_accelerator():\n print(\"No compatible GPU found\")\n else:\n ctx = DeviceContext()\n print(\"Found GPU:\", ctx.name())\n```\n\nSave the file and run it using the `mojo` CLI:\n\n```bash\nmojo vector_addition.mojo\n```\n\nYou should see output like the following (depending on the type of GPU you\nhave):\n\n```output\nFound GPU: NVIDIA A10G\n```\n\n[note]\nMojo requires a [compatible GPU development\nenvironment](/max/faq/#gpu-requirements) to compile kernel functions, otherwise\nit raises a compile-time error. In our code, we're using the\n[`@parameter`](/mojo/manual/decorators/parameter) decorator to evaluate the\n`has_accelerator()` function at compile time and compile only the corresponding\nbranch of the `if` statement. As a result, if you don't have a compatible GPU\ndevelopment environment, you'll see the following message when you run the\nprogram:\n\n```output\nNo compatible GPU found\n```\n\nIn that case, you need to find a system that has a supported GPU to continue\nwith this tutorial.", "position": 2, "token_count": 400, "has_code": true, "section_hierarchy": ["2. Get a reference to the GPU device"], "metadata": {"chunk_id": "gpu-intro-tutorial-002", "document_id": "gpu-intro-tutorial", "position": 2, "token_count": 400, "has_code": true, "overlap_with_previous": false, "section_hierarchy": ["2. Get a reference to the GPU device"], "file_path": "gpu/intro-tutorial.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial", "title": "Get started with GPU programming", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial#2-get-a-reference-to-the-gpu-device"}}
{"chunk_id": "gpu-intro-tutorial-003", "document_id": "gpu-intro-tutorial", "content": "```output\nNo compatible GPU found\n```\n\nIn that case, you need to find a system that has a supported GPU to continue\nwith this tutorial.\n\n## 3. Define a simple kernel\n\nA GPU *kernel* is simply a function that runs on a GPU, executing a specific\ncomputation on a large dataset in parallel across thousands or millions of\n*threads*. You might already be familiar with threads when programming for a\nCPU, but GPU threads are different. On a CPU, threads are managed by the\noperating system and can perform completely ind", "position": 3, "token_count": 115, "has_code": true, "section_hierarchy": ["2. Get a reference to the GPU device"], "metadata": {"chunk_id": "gpu-intro-tutorial-003", "document_id": "gpu-intro-tutorial", "position": 3, "token_count": 115, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["2. Get a reference to the GPU device"], "file_path": "gpu/intro-tutorial.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial", "title": "Get started with GPU programming", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial#2-get-a-reference-to-the-gpu-device"}}
{"chunk_id": "gpu-intro-tutorial-004", "document_id": "gpu-intro-tutorial", "content": "ependent tasks, such as managing\na user interface, fetching data from a database, and so on. But on a GPU,\nthreads are managed by the GPU itself. All the threads on a GPU execute the same\nkernel function, but they each work on a different part of the data.\n\nWhen you run a kernel, you need to specify the number of threads you want to\nuse. The number of threads you specify depends on the size of the data you want\nto process and the amount of parallelism you want to achieve. A common strategy\nis to use one thread per element of data in the result. So if you're performing\nan element-wise addition of two 1,024-element vectors, you'd use 1,024 threads.\n\nA *grid* is the top-level organizational structure for the threads executing a\nkernel function. A grid consists of multiple *thread blocks*, which are further\ndivided into individual threads that execute the kernel function concurrently.\nThe GPU assigns a unique block index to each thread block, and a unique thread\nindex to each thread within a block. Threads within the same thread block can\nshare data through shared memory and synchronize using built-in mechanisms, but\nthey cannot directly communicate with threads in other blocks. For this\ntutorial, we won't get in the details of why or how to do this, but it's an\nimportant concept to keep in mind when you're writing more complex kernels.\n\nTo better understand how grids, thread blocks, and threads are organized, let's\nwrite a simple kernel function that prints the thread block and thread indices.\nAdd the following code to your `vector_addition.mojo` file:\n\n```mojo title=\"vector_addition.mojo\"\nfrom gpu.id import block_idx, thread_idx\n\nfn print_threads():\n \"\"\"Print thread IDs.\"\"\"\n\n print(\"Block index: [\",\n block_idx.x,\n \"]\\tThread index: [\",\n thread_idx.x,\n \"]\"\n )\n```", "position": 4, "token_count": 397, "has_code": true, "section_hierarchy": ["3. Define a simple kernel"], "metadata": {"chunk_id": "gpu-intro-tutorial-004", "document_id": "gpu-intro-tutorial", "position": 4, "token_count": 397, "has_code": true, "overlap_with_previous": false, "section_hierarchy": ["3. Define a simple kernel"], "file_path": "gpu/intro-tutorial.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial", "title": "Get started with GPU programming", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial#3-define-a-simple-kernel"}}
{"chunk_id": "gpu-intro-tutorial-005", "document_id": "gpu-intro-tutorial", "content": "```mojo title=\"vector_addition.mojo\"\nfrom gpu.id import block_idx, thread_idx\n\nfn print_threads():\n \"\"\"Print thread IDs.\"\"\"\n\n print(\"Block index: [\",\n block_idx.x,\n \"]\\tThread index: [\",\n thread_idx.x,\n \"]\"\n )\n```\n\n[note]\nWe're using `fn` here without the `raises` keyword because a kernel function is\nnot allowed to raise an error condition. In contrast, when you define a Mojo\nfunction with `def`, the compiler always assumes that the function *can* raise\nan error condition. See the [Functions](/mojo/manual/functions) section of the\nMojo Manual for more information on the difference between using `fn` and `def`\nto define functions in Mojo.\n\n## 4. Compile and run the kernel\n\nNext, we need to update the `main()` function to compile the kernel function for\nour GPU and then run it, specifying the number of thread blocks in the grid and\nthe number of threads per thread block. For this initial example, let's define a\ngrid consisting of 2 thread blocks, each with 64 threads. Modify the `main()`\nfunction so that your program looks like this:\n\n```mojo title=\"vector", "position": 5, "token_count": 256, "has_code": true, "section_hierarchy": ["3. Define a simple kernel"], "metadata": {"chunk_id": "gpu-intro-tutorial-005", "document_id": "gpu-intro-tutorial", "position": 5, "token_count": 256, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["3. Define a simple kernel"], "file_path": "gpu/intro-tutorial.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial", "title": "Get started with GPU programming", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial#3-define-a-simple-kernel"}}
{"chunk_id": "gpu-intro-tutorial-006", "document_id": "gpu-intro-tutorial", "content": "_addition.mojo\"\nfrom gpu.host import DeviceContext\nfrom gpu.id import block_idx, thread_idx\nfrom sys import has_accelerator\n\nfn print_threads():\n \"\"\"Print thread IDs.\"\"\"\n print(\"Block index: [\",\n block_idx.x,\n \"]\\tThread index: [\",\n thread_idx.x,\n \"]\"\n )\n\ndef main():\n @parameter\n if not has_accelerator():\n print(\"No compatible GPU found\")\n else:\n ctx = DeviceContext()\n ctx.enqueue_function[print_threads](grid_dim=2, block_dim=64)\n ctx.synchronize()\n print(\"Program finished\")\n```\n\nSave the file and run it:\n\n```bash\nmojo vector_addition.mojo\n```\n\nYou should see something like the following output (which is abbreviated here):\n\n```output\nBlock index: [ 1 ]\tThread index: [ 32 ]\nBlock index: [ 1 ]\tThread index: [ 33 ]\nBlock index: [ 1 ]\tThread index: [ 34 ]\n...\nBlock index: [ 0 ]\tThread index: [ 30 ]\nBlock index: [ 0 ]\tThread index: [ 31 ]\nProgram finished\n```\n\nTypical CPU-GPU interaction is asynchronous, allowing the GPU to process tasks\nwhile the CPU is busy with other work. Each `DeviceContext` has an associated\nstream of queued operations to execute on the GPU. Operations within a stream\nexecute in the order they are issued.\n\nThe\n[`enqueue_function()`](/mojo/stdlib/gpu/host/device_context/DeviceContext#enqueue_function)\nmethod compiles a kernel function and enqueues it to run on the given device.\nYou must provide the name of the kernel function as a compile-time Mojo\nparameter, and the following arguments:\n\n- Any additional arguments specified by the kernel function definition (none, in\n this case).\n- The grid dimensions using the `grid_dim` keyword argument.\n- The thread block dimensions using the `block_dim` keyword argument.", "position": 6, "token_count": 396, "has_code": true, "section_hierarchy": ["4. Compile and run the kernel"], "metadata": {"chunk_id": "gpu-intro-tutorial-006", "document_id": "gpu-intro-tutorial", "position": 6, "token_count": 396, "has_code": true, "overlap_with_previous": false, "section_hierarchy": ["4. Compile and run the kernel"], "file_path": "gpu/intro-tutorial.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial", "title": "Get started with GPU programming", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial#4-compile-and-run-the-kernel"}}
{"chunk_id": "gpu-intro-tutorial-007", "document_id": "gpu-intro-tutorial", "content": "- Any additional arguments specified by the kernel function definition (none, in\n this case).\n- The grid dimensions using the `grid_dim` keyword argument.\n- The thread block dimensions using the `block_dim` keyword argument.\n\n(See the [Functions](/mojo/manual/functions) section of the Mojo Manual for more\ninformation on Mojo function arguments and the\n[Parameters](/mojo/manual/parameters) section for more information on Mojo\ncompile-time parameters and metaprogramming.)\n\nWe're invoking the compiled kernel function with `grid_dim=2` and\n`block_dim=64`, which means we're using a grid of 2 thread blocks, with 64\nthreads each for a total of 128 threads in the grid.\n\nWhen you run a kernel, the GPU assigns each thread block within the grid to a\n*streaming multiprocessor* for execution. A streaming multiprocessor (SM) is the\nfundamental processing unit of a GPU, designed to execute multiple parallel\nworkloads efficiently. Each SM contains several cores, which perform the actual\ncomputations of the threads executing on the SM, along with shared resources\nlike registers, shared memory, and control mechanisms to coordinate the\nexecution of threads. The number of SMs and the number of cores on a GPU depends\non its architecture. For example, the NVIDIA H100 PCIe contains 114 SMs, with\n128 32-bit floating point cores per SM.\n\n<figure>\n\n![](../images/gpu/sm-architecture.png#light)\n![](../images/gpu/sm-architecture-dark.png#dark)\n\n<figcaption><b>Figure 1.</b> High-level architecture of a streaming\nmultiprocessor (SM). (Click to enlarge.)</figcaption>\n\n</figure>", "position": 7, "token_count": 361, "has_code": false, "section_hierarchy": ["4. Compile and run the kernel"], "metadata": {"chunk_id": "gpu-intro-tutorial-007", "document_id": "gpu-intro-tutorial", "position": 7, "token_count": 361, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["4. Compile and run the kernel"], "file_path": "gpu/intro-tutorial.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial", "title": "Get started with GPU programming", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial#4-compile-and-run-the-kernel"}}
{"chunk_id": "gpu-intro-tutorial-008", "document_id": "gpu-intro-tutorial", "content": "<figure>\n\n![](../images/gpu/sm-architecture.png#light)\n![](../images/gpu/sm-architecture-dark.png#dark)\n\n<figcaption><b>Figure 1.</b> High-level architecture of a streaming\nmultiprocessor (SM). (Click to enlarge.)</figcaption>\n\n</figure>\n\nAdditionally, when an SM is assigned a thread block, it divides the block into\nmultiple *warps*, which are groups of 32 or 64 threads, depending on the GPU\narchitecture. These threads execute the same instruction simultaneously in a\n*single instruction, multiple threads* (SIMT) model. The SM's *warp scheduler*\ncoordinates the execution of warps on an SM's cores.\n\n<figure>\n\n![](../images/gpu/grid-hierarchy.png#light)\n![](../images/gpu/grid-hierarchy-dark.png#dark)\n\n<figcaption><b>Figure 2.</b> Hierarchy of threads running on a GPU, showing the\nrelationship of the grid, thread blocks, warps, and individual threads, based\non <cite><a\nhref=\"https://rocm.docs.amd.com/projects/HIP/en/latest/understand/programming_model.html\">HIP\nProgramming Guide</a></cite></figcaption>\n\n</figure>\n\nWarps are used to efficiently utilize GPU hardware by maximizing throughput and\nminimizing control overhead. Since GPUs are designed for high-performance\nparallel processing, grouping threads into warps allows for streamlined\ninstruction scheduling and execution, reducing the complexity of managing\nindividual threads. Multiple warps from multiple thread blocks can be active\nwithin an SM at any given time, enabling the GPU to keep execution units busy.\nFor example, if the threads of a particular warp are blocked waiting for data\nfrom memory, the warp scheduler can immediately switch execution to another warp\nthat's ready to run.", "position": 8, "token_count": 379, "has_code": false, "section_hierarchy": ["4. Compile and run the kernel"], "metadata": {"chunk_id": "gpu-intro-tutorial-008", "document_id": "gpu-intro-tutorial", "position": 8, "token_count": 379, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["4. Compile and run the kernel"], "file_path": "gpu/intro-tutorial.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial", "title": "Get started with GPU programming", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial#4-compile-and-run-the-kernel"}}
{"chunk_id": "gpu-intro-tutorial-009", "document_id": "gpu-intro-tutorial", "content": "After enqueuing the kernel function, we want to ensure that the CPU waits for it\nto finish execution before exiting the program. We do this by calling the\n[`synchronize()`](/mojo/stdlib/gpu/host/device_context/DeviceContext#synchronize)\nmethod of the `DeviceContext` object, which blocks until the device completes\nall operations in its queue.\n\n## 5. Manage grid dimensions\n\nThe grid in the previous step consisted of a one-dimensional grid of 2 thread\nblocks with 64 threads in each block. However, you can also organize the thread\nblocks in a two- or even a three-dimensional grid. Similarly, you can arrange\nthe threads in a thread block across one, two, or three dimensions. Typically,\nyou determine the dimensions of the grid and thread blocks based on the\ndimensionality of the data to process. For example, you might choose a\n1-dimensional gri", "position": 9, "token_count": 189, "has_code": false, "section_hierarchy": ["4. Compile and run the kernel"], "metadata": {"chunk_id": "gpu-intro-tutorial-009", "document_id": "gpu-intro-tutorial", "position": 9, "token_count": 189, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["4. Compile and run the kernel"], "file_path": "gpu/intro-tutorial.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial", "title": "Get started with GPU programming", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial#4-compile-and-run-the-kernel"}}
{"chunk_id": "gpu-intro-tutorial-010", "document_id": "gpu-intro-tutorial", "content": "d for processing large vectors, a 2-dimensional grid for\nprocessing matrices, and a 3-dimensional grid for processing the frames of a\nvideo.\n\n<figure>\n\n![](../images/gpu/multidimensional-grid.png#light)\n![](../images/gpu/multidimensional-grid-dark.png#dark)\n\n<figcaption><b>Figure 3.</b> Organization of thread blocks and threads within a\ngrid. </figcaption>\n\n</figure>\n\nTo better understand how grids, thread blocks, and threads work together, let's\nmodify our `print_threads()` kernel function to print the `x`, `y`, and `z`\ncomponents of the thread block and thread indices for each thread.\n\n```mojo title=\"vector_addition.mojo\"\nfn print_threads():\n \"\"\"Print thread IDs.\"\"\"\n\n print(\"Block index: [\",\n block_idx.x, block_idx.y, block_idx.z,\n \"]\\tThread index: [\",\n thread_idx.x, thread_idx.y, thread_idx.z,\n \"]\"\n )\n```\n\nThen, update `main()` to enqueue the kernel function with a 2x2x1 grid of\nthread blocks and a 16x4x2 arrangement of threads within each thread block:\n\n```mojo title=\"vector_addition.mojo\"\n ctx.enqueue_function[print_threads](\n grid_dim=(2, 2, 1),\n block_dim=(16, 4, 2)\n )\n```\n\nSave the file and run it again:\n\n```bash\nmojo vector_addition.mojo\n```\n\nYou should see something like the following output (which is abbreviated here):", "position": 10, "token_count": 325, "has_code": true, "section_hierarchy": ["5. Manage grid dimensions"], "metadata": {"chunk_id": "gpu-intro-tutorial-010", "document_id": "gpu-intro-tutorial", "position": 10, "token_count": 325, "has_code": true, "overlap_with_previous": false, "section_hierarchy": ["5. Manage grid dimensions"], "file_path": "gpu/intro-tutorial.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial", "title": "Get started with GPU programming", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial#5-manage-grid-dimensions"}}
{"chunk_id": "gpu-intro-tutorial-011", "document_id": "gpu-intro-tutorial", "content": "```mojo title=\"vector_addition.mojo\"\n ctx.enqueue_function[print_threads](\n grid_dim=(2, 2, 1),\n block_dim=(16, 4, 2)\n )\n```\n\nSave the file and run it again:\n\n```bash\nmojo vector_addition.mojo\n```\n\nYou should see something like the following output (which is abbreviated here):\n\n```output\nBlock index: [ 1 1 0 ]\tThread index: [ 0 2 0 ]\nBlock index: [ 1 1 0 ]\tThread index: [ 1 2 0 ]\nBlock index: [ 1 1 0 ]\tThread index: [ 2 2 0 ]\n...\nBlock index: [ 0 0 0 ]\tThread index: [ 14 1 0 ]\nBlock index: [ 0 0 0 ]\tThread index: [ 15 1 0 ]\nProgram finished\n```\n\nTry changing the grid and thread block dimensions to see how the output changes.\n\n[note]\nThe maximum number of threads per thread block and threads per SM is\nGPU-specific. For example, the NVIDIA A100 GPU has a maximum of 1,024 threads\nper thread block and 2,048 threads per SM.\n\nChoosing the size and shape of the grid and thread blocks is a balancing act\nbetween maximizing the number of threads that can execute concurrently and\nminimizing the amount of time spent waiting for data to be loaded from memory.\nFactors such as the size of the data to process, the number of SMs on the GPU,\nand the memory bandwidth of the GPU can all play a role in determining the\noptimal grid and thread block dimensions. One general guideline is to choose a\nthread block size that is a multiple of the warp size. This helps to maximize\nthe utilization of the GPU's resources and minimizes the overhead of managing\nmultiple warps.", "position": 11, "token_count": 395, "has_code": true, "section_hierarchy": ["5. Manage grid dimensions"], "metadata": {"chunk_id": "gpu-intro-tutorial-011", "document_id": "gpu-intro-tutorial", "position": 11, "token_count": 395, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["5. Manage grid dimensions"], "file_path": "gpu/intro-tutorial.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial", "title": "Get started with GPU programming", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial#5-manage-grid-dimensions"}}
{"chunk_id": "gpu-intro-tutorial-012", "document_id": "gpu-intro-tutorial", "content": "Now that you understand how to manage grid dimensions, let's get ready to create\na kernel that performs a simple element-wise addition of two vectors of floating\npoint numbers.\n\n## 6. Allocate host memory for the input vectors\n\nBefore creating the two input vectors for our kernel function, we need to\nunderstand the distinction between *host memory* and *device memory*. Host\nmemory is dynamic random-access memory (DRAM) accessible by the CPU, whereas\ndevice memory is DRAM accessible by the GPU. If you have data in host memory,\nyou must explicitly copy it to device memory before you can use it in a kernel\nfunction. Similarly, if your kernel function produces data that you want the CPU\nto use later, you must explicitly copy it back to host memory.\n\nFor thi", "position": 12, "token_count": 161, "has_code": false, "section_hierarchy": ["5. Manage grid dimensions"], "metadata": {"chunk_id": "gpu-intro-tutorial-012", "document_id": "gpu-intro-tutorial", "position": 12, "token_count": 161, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["5. Manage grid dimensions"], "file_path": "gpu/intro-tutorial.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial", "title": "Get started with GPU programming", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial#5-manage-grid-dimensions"}}
{"chunk_id": "gpu-intro-tutorial-013", "document_id": "gpu-intro-tutorial", "content": "s tutorial, we'll use the\n[`HostBuffer`](/mojo/stdlib/gpu/host/device_context/HostBuffer) type to\nrepresent our vectors on the host. A `HostBuffer` is a block of host memory\nassociated with a particular `DeviceContext`. It supports methods for\ntransferring data between host and device memory, as well as a basic set of\nmethods for accessing data elements by index and for printing the buffer.\n\nLet's update `main()` to create two `HostBuffer`s for our input vectors and\ninitialize them with values. You won't need the `print_threads()` kernel\nfunction anymore, so you can remove it and the code to compile and invoke it. So\nafter all that, your `vector_addition.mojo` file should look like this:\n\n```mojo title=\"vector_addition.mojo\"\nfrom gpu.host import DeviceContext\nfrom gpu.id import block_idx, thread_idx\nfrom sys import has_accelerator\n\n# Vector data type and size\nalias float_dtype = DType.float32\nalias vector_size = 1000\n\ndef main():\n @parameter\n if not has_accelerator():\n print(\"No compatible GPU found\")\n else:\n # Get the context for the attached GPU\n ctx = DeviceContext()\n\n # Create HostBuffers for input vectors\n lhs_host_buffer = ctx.enqueue_create_host_buffer[float_dtype](\n vector_size\n )\n rhs_host_buffer = ctx.enqueue_create_host_buffer[float_dtype](\n vector_size\n )\n ctx.synchronize()\n\n # Initialize the input vectors\n for i in range(vector_size):\n lhs_host_buffer[i] = Float32(i)\n rhs_host_buffer[i] = Floa", "position": 13, "token_count": 333, "has_code": true, "section_hierarchy": ["6. Allocate host memory for the input vectors"], "metadata": {"chunk_id": "gpu-intro-tutorial-013", "document_id": "gpu-intro-tutorial", "position": 13, "token_count": 333, "has_code": true, "overlap_with_previous": false, "section_hierarchy": ["6. Allocate host memory for the input vectors"], "file_path": "gpu/intro-tutorial.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial", "title": "Get started with GPU programming", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial#6-allocate-host-memory-for-the-input-vectors"}}
{"chunk_id": "gpu-intro-tutorial-014", "document_id": "gpu-intro-tutorial", "content": "t32(i * 0.5)\n\n print(\"LHS buffer: \", lhs_host_buffer)\n print(\"RHS buffer: \", rhs_host_buffer)\n```\n\nThe\n[`enqueue_create_host_buffer()`](/mojo/stdlib/gpu/host/device_context/DeviceContext#enqueue_create_host_buffer)\nmethod accepts the data type as a compile-time parameter and the size of the\nbuffer as a run-time argument and returns a `HostBuffer`. As with all\n`DeviceContext` methods whose name starts with `enqueue_`, the method is\nasynchronous and returns immediately, adding the operation to the queue to be\nexecuted by the `DeviceContext`. Therefore, we need to call the `synchronize()`\nmethod to ensure that the operation has completed before we use the `HostBuffer`\nobject. Then we can initialize the input vectors with values and print them.\n\nNow let's run the program to verify that everything is working so far.\n\n```bash\nmojo vector_addition.mojo\n```\n\nYou should see the following output:\n\n```output\nLHS buffer: HostBuffer([0.0, 1.0, 2.0, ..., 997.0, 998.0, 999.0])\nRHS buffer: HostBuffer([0.0, 0.5, 1.0, ..., 498.5, 499.0, 499.5])\n```", "position": 14, "token_count": 290, "has_code": true, "section_hierarchy": ["Vector data type and size"], "metadata": {"chunk_id": "gpu-intro-tutorial-014", "document_id": "gpu-intro-tutorial", "position": 14, "token_count": 290, "has_code": true, "overlap_with_previous": false, "section_hierarchy": ["Vector data type and size"], "file_path": "gpu/intro-tutorial.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial", "title": "Get started with GPU programming", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial#vector-data-type-and-size"}}
{"chunk_id": "gpu-intro-tutorial-015", "document_id": "gpu-intro-tutorial", "content": "```output\nLHS buffer: HostBuffer([0.0, 1.0, 2.0, ..., 997.0, 998.0, 999.0])\nRHS buffer: HostBuffer([0.0, 0.5, 1.0, ..., 498.5, 499.0, 499.5])\n```\n\n[note]\nYou might notice that we don't explicitly call any methods to free the host\nmemory allocated by our `HostBuffer`s. That's because a `HostBuffer` is subject\nto Mojo's standard ownership and lifecycle mechanisms. The Mojo compiler\nanalyzes our program to determine the last point that the owner of or a\nreference to an object is used and automatically adds a call to the object's\ndestructor. In our program, we last reference the buffers at the end of our\nprogram's `main()` method. However in a more complex program, the `HostBuffer`\ncould persist across calls to multiple kernel functions if it is referenced at\nlater points in the program. See the [Ownership](/mojo/manual/values/ownership)\nand [Intro to value lifecycle](/mojo/manual/lifecycle) sections of the Mojo\nManual for more information on Mojo value ownership and value lifecycle\nmanagement.\n\n## 7. Copy the input vectors to GPU memory and allocate an output vector\n\nNow that we have our input vectors allocated and initialized on the CPU, let's\ncopy them to the GPU so that they'll be available for the kernel function to\nuse. While we're at it, we'll also allocate memory on the GPU for the output\nvector that will hold the result of the kernel function.\n\nAdd the following code to the end of the `main()` function:", "position": 15, "token_count": 362, "has_code": true, "section_hierarchy": ["Vector data type and size"], "metadata": {"chunk_id": "gpu-intro-tutorial-015", "document_id": "gpu-intro-tutorial", "position": 15, "token_count": 362, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Vector data type and size"], "file_path": "gpu/intro-tutorial.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial", "title": "Get started with GPU programming", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial#vector-data-type-and-size"}}
{"chunk_id": "gpu-intro-tutorial-016", "document_id": "gpu-intro-tutorial", "content": "Now that we have our input vectors allocated and initialized on the CPU, let's\ncopy them to the GPU so that they'll be available for the kernel function to\nuse. While we're at it, we'll also allocate memory on the GPU for the output\nvector that will hold the result of the kernel function.\n\nAdd the following code to the end of the `main()` function:\n\n```mojo title=\"vector_addition.mojo\"\n # Create DeviceBuffers for the input vectors\n lhs_device_buffer = ctx.enqueue_create_buffer[float_dtype](vector_size)\n rhs_device_buffer = ctx.enqueue_create_buffer[float_dtype](vector_size)\n\n # Copy the input vectors from the HostBuffers to the DeviceBuffers\n ctx.enqueue_co", "position": 16, "token_count": 144, "has_code": true, "section_hierarchy": ["Vector data type and size"], "metadata": {"chunk_id": "gpu-intro-tutorial-016", "document_id": "gpu-intro-tutorial", "position": 16, "token_count": 144, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Vector data type and size"], "file_path": "gpu/intro-tutorial.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial", "title": "Get started with GPU programming", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial#vector-data-type-and-size"}}
{"chunk_id": "gpu-intro-tutorial-017", "document_id": "gpu-intro-tutorial", "content": "py(dst_buf=lhs_device_buffer, src_buf=lhs_host_buffer)\n ctx.enqueue_copy(dst_buf=rhs_device_buffer, src_buf=rhs_host_buffer)\n\n # Create a DeviceBuffer for the result vector\n result_device_buffer = ctx.enqueue_create_buffer[float_dtype](\n vector_size\n )\n```\n\nThe [`DeviceBuffer`](/mojo/stdlib/gpu/host/device_context/DeviceBuffer) type is\nanalogous to the `HostBuffer` type, but represents a block of device memory\nassociated with a particular `DeviceContext`. Specifically, the buffer is\nlocated in the device's *global memory* space, which is accessible by all\nthreads executing on the device. As with a `HostBuffer`, a `DeviceBuffer` is\nsubject to Mojo's standard ownership and lifecycle mechanisms. It persists until\nit is no longer referenced in the program or until the `DeviceContext` itself\nis destroyed.\n\nThe\n[`enqueue_create_buffer()`](/mojo/stdlib/gpu/host/device_context/DeviceContext#enqueue_create_buffer)\nmethod accepts the data type as a compile-time parameter and the size of the\nbuffer as a run-time argument and returns a `DeviceBuffer`. The operation is\nasynchronous, but we don't need to call the `synchronize()` method yet because\nwe have more operations to add to the queue.\n\nThe [`enqueue_copy()`](/mojo/stdlib/gpu/host/device_context/DeviceContext#enqueue_copy)\nmethod is overloaded to support copying from host to device, device to host, or\neven device to device for systems that have multiple GPUs. In this example, we\nuse it to copy the data in our `HostBuffer`s to the `DeviceBuffer`s.", "position": 17, "token_count": 349, "has_code": true, "section_hierarchy": ["Vector data type and size", "7. Copy the input vectors to GPU memory and allocate an output vector"], "metadata": {"chunk_id": "gpu-intro-tutorial-017", "document_id": "gpu-intro-tutorial", "position": 17, "token_count": 349, "has_code": true, "overlap_with_previous": false, "section_hierarchy": ["Vector data type and size", "7. Copy the input vectors to GPU memory and allocate an output vector"], "file_path": "gpu/intro-tutorial.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial", "title": "Get started with GPU programming", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial#7-copy-the-input-vectors-to-gpu-memory-and-allocate-an-output-vector"}}
{"chunk_id": "gpu-intro-tutorial-018", "document_id": "gpu-intro-tutorial", "content": "The [`enqueue_copy()`](/mojo/stdlib/gpu/host/device_context/DeviceContext#enqueue_copy)\nmethod is overloaded to support copying from host to device, device to host, or\neven device to device for systems that have multiple GPUs. In this example, we\nuse it to copy the data in our `HostBuffer`s to the `DeviceBuffer`s.\n\n[note]\nBoth `DeviceBuffer` and `HostBuffer` also include\n[`enqueue_copy_to()`](/mojo/stdlib/gpu/host/device_context/DeviceBuffer#enqueue_copy_to)\nand\n[`enqueue_copy_from()`](/mojo/stdlib/gpu/host/device_context/DeviceBuffer#enqueue_copy_from)\nmethods. These are simply convenience methods that call the `enqueue_copy()`\nmethod on their corresponding `DeviceContext`. Therefore, we could have written\nthe copy operations in the previous example with the following equivalent code:\n\n```mojo\n lhs_host_buffer.enqueue_copy_to(dst=lhs_device_buffer)\n rhs_host_buffer.enqueue_copy_to(dst=rhs_device_buffer)\n```\n\n## 8. Create `LayoutTensor` views\n\nOne last step before writing the kernel function is that we're going to create a\n[`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor) view for each\nof the vectors. `LayoutTensor` provides a powerful abstraction for\nmulti-dimensional data with precise control over memory organization. It\nsupports various memory layouts (row-major, column-major, tiled),\nhardware-specific optimizations, and efficient parallel access patterns.\nWe don't need all of these features for this tutorial, but in more\ncomplex kernels it's a useful tool for manipulating data. So even though it\nisn't strictly necessary for this example, we'll use `LayoutTensor` because\nyou'll see it in more complex examples and it's good to get familiar with it.\n\nFirst add the following import to the top of the file", "position": 18, "token_count": 390, "has_code": true, "section_hierarchy": ["Vector data type and size", "7. Copy the input vectors to GPU memory and allocate an output vector"], "metadata": {"chunk_id": "gpu-intro-tutorial-018", "document_id": "gpu-intro-tutorial", "position": 18, "token_count": 390, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Vector data type and size", "7. Copy the input vectors to GPU memory and allocate an output vector"], "file_path": "gpu/intro-tutorial.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial", "title": "Get started with GPU programming", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial#7-copy-the-input-vectors-to-gpu-memory-and-allocate-an-output-vector"}}
{"chunk_id": "gpu-intro-tutorial-019", "document_id": "gpu-intro-tutorial", "content": ":\n\n```mojo title=\"vector_addition.mojo\"\nfrom layout import Layout, LayoutTensor\n```\n\nA [`Layout`](/mojo/kernels/layout/layout/Layout) is a representation of memory\nlayouts using shape and stride information, and it maps between logical\ncoordinates and linear memory indices. We'll need to use the same `Layout`\ndefinition multiple times, so add the following alias to the top of the file\nafter the other aliases:\n\n```mojo title=\"vector_addition.mojo\"\nalias layout = Layout.row_major(vector_size)\n```\n\nAnd finally add the following code to the end of the `main()` function\nto create `LayoutTensor` views for each of the vectors:\n\n```mojo title=\"vector_addition.mojo\"\n # Wrap the DeviceBuffers in LayoutTensors\n lhs_tensor = LayoutTensor[float_dtype, layout](lhs_device_buffer)\n rhs_tensor = LayoutTensor[float_dtype, layout](rhs_device_buffer)\n result_tensor = LayoutTensor[float_dtype, layout](result_device_buffer)\n```\n\n## 9. Define the vector addition kernel function\n\nNow we're ready to write the kernel function. First add the following imports\n(note that we've added `block_dim` to the list of imports from `gpu.id`):\n\n```mojo title=\"vector_addition.mojo\"\nfrom gpu.id import block_dim, block_idx, thread_idx\nfrom math import ceildiv\n```\n\nThen, add the following code to `vector_addition.mojo` just before the\n`main()` function:\n\n```mojo title=\"vector_addition.mojo\"\n# Calculate the number of thread blocks needed by dividing the vector size\n# by the block size and rounding up.\nalias block_size = 256\nalias num_blocks = ceildiv(vector_size, block_size)", "position": 19, "token_count": 362, "has_code": true, "section_hierarchy": ["Vector data type and size", "8. Create `LayoutTensor` views"], "metadata": {"chunk_id": "gpu-intro-tutorial-019", "document_id": "gpu-intro-tutorial", "position": 19, "token_count": 362, "has_code": true, "overlap_with_previous": false, "section_hierarchy": ["Vector data type and size", "8. Create `LayoutTensor` views"], "file_path": "gpu/intro-tutorial.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial", "title": "Get started with GPU programming", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial#8-create-layouttensor-views"}}
{"chunk_id": "gpu-intro-tutorial-020", "document_id": "gpu-intro-tutorial", "content": "Then, add the following code to `vector_addition.mojo` just before the\n`main()` function:\n\n```mojo title=\"vector_addition.mojo\"\n# Calculate the number of thread blocks needed by dividing the vector size\n# by the block size and rounding up.\nalias block_size = 256\nalias num_blocks = ceildiv(vector_size, block_size)\n\nfn vector_addition(\n lhs_tensor: LayoutTensor[float_dtype, layout, MutableAnyOrigin],\n rhs_tensor: LayoutTensor[float_dtype, layout, MutableAnyOrigin],\n out_tensor: LayoutTensor[float_dtype, layout, MutableAnyOrigin]", "position": 20, "token_count": 123, "has_code": true, "section_hierarchy": ["Vector data type and size", "8. Create `LayoutTensor` views"], "metadata": {"chunk_id": "gpu-intro-tutorial-020", "document_id": "gpu-intro-tutorial", "position": 20, "token_count": 123, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Vector data type and size", "8. Create `LayoutTensor` views"], "file_path": "gpu/intro-tutorial.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial", "title": "Get started with GPU programming", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial#8-create-layouttensor-views"}}
{"chunk_id": "gpu-intro-tutorial-021", "document_id": "gpu-intro-tutorial", "content": ",\n):\n \"\"\"Calculate the element-wise sum of two vectors on the GPU.\"\"\"\n\n # Calculate the index of the vector element for the thread to process\n var tid = block_idx.x * block_dim.x + thread_idx.x\n\n # Don't process out of bounds elements\n if tid < vector_size:\n out_tensor[tid] = lhs_tensor[tid] + rhs_tensor[tid]\n```\n\nOur `vector_addition()` kernel function accepts the two input tensors and the\noutput tensor as arguments. We also need to know the siz", "position": 21, "token_count": 103, "has_code": true, "section_hierarchy": ["Vector data type and size", "9. Define the vector addition kernel function"], "metadata": {"chunk_id": "gpu-intro-tutorial-021", "document_id": "gpu-intro-tutorial", "position": 21, "token_count": 103, "has_code": true, "overlap_with_previous": false, "section_hierarchy": ["Vector data type and size", "9. Define the vector addition kernel function"], "file_path": "gpu/intro-tutorial.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial", "title": "Get started with GPU programming", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial#9-define-the-vector-addition-kernel-function"}}
{"chunk_id": "gpu-intro-tutorial-022", "document_id": "gpu-intro-tutorial", "content": "e of the vector (which\nwe've defined with the alias `vector_size`) because", "position": 22, "token_count": 18, "has_code": false, "section_hierarchy": ["Calculate the number of thread blocks needed by dividing the vector size"], "metadata": {"chunk_id": "gpu-intro-tutorial-022", "document_id": "gpu-intro-tutorial", "position": 22, "token_count": 18, "has_code": false, "overlap_with_previous": false, "section_hierarchy": ["Calculate the number of thread blocks needed by dividing the vector size"], "file_path": "gpu/intro-tutorial.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial", "title": "Get started with GPU programming", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial#calculate-the-number-of-thread-blocks-needed-by-dividing-the-vector-size"}}
{"chunk_id": "gpu-intro-tutorial-023", "document_id": "gpu-intro-tutorial", "content": "it might not be a multiple\nof the block size. In fact in this example, the size of the vector is 1,000,\nwhich is not a multiple of our block size of 256. So as we assign our threads to\nread elements from the tensor, we need to make sure we don't overrun the bounds\nof the tensor.\n\nThe body of the kernel function starts by calculating linear index of the tensor\nelement that a particular thread is responsible for. The `block_dim` object\n(which we added to the list of imports) contains the dimensions of the thread\nblocks as `x`, `y`, and `z` values. Because we're going to use a one-dimensional\ngrid of thread blocks, we need only the `x` dimension. We can then calculate\n`tid`, the unique \"global\" index of the thread within the output tensor as\n`block_dim.x * block_idx.x + thread_idx.x`. For example, the `tid` values for\nthe threads in the first thread block range from 0 to 255, the `tid` values for\nthe threads in the second thread block range from 256 to 511, and so on.\n\n[note]\nAs a convenience, the [`gpu.id`](/mojo/stdlib/gpu/id) module includes a\n`global_idx` alias that contains the unique \"global\" `x`, `y`, and `z` indices\nof the thread within the grid of thread blocks. So for our one-dimensional grid\nof one-dimensional thread blocks, `global_idx.x` is equivalent to the value of\n`tid` that we calculated above. However for this tutorial, it's best that you\nlearn how to calculate `tid` manually so that you understand how the grid and\nthread block dimensions work.", "position": 23, "token_count": 369, "has_code": false, "section_hierarchy": ["by the block size and rounding up."], "metadata": {"chunk_id": "gpu-intro-tutorial-023", "document_id": "gpu-intro-tutorial", "position": 23, "token_count": 369, "has_code": false, "overlap_with_previous": false, "section_hierarchy": ["by the block size and rounding up."], "file_path": "gpu/intro-tutorial.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial", "title": "Get started with GPU programming", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial#by-the-block-size-and-rounding-up"}}
{"chunk_id": "gpu-intro-tutorial-024", "document_id": "gpu-intro-tutorial", "content": "The function then checks if the calculated `tid` is less than the size of the\noutput tensor. If it is, the thread reads the corresponding elements from the\n`lhs_tensor` and `rhs_tensor` tensors, adds them together, and stores the result\nin the corresponding element of the `out_tensor` tensor.\n\n## 10. Invoke the kernel function and copy the output back to the CPU\n\nThe last step is to compile and invoke the kernel function, then copy the output\nback to the CPU. To do so, add the following code to the end of the `main()`\nfunction:\n\n```mojo title=\"vector_addition.mojo\"\n # Compile and enqueue the kernel\n ctx.enqueue_function[vector_addition](\n lhs_tensor,\n rhs_tensor,\n result_tensor,\n grid_dim=num_blocks,\n block_dim=block_size,\n )\n\n # Create a HostBuffer for the result vector\n result_host_buffer = ctx.enqueue_create_host_buffer[float_dtype](\n vector_size\n )\n\n # Copy the result vector from the DeviceBuffer to the HostBuffer\n ctx.enqueue_copy(\n dst_buf=result_host_buffer, src_buf=result_device_buffer\n )\n\n # Finally, synchronize the DeviceContext to run all enqueued operations\n ctx.synchronize()\n\n print(\"Result vector:\", result_host_buffer)\n```\n\n<details>\n <summary>Click here t", "position": 24, "token_count": 266, "has_code": true, "section_hierarchy": ["by the block size and rounding up."], "metadata": {"chunk_id": "gpu-intro-tutorial-024", "document_id": "gpu-intro-tutorial", "position": 24, "token_count": 266, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["by the block size and rounding up."], "file_path": "gpu/intro-tutorial.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial", "title": "Get started with GPU programming", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial#by-the-block-size-and-rounding-up"}}
{"chunk_id": "gpu-intro-tutorial-025", "document_id": "gpu-intro-tutorial", "content": "o see the complete version of `vector_addition.mojo`.</summary>\n\n```mojo title=\"vector_addition.mojo\"\nfrom gpu.host import DeviceContext\nfrom gpu.id import block_dim, block_idx, thread_idx\nfrom layout import Layout, LayoutTensor\nfrom math import ceildiv\nfrom sys import has_accelerator\n\n# Vector data type and size\nalias float_dtype = DType.float32\nalias vector_size = 1000\nalias layout = Layout.row_major(vector_size)\n\n# Calculate the number of thread blocks needed by dividing the vector size\n# by the block size and rounding up.\nalias block_size = 256\nalias num_blocks = ceildiv(vector_size, block_size)\n\nfn vector_addition(\n lhs_tensor: LayoutTensor[float_dtype, layout, MutableAnyOrigin],\n rhs_tensor: LayoutTensor[float_dtype, layout, MutableAnyOrigin],\n out_tensor: LayoutTensor[float_dtype, layout, MutableAnyOrigin],\n):\n \"\"\"Calculate the element-wise sum of two vectors on the GPU.\"\"\"\n\n # Calculate the index of the vector element for the thread to process\n var tid = block_idx.x * block_dim.x + thread_idx.x\n\n # Don't process out of bounds elements\n if tid < vector_size:\n out_tensor[tid] = lhs_tensor[tid] + rhs_tensor[tid]\n\ndef main():\n @parameter\n if not has_accelerator():\n print(\"No compatible GPU found\")\n else:\n # Get the context for the attached GPU\n ctx = DeviceContext()\n\n # Create HostBuffers for input vectors\n lhs_host_", "position": 25, "token_count": 308, "has_code": true, "section_hierarchy": ["by the block size and rounding up.", "10. Invoke the kernel function and copy the output back to the CPU"], "metadata": {"chunk_id": "gpu-intro-tutorial-025", "document_id": "gpu-intro-tutorial", "position": 25, "token_count": 308, "has_code": true, "overlap_with_previous": false, "section_hierarchy": ["by the block size and rounding up.", "10. Invoke the kernel function and copy the output back to the CPU"], "file_path": "gpu/intro-tutorial.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial", "title": "Get started with GPU programming", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial#10-invoke-the-kernel-function-and-copy-the-output-back-to-the-cpu"}}
{"chunk_id": "gpu-intro-tutorial-026", "document_id": "gpu-intro-tutorial", "content": "buffer = ctx.enqueue_create_host_buffer[float_dtype](\n vector_size\n )\n rhs_host_buffer = ctx.enqueue_create_host_buffer[float_dtype](", "position": 26, "token_count": 26, "has_code": false, "section_hierarchy": ["Vector data type and size"], "metadata": {"chunk_id": "gpu-intro-tutorial-026", "document_id": "gpu-intro-tutorial", "position": 26, "token_count": 26, "has_code": false, "overlap_with_previous": false, "section_hierarchy": ["Vector data type and size"], "file_path": "gpu/intro-tutorial.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial", "title": "Get started with GPU programming", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial#vector-data-type-and-size"}}
{"chunk_id": "gpu-intro-tutorial-027", "document_id": "gpu-intro-tutorial", "content": "vector_size\n )\n ctx.synchronize()\n\n # Initialize the input vectors\n for i", "position": 27, "token_count": 16, "has_code": false, "section_hierarchy": ["Calculate the number of thread blocks needed by dividing the vector size"], "metadata": {"chunk_id": "gpu-intro-tutorial-027", "document_id": "gpu-intro-tutorial", "position": 27, "token_count": 16, "has_code": false, "overlap_with_previous": false, "section_hierarchy": ["Calculate the number of thread blocks needed by dividing the vector size"], "file_path": "gpu/intro-tutorial.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial", "title": "Get started with GPU programming", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial#calculate-the-number-of-thread-blocks-needed-by-dividing-the-vector-size"}}
{"chunk_id": "gpu-intro-tutorial-028", "document_id": "gpu-intro-tutorial", "content": "in range(vector_size):\n lhs_host_buffer[i] = Float32(i)\n rhs_host_buffer[i] = Float32(i * 0.5)\n\n print(\"LHS buffer: \", lhs_host_buffer)\n print(\"RHS buffer: \", rhs_host_buffer)\n\n # Create DeviceBuffers for the input vectors\n lhs_device_buffer = ctx.enqueue_create_buffer[float_dtype](vector_size)\n rhs_device_buffer = ctx.enqueue_create_buffer[float_dtype](vector_size)\n\n # Copy the input vectors from the HostBuffers to the DeviceBuffers\n ctx.enqueue_copy(dst_buf=lhs_device_buffer, src_buf=lhs_host_buffer)\n ctx.enqueue_copy(dst_buf=rhs_device_buffer, src_buf=rhs_host_buffer)\n\n # Create a DeviceBuffer for the result vector\n result_device_buffer = ctx.enqueue_create_buffer[float_dtype](\n vector_size\n )\n\n # Wrap the DeviceBuffers in LayoutTensors\n lhs_tensor = LayoutTensor[float_dtype, layout](lhs_device_buffer)\n rhs_tensor = LayoutTensor[float_dtype, layout](rhs_device_buffer)\n result_tensor = LayoutTensor[float_dtype, layout](result_device_buffer)\n\n # Compile and enqueue the kernel\n ctx.enqueue_function[vector_addition](\n lhs_tensor,\n rhs_tensor,\n result_tensor,\n grid_dim=num_blocks,\n block_dim=block_size,\n )\n\n # Create a HostBuffer for the result vector\n result_host_buffer = ctx.enqueue_create_host_buffer[float_dtype](\n vector_size\n )\n\n # Copy the result vector from the DeviceBuffer to the HostBuffer\n ctx.enqueue_copy(\n dst_buf=result_host_buffer, src_buf=result_device_buffer\n )\n\n # Finally, synchronize the DeviceContext to run all enqueued operations\n ctx.synchronize()\n\n print(\"Result vector:\", result_host_buffer)\n```\n</details>", "position": 28, "token_count": 339, "has_code": true, "section_hierarchy": ["by the block size and rounding up."], "metadata": {"chunk_id": "gpu-intro-tutorial-028", "document_id": "gpu-intro-tutorial", "position": 28, "token_count": 339, "has_code": true, "overlap_with_previous": false, "section_hierarchy": ["by the block size and rounding up."], "file_path": "gpu/intro-tutorial.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial", "title": "Get started with GPU programming", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial#by-the-block-size-and-rounding-up"}}
{"chunk_id": "gpu-intro-tutorial-029", "document_id": "gpu-intro-tutorial", "content": " # Copy the result vector from the DeviceBuffer to the HostBuffer\n ctx.enqueue_copy(\n dst_buf=result_host_buffer, src_buf=result_device_buffer\n )\n\n # Finally, synchronize the DeviceContext to run all enqueued operations\n ctx.synchronize()\n\n print(\"Result vector:\", result_host_buffer)\n```\n</details>\n\nThe `enqueue_function()` method enqueues the compilation and invocation of the\n`vector_addition()` kernel function, passing the input and output tensors as\narguments. The `grid_dim` and `block_dim` arguments use the `num_blocks` and\n`block_size` aliases we defined in the previous step.\n\n[note]\nThe current implementation of `enqueue_function()` doesn't typecheck the\narguments to the compiled kernel function, which can lead to obscure run-time\nerrors if the argument ordering, types, or count doesn't match the kernel\nfunction's definition.\n\nFor compile-time typechecking, you can use the\n[`compile_function_checked()`](/mojo/stdlib/gpu/host/device_context/DeviceContext/#compile_function_checked)\nand\n[`enqueue_function_checked()`](/mojo/stdlib/gpu/host/device_context/DeviceContext/#enqueue_function_checked)\nmethods.\n\nHere's the typechecked equivalent of the `vector_addition()` kernel compilation\nand enqueuing shown above:\n\n```mojo\nctx.enqueue_function_checked[vector_addition, vector_addition](\n lhs_tensor,\n rhs_tensor,\n result_tensor,\n grid_dim=num_blocks,\n block_dim=block_size,\n)\n```\n\nNote that `enqueue_function_checked()` currently requires the kernel function to\nbe provided *twice* as parameters when you use it to compile the kernel. This\nrequirement will be removed in a future API update, when typechecking will\nbecome the default behavior for both `compile_function()` and\n`enqueue_function()`.", "position": 29, "token_count": 372, "has_code": true, "section_hierarchy": ["by the block size and rounding up."], "metadata": {"chunk_id": "gpu-intro-tutorial-029", "document_id": "gpu-intro-tutorial", "position": 29, "token_count": 372, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["by the block size and rounding up."], "file_path": "gpu/intro-tutorial.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial", "title": "Get started with GPU programming", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial#by-the-block-size-and-rounding-up"}}
{"chunk_id": "gpu-intro-tutorial-030", "document_id": "gpu-intro-tutorial", "content": "Note that `enqueue_function_checked()` currently requires the kernel function to\nbe provided *twice* as parameters when you use it to compile the kernel. This\nrequirement will be removed in a future API update, when typechecking will\nbecome the default behavior for both `compile_function()` and\n`enqueue_function()`.\n\nAfter the kernel function has been compiled and enqueued, we create a\n`HostBuffer` to hold the result vector. Then we copy the result vector from the\n`DeviceBuffer` to the `HostBuffer`. Finally, we synchronize the `DeviceContext`\nto run all enqueued operations. After synchronizing, we can print the result\nvector to the console.\n\nAt this point, the Mojo compiler determines that the `DeviceContext`, the\n`DeviceBuffer`s, the `HostBuffer`s, and the `LayoutTensor`s are no longer used\nand so it automatically invokes their destructors to free their allocated\nmemory. (For a detailed explanation of object lifetime and destruction in Mojo,\nsee the [Death of a value](/mojo/manual/lifecycle/death) section of the Mojo\nManual.)\n\nSo it's finally time to run the program to see the results of our hard work.\n\n```bash\nmojo vector_addition.mojo\n```\n\nYou should see the following output:\n\n```output\nLHS buffer: HostBuffer([0.0, 1.0, 2.0, ..., 997.0, 998.0, 999.0])\nRHS buffer: HostBuffer([0.0, 0.5, 1.0, ..., 498.5, 499.0, 499.5])\nResult vector: HostBuffer([0.0, 1.5, 3.0, ..., 1495.5, 1497.0, 1498.5])\n```", "position": 30, "token_count": 387, "has_code": true, "section_hierarchy": ["by the block size and rounding up."], "metadata": {"chunk_id": "gpu-intro-tutorial-030", "document_id": "gpu-intro-tutorial", "position": 30, "token_count": 387, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["by the block size and rounding up."], "file_path": "gpu/intro-tutorial.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial", "title": "Get started with GPU programming", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial#by-the-block-size-and-rounding-up"}}
{"chunk_id": "gpu-intro-tutorial-031", "document_id": "gpu-intro-tutorial", "content": "And now that you're done with the tutorial, exit your project's virtual\nenvironment:\n\n```bash\nexit\n```\n\n## Summary\n\nIn this tutorial, we've learned how to use Mojo's `gpu.host` package to write a\nsimple kernel function that performs an element-wise addition of two vectors. We\ncovered:\n\n- Understanding basic GPU concepts like devices, grids, and thread blocks.\n- Moving data between CPU and GPU memory.\n- Writing and compiling a GPU kernel function.\n- Executing parallel computations on the GPU.\n\n## Next steps\n\nNow that you understand the basics of GPU programming with Mojo, here are some\nsuggested next steps:\n\nexport const Puzzles ={\n title: \"GPU Puzzles\",\n description: \"Learn to program GPUs in Mojo by solving increasingly complex challenges\",\n metadata: {\n permalink: \"https://puzzles.modular.com/introduction.html\"\n }\n};\n\nexport const Examples ={\n title: \"GPU code examples\",\n description: \"Check out some more examples for programming GPUs with Mojo\",\n metadata: {\n permalink: \"https://github.com/modular/modular/tree/main/examples/mojo/gpu-functions\"\n }\n};\n\nexport const nextSteps = [\n '../../layout/layouts',\n Puzzles,\n Examples,\n]", "position": 31, "token_count": 249, "has_code": true, "section_hierarchy": ["by the block size and rounding up."], "metadata": {"chunk_id": "gpu-intro-tutorial-031", "document_id": "gpu-intro-tutorial", "position": 31, "token_count": 249, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["by the block size and rounding up."], "file_path": "gpu/intro-tutorial.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial", "title": "Get started with GPU programming", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/intro-tutorial#by-the-block-size-and-rounding-up"}}
