{"chunk_id": "gpu-architecture-000", "document_id": "gpu-architecture", "content": "GPUs are essential for high-performance computation, but programming them has\nhistorically been a highly specialized skill. Mojo represents a chance to\nrethink GPU programming and make it more approachable. But if you've never\nprogrammed a GPU before, you first need to understand how the GPU hardware and\nexecution model is different from a CPU. That's what this page is all\nabout—there's no code here, so if you already understand GPU hardware, you can\nskip to the [Get started tutorial](/mojo/manual/gpu/intro-tutorial) or [GPU\nprogramming fundamentals](/mojo/manual/gpu/fundamentals).", "position": 0, "token_count": 133, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "gpu-architecture-000", "document_id": "gpu-architecture", "position": 0, "token_count": 133, "has_code": false, "overlap_with_previous": false, "section_hierarchy": [], "file_path": "gpu/architecture.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/architecture", "title": "Intro to GPUs", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/architecture"}}
{"chunk_id": "gpu-architecture-001", "document_id": "gpu-architecture", "content": "## GPU architecture overview\n\nGraphics Processing Units (GPUs) and Central Processing Units (CPUs) represent\nfundamentally different approaches to computation. While CPUs feature a few\npowerful cores optimized for sequential processing and complex decision making,\nGPUs contain thousands of smaller, simpler cores designed for parallel\nprocessing. These simpler cores lack sophisticated features like branch\nprediction or deep instruction pipelines, but excel at performing the same\noperation across large datasets simultaneously.\n\nModern systems take advantage of both CPU and GPU processors' strengths by\nhaving the CPU handle primary program flow and complex logic, while offloading\nparallel computations to the GPU through specialized APIs. Figure 1 illustrates\nsome of the architectural differences between the two, which we'll discuss\nfurther below.\n\n<figure>\n\n![](../images/gpu/cpu-gpu-architecture.png#light)\n![](../images/gpu/cpu-gpu-architecture-dark.png#dark)\n\n<figcaption><b>Figure 1.</b> High-level comparison of CPU vs GPU architecture,\nbased on <cite><a\nhref=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/\">CUDA\nC++ Programming Guide</a></cite>.</figcaption>\n\n</figure>\n\nThe basic building block of a GPU is called a *streaming multiprocessor* (SM)\non NVIDIA GPUs or a *compute unit* (CU) on AMD GPUs (they're the same idea and\nwe'll refer to them both as SM). SMs sit between the high-level GPU control\nlogic and the individual execution units, acting as self-contained processing\nfactories that can operate independently and in parallel.\n\n[note]\nAMD and NVIDIA use different terminology to describe their GPU hardware and\nprogramming models. For simplicity, we typically use the NVIDIA terminology\nthroughout the Mojo Manual because it's used more widely in the industry.", "position": 1, "token_count": 375, "has_code": false, "section_hierarchy": ["GPU architecture overview"], "metadata": {"chunk_id": "gpu-architecture-001", "document_id": "gpu-architecture", "position": 1, "token_count": 375, "has_code": false, "overlap_with_previous": false, "section_hierarchy": ["GPU architecture overview"], "file_path": "gpu/architecture.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/architecture", "title": "Intro to GPUs", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/architecture#gpu-architecture-overview"}}
{"chunk_id": "gpu-architecture-002", "document_id": "gpu-architecture", "content": "[note]\nAMD and NVIDIA use different terminology to describe their GPU hardware and\nprogramming models. For simplicity, we typically use the NVIDIA terminology\nthroughout the Mojo Manual because it's used more widely in the industry.\n\nMultiple SMs are arranged on a single GPU chip, with each SM capable of handling\nmultiple workloads simultaneously. The GPU's global scheduler assigns work to\nindividual SMs, while the memory controller manages data flow between the SMs\nand various memory hierarchies (global memory, L2 cache, etc.).\n\nThe number of SMs in a GPU can vary significantly based on the model and\nintended use case, from a handful in entry-level GPUs to dozens or even hundreds\nin high-end professional cards. This scalable architecture enables GPUs to\nmaintain excellent performance across different workload sizes and types.\n\nEach SM contains several essential components:\n\n- **CUDA Cores (NVIDIA)/Stream Processors (AMD):** These are the basic\n arithmetic logic units (ALUs) that perform integer and floating-point\n calculations. A single SM can contain dozens or hundreds of these cores.\n- **Tensor Cores (NVIDIA)/Matrix Cores (AMD):** Specialized units optimized for\n matrix multiplication and convolution operations.\n- **Special Function Units (SFUs):** Handle complex mathematical operations like\n trigonometry, square roots, and exponential functions.\n- **Register Files:** Ultra-fast storage that holds intermediate results and\n thread-specific data. Modern SMs can have hundreds of kilobytes of register\n space shared among active threads.\n- **Shared Memory/L1 Cache:** A programmable, low-latency memory space that\n enables data sharing between threads. This memory is typically configurable\n between shared memory and L1 cache functions.\n- **Load/Store Units:** Manage data movement between different memory spaces,\n handling memory access requests from threads.\n\n<figure>", "position": 2, "token_count": 380, "has_code": false, "section_hierarchy": ["GPU architecture overview"], "metadata": {"chunk_id": "gpu-architecture-002", "document_id": "gpu-architecture", "position": 2, "token_count": 380, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["GPU architecture overview"], "file_path": "gpu/architecture.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/architecture", "title": "Intro to GPUs", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/architecture#gpu-architecture-overview"}}
{"chunk_id": "gpu-architecture-003", "document_id": "gpu-architecture", "content": "model\n\nA GPU *kernel* is simply a function that runs on a GPU, executing a specific\ncomputation on a large dataset in parallel across thousands or millions of\n*threads* (also known as *work items* on AMD GPUs). You might already be\nfamiliar with threads when programming for a CPU, but GPU threads are different.\nOn a CPU, threads are managed by the operating system and can perform completely\nindependent tasks, such as managing a user interface, fetching data from a\ndatabase, and so on. But on a GPU, threads are managed by the GPU itself. For a\ngiven kernel function, all the threads on a GPU execute the same function, but\nthey each work on a different part of the data.\n\nA *grid* is the top-level organizational structure of the threads executing a\nkernel function on a GPU. A grid consists of multiple *thread blocks* (or\n*workgroups* on AMD GPUs), which are further divided into individual threads\nthat execute the kernel function concurrently.\n\n<figure>\n\n![](../images/gpu/grid-hierarchy.png#light)\n![](../images/gpu/grid-hierarchy-dark.png#dark)\n\n<figcaption><b>Figure 3.</b> Hierarchy of threads running on a GPU, showing the\nrelationship of the grid, thread blocks, warps, and individual threads, based\non <cite><a\nhref=\"https://rocm.docs.amd.com/projects/HIP/en/latest/understand/programming_model.html\">HIP\nProgramming Guide</a></cite></figcaption>\n\n</figure>\n\nThe division of a grid into thread blocks serves multiple purposes:", "position": 3, "token_count": 335, "has_code": false, "section_hierarchy": ["GPU execution model"], "metadata": {"chunk_id": "gpu-architecture-003", "document_id": "gpu-architecture", "position": 3, "token_count": 335, "has_code": false, "overlap_with_previous": false, "section_hierarchy": ["GPU execution model"], "file_path": "gpu/architecture.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/architecture", "title": "Intro to GPUs", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/architecture#gpu-execution-model"}}
{"chunk_id": "gpu-architecture-004", "document_id": "gpu-architecture", "content": "</figure>\n\nThe division of a grid into thread blocks serves multiple purposes:\n\n- It breaks down the overall workload—managed by the grid—into smaller, more\n manageable portions that can be processed independently. This division allows\n for better resource utilization and scheduling flexibility across multiple SMs\n in the GPU.\n- Thread blocks provide a scope for threads to collaborate through shared memory\n and synchronization primitives, enabling efficient parallel algorithms and\n data sharing patterns.\n- Thread blocks help with scalability by allowing the same program to run\n efficiently across different GPU architectures, as the hardware can\n automatically distribute blocks based on available resources.\n\nYou must specify the number of thread blocks in a grid and how they are arranged\nacross one, two, or three dimensions. Typically, you determine the dimensions of\nthe grid based on the dimensionality of the data to process. For example, you\nmight choose a 1-dimensional grid for processing large vectors, a 2-dimensional\ngrid for processing matrices, and a 3-dimensional grid for processing the frames\nof a video. The GPU assigns each block within the grid a unique *block index*\nthat determines its position within the grid.\n\nSimilarly, you also must specify the number of threads per thread block and how\nthey are arranged across one, two, or three dimensions. The GPU also assigns\neach thread within a block a unique *thread index* that determines its position\nwithin the block. The combination of block index and thread index uniquely\nidentify the position of a thread within the overall grid.\n\nWhen a GPU assigns a thread block to execute on an SM, the SM divides the thread\nblock into subsets of threads called a *warp* (or *wavefront* on AMD GPUs). The\nsize of a warp depends on the GPU architecture, but most modern GPUs currently\nuse a warp size of 32 or 64 threads.", "position": 4, "token_count": 378, "has_code": false, "section_hierarchy": ["GPU execution model"], "metadata": {"chunk_id": "gpu-architecture-004", "document_id": "gpu-architecture", "position": 4, "token_count": 378, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["GPU execution model"], "file_path": "gpu/architecture.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/architecture", "title": "Intro to GPUs", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/architecture#gpu-execution-model"}}
{"chunk_id": "gpu-architecture-005", "document_id": "gpu-architecture", "content": "When a GPU assigns a thread block to execute on an SM, the SM divides the thread\nblock into subsets of threads called a *warp* (or *wavefront* on AMD GPUs). The\nsize of a warp depends on the GPU architecture, but most modern GPUs currently\nuse a warp size of 32 or 64 threads.\n\nIf a thread block contains a number of threads not evenly divisible by the warp\nsize, the SM creates a partially filled final warp that still consumes the full\nwarp's resources. For example, if a thread block has 100 threads and the warp\nsize is 32, the SM creates:\n\n- 3 full warps of 32 threads each (96 threads total).\n- 1 partial warp with only 4 active threads but still occupying a full warp's\n worth of resources (32 thread slots).\n\nThe SM effectively disables the unused thread slots in partial warps, but these\nslots still consume hardware resources. For this reason, you generally\nshould make thread block sizes a multiple of the warp size to optimize resource\nusage.\n\nEach thread in a warp executes the same instruction at the same time on\ndifferent data, following the *single instruction, multiple threads* (SIMT)\nexecution model. If threads within a warp take different execution paths (called\n*warp divergence*), the warp serially executes each branch path taken, disabling\nthreads that are not on that path. This means that optimal performance is\nachieved when all threads in a warp follow the same execution path.\n\nAn SM can actively manage multiple warps from different thread blocks\nsimultaneously, helping keep execution units busy. For example, the warp\nscheduler can quickly switch to another ready warp if the current warp's threads\nmust wait for memory access.\n\nWarps deliver several key performance advantages:", "position": 5, "token_count": 370, "has_code": false, "section_hierarchy": ["GPU execution model"], "metadata": {"chunk_id": "gpu-architecture-005", "document_id": "gpu-architecture", "position": 5, "token_count": 370, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["GPU execution model"], "file_path": "gpu/architecture.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/architecture", "title": "Intro to GPUs", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/architecture#gpu-execution-model"}}
{"chunk_id": "gpu-architecture-006", "document_id": "gpu-architecture", "content": "An SM can actively manage multiple warps from different thread blocks\nsimultaneously, helping keep execution units busy. For example, the warp\nscheduler can quickly switch to another ready warp if the current warp's threads\nmust wait for memory access.\n\nWarps deliver several key performance advantages:\n\n- The hardware needs to manage only warps instead of individual threads,\n reducing scheduling overhead.\n- Threads in a warp can access contiguous memory locations efficiently through\n memory coalescing.\n- The hardware automatically synchronizes threads within a warp, eliminating the\n need for explicit synchronization.\n- The warp scheduler can hide memory latency by switching between warps,\n maximizing compute resource utilization.\n\nSo far, we've explained some basic concepts about the GPU hardware and how the\nGPU executes parallel processes using threads, but without showing any code. To\nstart programming a GPU with Mojo, you can either read [GPU programming\nfundamentals](/mojo/manual/gpu/fundamentals) for an overview of the Mojo\nprogramming model, or jump right into a project with the tutorial to [Get\nstarted with GPU programming](/mojo/manual/gpu/intro-tutorial).", "position": 6, "token_count": 231, "has_code": false, "section_hierarchy": ["GPU execution model"], "metadata": {"chunk_id": "gpu-architecture-006", "document_id": "gpu-architecture", "position": 6, "token_count": 231, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["GPU execution model"], "file_path": "gpu/architecture.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/architecture", "title": "Intro to GPUs", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/architecture#gpu-execution-model"}}
