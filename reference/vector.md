# Comprehensive Guide to Creating Vector Embeddings from Technical Documentation

## 1. Introduction and System Overview

This guide provides a complete methodology for transforming technical documentation into searchable vector embeddings using the MAX serving framework, PostgreSQL with pgvector, and a hybrid search system. The approach is designed to be modular and repeatable across different documentation sources while maintaining high accuracy and search relevance.

### Core Architecture Components

The system consists of five main layers that work together to provide intelligent document retrieval:

1. **Document Processing Layer**: Handles MDX parsing, metadata extraction, and content preprocessing
2. **Chunking Layer**: Intelligently segments documents while preserving context
3. **Embedding Layer**: Transforms text chunks into semantic vectors using MAX's optimized models
4. **Storage Layer**: PostgreSQL with pgvector for efficient vector storage and retrieval
5. **Search Layer**: Hybrid search combining semantic and keyword-based retrieval

### Why This Architecture?

Studies show that recursive text splitting with minimal overlap delivers 30-50% higher retrieval precision versus naive fixed sizing while preserving decision-critical context. By combining this with hybrid search that merges BM25's precision with vector search's contextual understanding, we achieve both accuracy and semantic relevance.

## 2. Setting Up the Development Environment

### Initial Project Configuration with Pixi

Since Mojo and MAX integrate best with Pixi, we'll use it as our primary package manager:

```bash
# Initialize the project
pixi init mojo-docs-embeddings \
  -c https://conda.modular.com/max-nightly/ \
  -c conda-forge

cd mojo-docs-embeddings

# Add required packages
pixi add modular python=3.12 postgresql psycopg2-binary
pixi add nodejs  # For MDX processing tools

# Activate the environment
pixi shell
```

### Required Python Dependencies

Create a `pyproject.toml` for Python dependencies:

```toml
[project]
name = "doc-embeddings"
version = "0.1.0"
dependencies = [
    "psycopg[binary]>=3.1",
    "pgvector>=0.2.0",
    "pyyaml>=6.0",
    "python-frontmatter>=1.0",
    "tiktoken>=0.5.0",  # For accurate token counting
    "numpy>=1.24.0",
    "pandas>=2.0.0",
    "tqdm>=4.65.0",
    "langchain>=0.1.0",  # For chunking utilities
    "openai>=1.0.0",  # For API compatibility with MAX
]
```

## 3. Document Preprocessing Pipeline

### Understanding MDX Structure

MDX files combine Markdown with JSX components, requiring special handling. A typical Mojo documentation file might look like:

```mdx
---
title: "Advanced Memory Management in Mojo"
category: "performance"
tags: ["memory", "optimization", "advanced"]
last_updated: "2025-01-15"
difficulty: "advanced"
---

import CodeExample from '../components/CodeExample'

# Advanced Memory Management

Learn how Mojo's ownership system enables zero-copy operations...

<CodeExample language="mojo">
fn optimize_memory(data: Tensor) -> Tensor:
    # Implementation here
</CodeExample>
```

### Metadata Extraction Strategy

Create a robust metadata extractor that preserves document structure and relationships:

```python
import frontmatter
import os
from pathlib import Path
from typing import Dict, List, Optional
import hashlib
import re

class MDXProcessor:
    """Processes MDX files and extracts content with metadata."""
    
    def __init__(self, docs_path: str):
        self.docs_path = Path(docs_path)
        self.jsx_pattern = re.compile(r'<[^>]+>.*?</[^>]+>', re.DOTALL)
        self.import_pattern = re.compile(r'^import .+ from .+$', re.MULTILINE)
    
    def extract_document_metadata(self, file_path: Path) -> Dict:
        """Extract frontmatter and compute additional metadata."""
        with open(file_path, 'r', encoding='utf-8') as f:
            post = frontmatter.load(f)
        
        # Calculate content hash for change detection
        content_hash = hashlib.md5(post.content.encode()).hexdigest()
        
        # Extract document structure
        headers = self._extract_headers(post.content)
        code_blocks = self._extract_code_blocks(post.content)
        
        return {
            'file_path': str(file_path.relative_to(self.docs_path)),
            'url': self._generate_url(file_path),
            'title': post.metadata.get('title', ''),
            'category': post.metadata.get('category', 'general'),
            'tags': post.metadata.get('tags', []),
            'difficulty': post.metadata.get('difficulty', 'intermediate'),
            'last_updated': post.metadata.get('last_updated'),
            'content_hash': content_hash,
            'headers': headers,
            'has_code_examples': len(code_blocks) > 0,
            'raw_content': post.content,
            'clean_content': self._clean_content(post.content)
        }
    
    def _clean_content(self, content: str) -> str:
        """Remove JSX components and imports while preserving text."""
        # Remove imports
        content = self.import_pattern.sub('', content)
        
        # Replace JSX components with placeholders or extract their content
        content = self.jsx_pattern.sub('[Component Content]', content)
        
        # Normalize whitespace
        content = re.sub(r'\n{3,}', '\n\n', content)
        
        return content.strip()
    
    def _extract_headers(self, content: str) -> List[Dict]:
        """Extract document structure from headers."""
        headers = []
        for match in re.finditer(r'^(#{1,6})\s+(.+)$', content, re.MULTILINE):
            level = len(match.group(1))
            text = match.group(2)
            headers.append({
                'level': level,
                'text': text,
                'position': match.start()
            })
        return headers
    
    def _generate_url(self, file_path: Path) -> str:
        """Generate the documentation URL from file path."""
        relative_path = file_path.relative_to(self.docs_path)
        url_path = str(relative_path).replace('.mdx', '').replace('\\', '/')
        return f"https://docs.modular.com/mojo/manual/{url_path}"
```

## 4. Advanced Chunking Strategies

### Implementing Recursive Text Splitting

Research demonstrates that recursive strategies with token windows of 100 tokens and 20% overlap yield the highest F2 scores for technical content retrieval. Here's an implementation optimized for technical documentation:

```python
from typing import List, Dict, Optional
from dataclasses import dataclass
import tiktoken

@dataclass
class DocumentChunk:
    """Represents a chunk of document with metadata."""
    content: str
    metadata: Dict
    chunk_id: str
    parent_doc_id: str
    position: int
    token_count: int
    overlap_with_previous: bool
    section_hierarchy: List[str]

class TechnicalDocChunker:
    """Specialized chunker for technical documentation."""
    
    def __init__(
        self,
        chunk_size: int = 400,  # Optimal for all-mpnet-base-v2
        chunk_overlap: int = 80,  # 20% overlap
        min_chunk_size: int = 100,
        preserve_code_blocks: bool = True
    ):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.min_chunk_size = min_chunk_size
        self.preserve_code_blocks = preserve_code_blocks
        
        # Initialize tokenizer for accurate counting
        self.tokenizer = tiktoken.get_encoding("cl100k_base")
    
    def chunk_document(self, doc: Dict) -> List[DocumentChunk]:
        """
        Chunk a document using recursive splitting with semantic boundaries.
        
        This approach:
        1. Preserves section boundaries when possible
        2. Keeps code blocks intact
        3. Maintains context through overlap
        4. Adds hierarchical metadata for better retrieval
        """
        chunks = []
        content = doc['clean_content']
        headers = doc['headers']
        
        # Split by major sections first
        sections = self._split_by_headers(content, headers)
        
        for section_idx, section in enumerate(sections):
            section_chunks = self._chunk_section(
                section['content'],
                section['hierarchy'],
                preserve_code=self.preserve_code_blocks
            )
            
            for chunk_idx, chunk_content in enumerate(section_chunks):
                chunk = DocumentChunk(
                    content=chunk_content,
                    metadata={
                        'source_file': doc['file_path'],
                        'url': doc['url'],
                        'title': doc['title'],
                        'category': doc['category'],
                        'tags': doc['tags'],
                        'difficulty': doc['difficulty'],
                        'section': section['hierarchy'][-1] if section['hierarchy'] else '',
                        'section_path': ' > '.join(section['hierarchy'])
                    },
                    chunk_id=f"{doc['content_hash']}_{section_idx}_{chunk_idx}",
                    parent_doc_id=doc['content_hash'],
                    position=section_idx * 1000 + chunk_idx,
                    token_count=self._count_tokens(chunk_content),
                    overlap_with_previous=chunk_idx > 0,
                    section_hierarchy=section['hierarchy']
                )
                chunks.append(chunk)
        
        return chunks
    
    def _chunk_section(
        self, 
        text: str, 
        hierarchy: List[str],
        preserve_code: bool = True
    ) -> List[str]:
        """
        Recursively chunk a section of text.
        
        Order of splitting:
        1. Paragraphs (double newline)
        2. Sentences
        3. Words (last resort)
        """
        if self._count_tokens(text) <= self.chunk_size:
            return [text] if text.strip() else []
        
        # Try splitting by paragraphs first
        paragraphs = text.split('\n\n')
        if len(paragraphs) > 1:
            return self._merge_chunks(paragraphs, hierarchy)
        
        # Then try sentences
        sentences = self._split_sentences(text)
        if len(sentences) > 1:
            return self._merge_chunks(sentences, hierarchy)
        
        # Last resort: split by tokens
        return self._split_by_tokens(text)
    
    def _merge_chunks(
        self, 
        segments: List[str], 
        hierarchy: List[str]
    ) -> List[str]:
        """
        Merge segments into chunks of appropriate size with overlap.
        """
        chunks = []
        current_chunk = []
        current_tokens = 0
        
        for segment in segments:
            segment_tokens = self._count_tokens(segment)
            
            if current_tokens + segment_tokens > self.chunk_size:
                if current_chunk:
                    # Add the current chunk
                    chunks.append('\n\n'.join(current_chunk))
                    
                    # Start new chunk with overlap
                    if self.chunk_overlap > 0 and len(current_chunk) > 1:
                        # Include last segment(s) for overlap
                        overlap_segments = []
                        overlap_tokens = 0
                        
                        for seg in reversed(current_chunk):
                            seg_tokens = self._count_tokens(seg)
                            if overlap_tokens + seg_tokens <= self.chunk_overlap:
                                overlap_segments.insert(0, seg)
                                overlap_tokens += seg_tokens
                            else:
                                break
                        
                        current_chunk = overlap_segments + [segment]
                        current_tokens = overlap_tokens + segment_tokens
                    else:
                        current_chunk = [segment]
                        current_tokens = segment_tokens
                else:
                    # Single segment exceeds chunk size
                    if segment_tokens > self.chunk_size:
                        # Recursively chunk this segment
                        sub_chunks = self._chunk_section(segment, hierarchy, False)
                        chunks.extend(sub_chunks)
                    else:
                        current_chunk = [segment]
                        current_tokens = segment_tokens
            else:
                current_chunk.append(segment)
                current_tokens += segment_tokens
        
        if current_chunk:
            chunks.append('\n\n'.join(current_chunk))
        
        return chunks
    
    def _count_tokens(self, text: str) -> int:
        """Count tokens using tiktoken for accuracy."""
        return len(self.tokenizer.encode(text))
    
    def _split_sentences(self, text: str) -> List[str]:
        """Split text into sentences while preserving code blocks."""
        # Simple sentence splitter - can be enhanced with spaCy or NLTK
        sentences = re.split(r'(?<=[.!?])\s+', text)
        return [s.strip() for s in sentences if s.strip()]
```

## 5. Embedding Generation with MAX

### Setting Up MAX Serving Infrastructure

The MAX framework provides an optimized implementation of sentence-transformers/all-mpnet-base-v2, which generates 768-dimensional embeddings with a recommended sequence length of 384 tokens.

```python
from openai import OpenAI
import numpy as np
from typing import List, Dict
import asyncio
from concurrent.futures import ThreadPoolExecutor
import time

class MAXEmbeddingGenerator:
    """
    Generate embeddings using MAX serving framework.
    
    Key considerations:
    - all-mpnet-base-v2 has 384 token limit (can extend to 512 with degradation)
    - Outputs 768-dimensional vectors
    - Optimized for sentence and short paragraph encoding
    """
    
    def __init__(
        self,
        max_url: str = "http://localhost:8000",
        model: str = "sentence-transformers/all-mpnet-base-v2",
        batch_size: int = 32,
        max_retries: int = 3
    ):
        self.client = OpenAI(
            base_url=f"{max_url}/v1",
            api_key="EMPTY"  # MAX doesn't require API key
        )
        self.model = model
        self.batch_size = batch_size
        self.max_retries = max_retries
        self.dimensions = 768
        
    def generate_embeddings(
        self, 
        chunks: List[DocumentChunk],
        show_progress: bool = True
    ) -> List[Dict]:
        """
        Generate embeddings for document chunks in batches.
        
        Returns list of dicts with chunk data and embeddings.
        """
        results = []
        
        # Process in batches for efficiency
        from tqdm import tqdm
        iterator = range(0, len(chunks), self.batch_size)
        if show_progress:
            iterator = tqdm(iterator, desc="Generating embeddings")
        
        for i in iterator:
            batch = chunks[i:i + self.batch_size]
            batch_texts = [chunk.content for chunk in batch]
            
            # Generate embeddings with retry logic
            embeddings = self._generate_with_retry(batch_texts)
            
            # Combine chunks with embeddings
            for chunk, embedding in zip(batch, embeddings):
                results.append({
                    'chunk_id': chunk.chunk_id,
                    'content': chunk.content,
                    'embedding': embedding,
                    'metadata': chunk.metadata,
                    'token_count': chunk.token_count,
                    'position': chunk.position,
                    'parent_doc_id': chunk.parent_doc_id
                })
        
        return results
    
    def _generate_with_retry(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings with exponential backoff retry."""
        for attempt in range(self.max_retries):
            try:
                response = self.client.embeddings.create(
                    model=self.model,
                    input=texts
                )
                
                # Extract embeddings from response
                embeddings = [item.embedding for item in response.data]
                
                # Normalize embeddings for better similarity search
                embeddings = [self._normalize_vector(emb) for emb in embeddings]
                
                return embeddings
                
            except Exception as e:
                if attempt < self.max_retries - 1:
                    wait_time = 2 ** attempt
                    print(f"Retry {attempt + 1} after {wait_time}s: {e}")
                    time.sleep(wait_time)
                else:
                    raise e
    
    def _normalize_vector(self, vector: List[float]) -> List[float]:
        """
        Normalize vector to unit length for cosine similarity.
        
        This ensures all vectors have magnitude 1, making cosine
        similarity calculations more efficient in the database.
        """
        v = np.array(vector)
        norm = np.linalg.norm(v)
        if norm == 0:
            return vector
        return (v / norm).tolist()
```

### Starting the MAX Server

Create a startup script for the MAX serving infrastructure:

```bash
#!/bin/bash
# start_max_server.sh

echo "Starting MAX embedding server..."

# Start MAX serve with the embedding model
max serve \
  --model sentence-transformers/all-mpnet-base-v2 \
  --host 0.0.0.0 \
  --port 8000 \
  --max-batch-size 32 \
  --num-workers 4

echo "MAX server ready at http://localhost:8000"
```

## 6. Database Design and Storage

### PostgreSQL Schema with pgvector

Design a schema that supports both vector similarity and keyword search efficiently:

```sql
-- Enable required extensions
CREATE EXTENSION IF NOT EXISTS vector;
CREATE EXTENSION IF NOT EXISTS pg_trgm;  -- For fuzzy text search
CREATE EXTENSION IF NOT EXISTS btree_gin;  -- For compound indexes

-- Main documents table
CREATE TABLE documents (
    id SERIAL PRIMARY KEY,
    file_path TEXT UNIQUE NOT NULL,
    url TEXT,
    title TEXT NOT NULL,
    category VARCHAR(100),
    tags TEXT[],
    difficulty VARCHAR(20),
    content_hash VARCHAR(32) UNIQUE NOT NULL,
    last_updated TIMESTAMP,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    
    -- Indexes for filtering
    INDEX idx_documents_category (category),
    INDEX idx_documents_tags USING GIN (tags),
    INDEX idx_documents_difficulty (difficulty)
);

-- Document chunks table with embeddings
CREATE TABLE document_chunks (
    id SERIAL PRIMARY KEY,
    chunk_id VARCHAR(100) UNIQUE NOT NULL,
    document_id INTEGER REFERENCES documents(id) ON DELETE CASCADE,
    content TEXT NOT NULL,
    embedding vector(768) NOT NULL,  -- all-mpnet-base-v2 dimensions
    
    -- Metadata
    section TEXT,
    section_path TEXT,
    position INTEGER NOT NULL,
    token_count INTEGER NOT NULL,
    
    -- Full-text search
    content_tsvector tsvector GENERATED ALWAYS AS (to_tsvector('english', content)) STORED,
    
    -- Timestamps
    created_at TIMESTAMP DEFAULT NOW(),
    
    -- Indexes
    INDEX idx_chunks_document_id (document_id),
    INDEX idx_chunks_position (document_id, position),
    INDEX idx_chunks_content_fts USING GIN (content_tsvector),
    INDEX idx_chunks_content_trigram USING GIN (content gin_trgm_ops)
);

-- Create HNSW index for vector similarity search
-- HNSW provides better recall than IVFFlat for our use case
CREATE INDEX idx_chunks_embedding_hnsw ON document_chunks 
USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);

-- Table for tracking document updates
CREATE TABLE document_versions (
    id SERIAL PRIMARY KEY,
    document_id INTEGER REFERENCES documents(id) ON DELETE CASCADE,
    content_hash VARCHAR(32) NOT NULL,
    change_type VARCHAR(20) NOT NULL, -- 'created', 'updated', 'deleted'
    changed_at TIMESTAMP DEFAULT NOW()
);

-- Search history for analytics and improvement
CREATE TABLE search_queries (
    id SERIAL PRIMARY KEY,
    query_text TEXT NOT NULL,
    query_embedding vector(768),
    search_type VARCHAR(20), -- 'semantic', 'keyword', 'hybrid'
    result_count INTEGER,
    top_results JSONB,
    user_feedback INTEGER, -- 1-5 rating
    executed_at TIMESTAMP DEFAULT NOW()
);
```

### Database Access Layer

Implement a robust database layer for storing and retrieving embeddings:

```python
import psycopg
from psycopg.rows import dict_row
from pgvector.psycopg import register_vector
import json
from typing import List, Dict, Optional, Tuple

class VectorDatabase:
    """
    PostgreSQL + pgvector database interface for embeddings.
    
    Handles both vector similarity and text search operations.
    """
    
    def __init__(self, connection_string: str):
        self.connection_string = connection_string
        self._init_connection()
    
    def _init_connection(self):
        """Initialize database connection with pgvector support."""
        self.conn = psycopg.connect(
            self.connection_string,
            row_factory=dict_row
        )
        register_vector(self.conn)
    
    def store_document_with_chunks(
        self,
        doc_metadata: Dict,
        chunks_with_embeddings: List[Dict]
    ) -> int:
        """
        Store a document and its chunks atomically.
        
        Uses a transaction to ensure consistency between
        document and chunk storage.
        """
        with self.conn.transaction():
            # Check if document exists
            existing = self.conn.execute(
                "SELECT id FROM documents WHERE content_hash = %s",
                (doc_metadata['content_hash'],)
            ).fetchone()
            
            if existing:
                # Update existing document
                doc_id = existing['id']
                self.conn.execute("""
                    UPDATE documents 
                    SET title = %s, category = %s, tags = %s,
                        difficulty = %s, last_updated = %s,
                        updated_at = NOW()
                    WHERE id = %s
                """, (
                    doc_metadata['title'],
                    doc_metadata['category'],
                    doc_metadata['tags'],
                    doc_metadata['difficulty'],
                    doc_metadata['last_updated'],
                    doc_id
                ))
                
                # Delete old chunks
                self.conn.execute(
                    "DELETE FROM document_chunks WHERE document_id = %s",
                    (doc_id,)
                )
            else:
                # Insert new document
                result = self.conn.execute("""
                    INSERT INTO documents 
                    (file_path, url, title, category, tags, difficulty, 
                     content_hash, last_updated)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                    RETURNING id
                """, (
                    doc_metadata['file_path'],
                    doc_metadata['url'],
                    doc_metadata['title'],
                    doc_metadata['category'],
                    doc_metadata['tags'],
                    doc_metadata['difficulty'],
                    doc_metadata['content_hash'],
                    doc_metadata['last_updated']
                )).fetchone()
                doc_id = result['id']
            
            # Insert chunks with embeddings
            for chunk in chunks_with_embeddings:
                self.conn.execute("""
                    INSERT INTO document_chunks
                    (chunk_id, document_id, content, embedding,
                     section, section_path, position, token_count)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                """, (
                    chunk['chunk_id'],
                    doc_id,
                    chunk['content'],
                    chunk['embedding'],
                    chunk['metadata'].get('section', ''),
                    chunk['metadata'].get('section_path', ''),
                    chunk['position'],
                    chunk['token_count']
                ))
            
            self.conn.commit()
            return doc_id
```

## 7. Hybrid Search Implementation

### Combining Semantic and Keyword Search

Hybrid retrieval combines lexical (BM25/keyword) and semantic (vector) scores, with Reciprocal Rank Fusion (RRF) being an effective method for merging results:

```python
from typing import List, Dict, Optional, Tuple
import numpy as np

class HybridSearchEngine:
    """
    Implements hybrid search combining vector similarity and text search.
    
    Uses Reciprocal Rank Fusion (RRF) to merge results from both methods,
    providing better recall and precision than either method alone.
    """
    
    def __init__(
        self,
        db: VectorDatabase,
        embedding_generator: MAXEmbeddingGenerator,
        semantic_weight: float = 0.6,
        keyword_weight: float = 0.4,
        rrf_k: int = 60
    ):
        self.db = db
        self.embedding_generator = embedding_generator
        self.semantic_weight = semantic_weight
        self.keyword_weight = keyword_weight
        self.rrf_k = rrf_k
    
    def search(
        self,
        query: str,
        top_k: int = 10,
        filters: Optional[Dict] = None,
        rerank: bool = True
    ) -> List[Dict]:
        """
        Perform hybrid search on the document collection.
        
        Steps:
        1. Generate query embedding
        2. Perform vector similarity search
        3. Perform keyword search (FTS + trigram)
        4. Merge results using RRF
        5. Optional: Re-rank using cross-encoder
        """
        
        # Generate query embedding
        query_embedding = self.embedding_generator.generate_embeddings(
            [DocumentChunk(content=query, metadata={}, chunk_id="query",
                         parent_doc_id="", position=0, token_count=0,
                         overlap_with_previous=False, section_hierarchy=[])],
            show_progress=False
        )[0]['embedding']
        
        # Perform semantic search
        semantic_results = self._semantic_search(
            query_embedding, 
            top_k=top_k * 2,  # Fetch more for fusion
            filters=filters
        )
        
        # Perform keyword search
        keyword_results = self._keyword_search(
            query,
            top_k=top_k * 2,
            filters=filters
        )
        
        # Merge results using RRF
        merged_results = self._reciprocal_rank_fusion(
            semantic_results,
            keyword_results,
            k=self.rrf_k
        )
        
        # Optional re-ranking step
        if rerank and len(merged_results) > 0:
            merged_results = self._rerank_results(query, merged_results)
        
        # Return top-k results
        return merged_results[:top_k]
    
    def _semantic_search(
        self,
        query_embedding: List[float],
        top_k: int,
        filters: Optional[Dict] = None
    ) -> List[Dict]:
        """
        Perform vector similarity search using cosine similarity.
        """
        # Build the base query
        query = """
            SELECT 
                c.id,
                c.chunk_id,
                c.content,
                c.section,
                c.section_path,
                d.title,
                d.url,
                d.category,
                d.tags,
                1 - (c.embedding <=> %s::vector) as similarity,
                'semantic' as search_method
            FROM document_chunks c
            JOIN documents d ON c.document_id = d.id
            WHERE 1=1
        """
        
        params = [query_embedding]
        
        # Add filters if provided
        if filters:
            if 'category' in filters:
                query += " AND d.category = %s"
                params.append(filters['category'])
            if 'tags' in filters and filters['tags']:
                query += " AND d.tags && %s::text[]"
                params.append(filters['tags'])
            if 'difficulty' in filters:
                query += " AND d.difficulty = %s"
                params.append(filters['difficulty'])
        
        query += """
            ORDER BY c.embedding <=> %s::vector
            LIMIT %s
        """
        params.extend([query_embedding, top_k])
        
        results = self.db.conn.execute(query, params).fetchall()
        
        # Add rank for RRF
        for i, result in enumerate(results):
            result['rank'] = i + 1
        
        return results
    
    def _keyword_search(
        self,
        query_text: str,
        top_k: int,
        filters: Optional[Dict] = None
    ) -> List[Dict]:
        """
        Perform keyword search using PostgreSQL full-text search.
        
        Combines FTS with trigram similarity for better fuzzy matching.
        """
        # Prepare query for FTS
        tsquery = ' & '.join(query_text.split())
        
        query = """
            SELECT 
                c.id,
                c.chunk_id,
                c.content,
                c.section,
                c.section_path,
                d.title,
                d.url,
                d.category,
                d.tags,
                ts_rank(c.content_tsvector, plainto_tsquery('english', %s)) * 0.7 +
                similarity(c.content, %s) * 0.3 as relevance,
                'keyword' as search_method
            FROM document_chunks c
            JOIN documents d ON c.document_id = d.id
            WHERE 
                c.content_tsvector @@ plainto_tsquery('english', %s)
                OR similarity(c.content, %s) > 0.2
        """
        
        params = [query_text, query_text, query_text, query_text]
        
        # Add filters
        if filters:
            if 'category' in filters:
                query += " AND d.category = %s"
                params.append(filters['category'])
            if 'tags' in filters and filters['tags']:
                query += " AND d.tags && %s::text[]"
                params.append(filters['tags'])
        
        query += """
            ORDER BY relevance DESC
            LIMIT %s
        """
        params.append(top_k)
        
        results = self.db.conn.execute(query, params).fetchall()
        
        # Add rank for RRF
        for i, result in enumerate(results):
            result['rank'] = i + 1
        
        return results
    
    def _reciprocal_rank_fusion(
        self,
        semantic_results: List[Dict],
        keyword_results: List[Dict],
        k: int = 60
    ) -> List[Dict]:
        """
        Merge results using Reciprocal Rank Fusion.
        
        RRF score = Î£(1 / (k + rank_i))
        
        This method is robust and doesn't require score normalization.
        """
        # Create a dictionary to store combined scores
        chunk_scores = {}
        chunk_data = {}
        
        # Process semantic results
        for result in semantic_results:
            chunk_id = result['chunk_id']
            rrf_score = self.semantic_weight * (1.0 / (k + result['rank']))
            
            if chunk_id not in chunk_scores:
                chunk_scores[chunk_id] = 0
                chunk_data[chunk_id] = result
            
            chunk_scores[chunk_id] += rrf_score
            chunk_data[chunk_id]['semantic_rank'] = result['rank']
            chunk_data[chunk_id]['semantic_score'] = result.get('similarity', 0)
        
        # Process keyword results
        for result in keyword_results:
            chunk_id = result['chunk_id']
            rrf_score = self.keyword_weight * (1.0 / (k + result['rank']))
            
            if chunk_id not in chunk_scores:
                chunk_scores[chunk_id] = 0
                chunk_data[chunk_id] = result
            
            chunk_scores[chunk_id] += rrf_score
            chunk_data[chunk_id]['keyword_rank'] = result['rank']
            chunk_data[chunk_id]['keyword_score'] = result.get('relevance', 0)
        
        # Sort by combined RRF score
        sorted_chunks = sorted(
            chunk_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )
        
        # Prepare final results
        results = []
        for chunk_id, rrf_score in sorted_chunks:
            result = chunk_data[chunk_id]
            result['rrf_score'] = rrf_score
            result['hybrid_rank'] = len(results) + 1
            results.append(result)
        
        return results
    
    def _rerank_results(
        self,
        query: str,
        results: List[Dict],
        model: str = "cross-encoder/ms-marco-MiniLM-L-6-v2"
    ) -> List[Dict]:
        """
        Optional: Re-rank results using a cross-encoder model.
        
        Cross-encoders provide more accurate relevance scores but
        are computationally expensive, so we only use them on the
        top results from the initial search.
        """
        # This would require additional model setup
        # Placeholder for cross-encoder reranking
        return results
```

## 8. Complete Processing Pipeline

### Main Orchestration Script

Bring everything together in a main processing script:

```python
import asyncio
from pathlib import Path
from typing import List, Dict
import json
import logging
from datetime import datetime

class DocumentProcessingPipeline:
    """
    Complete pipeline for processing documentation into searchable embeddings.
    
    Modular design allows easy adaptation to different documentation sources.
    """
    
    def __init__(
        self,
        docs_path: str,
        db_connection: str,
        max_url: str = "http://localhost:8000",
        chunk_size: int = 400,
        chunk_overlap: int = 80
    ):
        # Initialize components
        self.processor = MDXProcessor(docs_path)
        self.chunker = TechnicalDocChunker(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        self.embedding_gen = MAXEmbeddingGenerator(max_url=max_url)
        self.db = VectorDatabase(db_connection)
        
        # Setup logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
    
    def process_all_documents(self, force_update: bool = False):
        """
        Process all documents in the documentation directory.
        
        Args:
            force_update: If True, reprocess all documents regardless of changes
        """
        docs_path = Path(self.processor.docs_path)
        mdx_files = list(docs_path.rglob("*.mdx"))
        
        self.logger.info(f"Found {len(mdx_files)} MDX files to process")
        
        # Track processing statistics
        stats = {
            'total_files': len(mdx_files),
            'processed': 0,
            'skipped': 0,
            'errors': 0,
            'total_chunks': 0,
            'start_time': datetime.now()
        }
        
        for file_path in mdx_files:
            try:
                # Extract metadata
                doc_metadata = self.processor.extract_document_metadata(file_path)
                
                # Check if document needs processing
                if not force_update:
                    existing = self.db.conn.execute(
                        "SELECT content_hash FROM documents WHERE file_path = %s",
                        (doc_metadata['file_path'],)
                    ).fetchone()
                    
                    if existing and existing['content_hash'] == doc_metadata['content_hash']:
                        self.logger.info(f"Skipping unchanged: {file_path.name}")
                        stats['skipped'] += 1
                        continue
                
                self.logger.info(f"Processing: {file_path.name}")
                
                # Create chunks
                chunks = self.chunker.chunk_document(doc_metadata)
                self.logger.info(f"  Created {len(chunks)} chunks")
                
                # Generate embeddings
                chunks_with_embeddings = self.embedding_gen.generate_embeddings(
                    chunks, 
                    show_progress=False
                )
                
                # Store in database
                doc_id = self.db.store_document_with_chunks(
                    doc_metadata,
                    chunks_with_embeddings
                )
                
                self.logger.info(f"  Stored document ID: {doc_id}")
                stats['processed'] += 1
                stats['total_chunks'] += len(chunks)
                
            except Exception as e:
                self.logger.error(f"Error processing {file_path}: {e}")
                stats['errors'] += 1
        
        # Calculate and display statistics
        stats['end_time'] = datetime.now()
        stats['duration'] = (stats['end_time'] - stats['start_time']).total_seconds()
        
        self.logger.info("\n" + "="*50)
        self.logger.info("Processing Complete!")
        self.logger.info(f"Total files: {stats['total_files']}")
        self.logger.info(f"Processed: {stats['processed']}")
        self.logger.info(f"Skipped: {stats['skipped']}")
        self.logger.info(f"Errors: {stats['errors']}")
        self.logger.info(f"Total chunks created: {stats['total_chunks']}")
        self.logger.info(f"Duration: {stats['duration']:.2f} seconds")
        
        # Save statistics to file
        with open('processing_stats.json', 'w') as f:
            json.dump(stats, f, indent=2, default=str)
    
    def validate_embeddings(self, sample_size: int = 10):
        """
        Validate that embeddings are properly stored and searchable.
        """
        self.logger.info("Validating embeddings...")
        
        # Get sample chunks
        sample_chunks = self.db.conn.execute("""
            SELECT chunk_id, content, embedding
            FROM document_chunks
            ORDER BY RANDOM()
            LIMIT %s
        """, (sample_size,)).fetchall()
        
        for chunk in sample_chunks:
            # Verify embedding dimensions
            embedding = chunk['embedding']
            if len(embedding) != 768:
                self.logger.error(f"Invalid embedding dimension for {chunk['chunk_id']}: {len(embedding)}")
                continue
            
            # Test similarity search
            similar = self.db.conn.execute("""
                SELECT chunk_id, 1 - (embedding <=> %s::vector) as similarity
                FROM document_chunks
                WHERE chunk_id != %s
                ORDER BY embedding <=> %s::vector
                LIMIT 5
            """, (embedding, chunk['chunk_id'], embedding)).fetchall()
            
            self.logger.info(f"Chunk {chunk['chunk_id'][:20]}... has {len(similar)} similar chunks")
        
        self.logger.info("Validation complete!")

# Main execution
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Process documentation for embeddings")
    parser.add_argument(
        "--docs-path",
        default="/home/james/mojo/docs/manual",
        help="Path to documentation files"
    )
    parser.add_argument(
        "--db-connection",
        default="postgresql://user:password@localhost/mojo_docs",
        help="PostgreSQL connection string"
    )
    parser.add_argument(
        "--max-url",
        default="http://localhost:8000",
        help="MAX serving URL"
    )
    parser.add_argument(
        "--force-update",
        action="store_true",
        help="Force reprocessing of all documents"
    )
    parser.add_argument(
        "--validate",
        action="store_true",
        help="Run validation after processing"
    )
    
    args = parser.parse_args()
    
    # Initialize and run pipeline
    pipeline = DocumentProcessingPipeline(
        docs_path=args.docs_path,
        db_connection=args.db_connection,
        max_url=args.max_url
    )
    
    # Process documents
    pipeline.process_all_documents(force_update=args.force_update)
    
    # Optional validation
    if args.validate:
        pipeline.validate_embeddings()
```

## 9. Configuration Management

### Modular Configuration System

Create a flexible configuration system for different documentation sources:

```yaml
# config.yaml
documentation_sources:
  mojo:
    path: "/home/james/mojo/docs/manual"
    url_base: "https://github.com/modular/modular/tree/main/mojo/docs/manual"
    chunking:
      strategy: "recursive"
      chunk_size: 400
      chunk_overlap: 80
      preserve_code_blocks: true
    metadata:
      extract_headers: true
      extract_code_examples: true
      custom_extractors:
        - "difficulty_level"
        - "api_version"

embedding:
  model: "sentence-transformers/all-mpnet-base-v2"
  dimensions: 768
  max_tokens: 384
  batch_size: 32
  normalize: true

database:
  host: "localhost"
  port: 5432
  database: "documentation_embeddings"
  user: "doc_user"
  vector_index:
    type: "hnsw"
    m: 16
    ef_construction: 64

search:
  hybrid:
    semantic_weight: 0.6
    keyword_weight: 0.4
    rrf_k: 60
  reranking:
    enabled: false
    model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
  result_limit: 20
```

### Configuration Loader

```python
import yaml
from dataclasses import dataclass
from typing import Dict, List, Optional

@dataclass
class ChunkingConfig:
    strategy: str
    chunk_size: int
    chunk_overlap: int
    preserve_code_blocks: bool

@dataclass
class DocumentationSourceConfig:
    path: str
    url_base: str
    chunking: ChunkingConfig
    metadata: Dict

class ConfigurationManager:
    """Manages configuration for the documentation processing pipeline."""
    
    def __init__(self, config_path: str = "config.yaml"):
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
    
    def get_source_config(self, source_name: str) -> DocumentationSourceConfig:
        """Get configuration for a specific documentation source."""
        source_cfg = self.config['documentation_sources'][source_name]
        return DocumentationSourceConfig(
            path=source_cfg['path'],
            url_base=source_cfg['url_base'],
            chunking=ChunkingConfig(**source_cfg['chunking']),
            metadata=source_cfg['metadata']
        )
    
    def get_database_connection_string(self) -> str:
        """Build PostgreSQL connection string from config."""
        db = self.config['database']
        return f"postgresql://{db['user']}@{db['host']}:{db['port']}/{db['database']}"
```

## 10. Best Practices and Optimization

### Key Recommendations

**1. Chunk Size Optimization**

The optimal chunk size depends on balancing model context windows with information granularity - chunks that are too small lose context while chunks that are too large dilute specific information during retrieval. For technical documentation with code examples, use 400-500 tokens with 20% overlap.

**2. Metadata Preservation**

Always preserve document structure and hierarchy in metadata. This enables filtered searches and helps maintain context when presenting results to users.

**3. Incremental Processing**

Implement content hashing to detect changes and only reprocess modified documents. This significantly reduces processing time for large documentation sets.

**4. Vector Index Selection**

HNSW indexes provide better recall than IVFFlat for most documentation use cases, though they require more memory. For collections under 1 million chunks, HNSW is recommended.

**5. Hybrid Search Tuning**

Start with a 60/40 split favoring semantic search, but tune based on your specific content. Technical documentation with specific terminology may benefit from higher keyword weight.

### Performance Monitoring

Implement comprehensive monitoring to track system performance:

```python
class PerformanceMonitor:
    """Monitor and log performance metrics for the embedding system."""
    
    def __init__(self, db: VectorDatabase):
        self.db = db
    
    def log_search_performance(
        self,
        query: str,
        search_type: str,
        result_count: int,
        latency_ms: float,
        top_results: List[Dict]
    ):
        """Log search query performance for analysis."""
        self.db.conn.execute("""
            INSERT INTO search_queries 
            (query_text, search_type, result_count, latency_ms, top_results, executed_at)
            VALUES (%s, %s, %s, %s, %s, NOW())
        """, (
            query,
            search_type,
            result_count,
            latency_ms,
            json.dumps([{'id': r['chunk_id'], 'score': r.get('rrf_score', 0)} 
                       for r in top_results[:5]])
        ))
    
    def analyze_search_patterns(self) -> Dict:
        """Analyze search patterns to identify optimization opportunities."""
        # Query analysis
        patterns = self.db.conn.execute("""
            SELECT 
                search_type,
                AVG(result_count) as avg_results,
                AVG(latency_ms) as avg_latency,
                COUNT(*) as query_count
            FROM search_queries
            WHERE executed_at > NOW() - INTERVAL '7 days'
            GROUP BY search_type
        """).fetchall()
        
        return {
            'search_patterns': patterns,
            'recommendations': self._generate_recommendations(patterns)
        }
    
    def _generate_recommendations(self, patterns: List[Dict]) -> List[str]:
        """Generate optimization recommendations based on patterns."""
        recommendations = []
        
        for pattern in patterns:
            if pattern['avg_latency'] > 100:
                recommendations.append(
                    f"Consider optimizing {pattern['search_type']} search - "
                    f"average latency is {pattern['avg_latency']:.2f}ms"
                )
            if pattern['avg_results'] < 5:
                recommendations.append(
                    f"Low result count for {pattern['search_type']} - "
                    "consider adjusting similarity thresholds"
                )
        
        return recommendations
```

## 11. Testing and Validation

### Comprehensive Test Suite

Create tests to ensure system reliability:

```python
import unittest
from typing import List
import tempfile

class EmbeddingSystemTests(unittest.TestCase):
    """Test suite for the documentation embedding system."""
    
    def setUp(self):
        """Setup test environment."""
        self.test_docs = [
            {
                'content': "Mojo provides zero-cost abstractions for systems programming.",
                'title': "Test Doc 1"
            },
            {
                'content': "The ownership system in Mojo prevents memory leaks.",
                'title': "Test Doc 2"
            }
        ]
        
        # Initialize test components
        self.chunker = TechnicalDocChunker(chunk_size=50, chunk_overlap=10)
        self.temp_db = self._create_temp_database()
    
    def test_chunking_preserves_content(self):
        """Test that chunking doesn't lose content."""
        doc = {'clean_content': self.test_docs[0]['content'], 'headers': []}
        chunks = self.chunker.chunk_document(doc)
        
        # Reconstruct content from chunks (accounting for overlap)
        reconstructed = ' '.join([c.content for c in chunks])
        
        # Check that all original words are present
        original_words = set(doc['clean_content'].split())
        reconstructed_words = set(reconstructed.split())
        
        self.assertTrue(original_words.issubset(reconstructed_words))
    
    def test_embedding_dimensions(self):
        """Test that embeddings have correct dimensions."""
        # Mock embedding generation
        embedding = [0.1] * 768  # all-mpnet-base-v2 dimensions
        self.assertEqual(len(embedding), 768)
    
    def test_semantic_search_relevance(self):
        """Test that semantic search returns relevant results."""
        # This would test with actual embeddings
        # Checking that similar content ranks higher
        pass
    
    def test_hybrid_search_combination(self):
        """Test that hybrid search properly combines results."""
        semantic_results = [
            {'chunk_id': '1', 'rank': 1},
            {'chunk_id': '2', 'rank': 2}
        ]
        keyword_results = [
            {'chunk_id': '2', 'rank': 1},
            {'chunk_id': '3', 'rank': 2}
        ]
        
        # Test RRF fusion
        engine = HybridSearchEngine(None, None)
        merged = engine._reciprocal_rank_fusion(
            semantic_results, 
            keyword_results
        )
        
        # Chunk 2 should rank highest (appears in both)
        self.assertEqual(merged[0]['chunk_id'], '2')
    
    def tearDown(self):
        """Clean up test environment."""
        self.temp_db.close()

if __name__ == "__main__":
    unittest.main()
```

## 12. Deployment Considerations

### Production Deployment Checklist

**Infrastructure Requirements:**
- PostgreSQL 14+ with pgvector extension
- MAX serving infrastructure with GPU support recommended
- Minimum 16GB RAM for embedding generation
- SSD storage for database (estimate 1GB per 100k chunks)

**Security Considerations:**
- Use environment variables for database credentials
- Implement rate limiting for the search API
- Enable SSL for database connections
- Regular backups of the vector database

**Scaling Strategies:**
- Horizontal scaling: Partition chunks across multiple databases
- Caching: Implement Redis for frequently accessed embeddings
- Batch processing: Process document updates during off-peak hours
- CDN: Cache static documentation content

### Docker Deployment

Create a containerized deployment:

```dockerfile
# Dockerfile
FROM python:3.11-slim

# Install system dependencies
RUN apt-get update && apt-get install -y \
    postgresql-client \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Install Pixi
RUN curl -fsSL https://pixi.sh/install.sh | sh

# Set working directory
WORKDIR /app

# Copy requirements
COPY pyproject.toml .
COPY config.yaml .

# Install Python dependencies
RUN pip install -e .

# Copy application code
COPY src/ ./src/

# Start script
COPY start.sh .
RUN chmod +x start.sh

# Health check
HEALTHCHECK --interval=30s --timeout=3s \
  CMD python -c "import psycopg; psycopg.connect('${DATABASE_URL}')" || exit 1

ENTRYPOINT ["./start.sh"]
```

```yaml
# docker-compose.yaml
version: '3.8'

services:
  postgres:
    image: pgvector/pgvector:pg16
    environment:
      POSTGRES_DB: mojo_docs
      POSTGRES_USER: doc_user
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"

  max-serving:
    image: modular/max-serving:latest
    command: [
      "serve",
      "--model", "sentence-transformers/all-mpnet-base-v2",
      "--host", "0.0.0.0",
      "--port", "8000"
    ]
    ports:
      - "8000:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  doc-processor:
    build: .
    depends_on:
      - postgres
      - max-serving
    environment:
      DATABASE_URL: postgresql://doc_user:${DB_PASSWORD}@postgres/mojo_docs
      MAX_URL: http://max-serving:8000
    volumes:
      - /home/james/mojo/docs:/docs:ro
      - ./logs:/app/logs

volumes:
  pgdata:
```

## Conclusion

This comprehensive guide provides a production-ready system for converting technical documentation into searchable vector embeddings. The modular architecture ensures that you can easily adapt it for different documentation sources by simply modifying the configuration and preprocessing steps.

The combination of recursive chunking strategies that maintain semantic coherence, the optimized all-mpnet-base-v2 model providing 768-dimensional embeddings, and hybrid search using Reciprocal Rank Fusion creates a powerful retrieval system that balances accuracy with performance.

Key success factors for implementation:
1. Carefully tune chunk sizes based on your specific content
2. Preserve rich metadata for filtered searches
3. Monitor search patterns to optimize weights
4. Implement incremental processing for efficiency
5. Regular validation of embedding quality

This system provides the foundation for your MCP resource server, enabling AI agents to efficiently search and understand technical documentation with both semantic understanding and keyword precision.
