import os
import json
import pandas as pd
from tqdm import tqdm

# --- Configuration ---
CHUNKS_DIR = "processed_docs/chunks"
EMBEDDINGS_DIR = "processed_docs/embeddings"
OUTPUT_FILE = "processed_docs/mojo_manual_embeddings.parquet"
MIN_CHUNK_LENGTH = 80  # Relaxed threshold; consider token-based threshold downstream
# --- End Configuration ---

def load_jsonl(file_path):
    """Loads a .jsonl file and returns a list of dictionaries."""
    data = []
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    return data

def main():
    """
    Consolidates chunks, metadata, and embeddings into a single Parquet file.
    """
    print("ðŸ”¥ Starting data consolidation process...")

    # 1. Load all embeddings into a dictionary for quick lookup
    print("Loading embeddings...")
    embeddings_map = {}
    embedding_files = [f for f in os.listdir(EMBEDDINGS_DIR) if f.endswith("_embeddings.jsonl")]
    for file_name in tqdm(embedding_files, desc="Reading embedding files"):
        file_path = os.path.join(EMBEDDINGS_DIR, file_name)
        for item in load_jsonl(file_path):
            embeddings_map[item["chunk_id"]] = item["embedding"]
    print(f"âœ“ Loaded {len(embeddings_map)} embeddings.")

    # 2. Load all chunks and join with embeddings and metadata
    print("\nLoading chunks and metadata...")
    consolidated_data = []
    chunk_files = [f for f in os.listdir(CHUNKS_DIR) if f.endswith(".jsonl")]
    for file_name in tqdm(chunk_files, desc="Reading chunk files"):
        file_path = os.path.join(CHUNKS_DIR, file_name)
        for chunk_data in load_jsonl(file_path):
            chunk_id = chunk_data.get("chunk_id")
            if not chunk_id:
                continue

            # --- Quality Filter ---
            # Only include chunks with a minimal length to avoid extreme noise.
            content = chunk_data.get("content")
            if not content or len(content) < MIN_CHUNK_LENGTH:
                # Keep short chunks if they look like headers or intros
                meta = chunk_data.get("metadata", {})
                section_h = meta.get("section_hierarchy", [])
                if not section_h:
                    continue

            # Get the corresponding embedding
            embedding = embeddings_map.get(chunk_id)
            if not embedding:
                print(f"Warning: No embedding found for chunk_id {chunk_id}")
                continue

            # Extract relevant metadata
            metadata = chunk_data.get("metadata", {})
            
            # Create a unified record
            record = {
                "chunk_id": chunk_id,
                "document_id": chunk_data.get("document_id"),
                "content": content,
                "embedding": embedding,
                "title": metadata.get("title"),
                "url": metadata.get("url"),
                "section_hierarchy": metadata.get("section_hierarchy", []),
                # Persist a section_url if present; else fall back to url at read time
                "section_url": metadata.get("section_url"),
                # Persist useful quality features when present
                "token_count": chunk_data.get("token_count"),
                "has_code": chunk_data.get("has_code"),
            }
            consolidated_data.append(record)

    if not consolidated_data:
        print("âŒ No data was consolidated. Exiting.")
        return

    print(f"âœ“ Consolidated {len(consolidated_data)} records.")

    # 3. Create a DataFrame and save to Parquet
    print("\nCreating DataFrame and saving to Parquet...")
    df = pd.DataFrame(consolidated_data)

    # Ensure the output directory exists
    output_dir = os.path.dirname(OUTPUT_FILE)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    df.to_parquet(OUTPUT_FILE, index=False)
    print(f"âœ… Successfully saved consolidated data to {OUTPUT_FILE}")

if __name__ == "__main__":
    main()
