{"chunk_id": "data-partitioning-partitioned_writes-000", "document_id": "data-partitioning-partitioned_writes", "content": "## Examples\n\nWrite a table to a Hive partitioned dataset of Parquet files:\n\n```sql\nCOPY orders TO 'orders'\n(FORMAT parquet, PARTITION_BY (year, month));\n```\n\nWrite a table to a Hive partitioned dataset of CSV files, allowing overwrites:\n\n```sql\nCOPY orders TO 'orders'\n(FORMAT csv, PARTITION_BY (year, month), OVERWRITE_OR_IGNORE);\n```\n\nWrite a table to a Hive partitioned dataset of GZIP-compressed CSV files, setting explicit data files' extension:\n\n```sql\nCOPY orders TO 'orders'\n(FORMAT csv, PARTITION_BY (year, month), COMPRESSION gzip, FILE_EXTENSION 'csv.gz');\n```", "position": 0, "token_count": 174, "has_code": true, "section_hierarchy": ["Examples"], "metadata": {"chunk_id": "data-partitioning-partitioned_writes-000", "document_id": "data-partitioning-partitioned_writes", "position": 0, "token_count": 174, "has_code": true, "overlap_with_previous": false, "section_hierarchy": ["Examples"], "file_path": "data/partitioning/partitioned_writes.md", "url": "/data/partitioning/partitioned_writes", "title": "Partitioned Writes", "category": null, "tags": [], "section_url": "/data/partitioning/partitioned_writes#examples"}}
{"chunk_id": "data-partitioning-partitioned_writes-001", "document_id": "data-partitioning-partitioned_writes", "content": "## Partitioned Writes\n\nWhen the `PARTITION_BY` clause is specified for the [`COPY` statement]({% link docs/stable/sql/statements/copy.md %}), the files are written in a [Hive partitioned]({% link docs/stable/data/partitioning/hive_partitioning.md %}) folder hierarchy. The target is the name of the root directory (in the example above: `orders`). The files are written in-order in the file hierarchy. Currently, one file is written per thread to each directory.\n\n```text\norders\n├── year=2021\n│ ├── month=1\n│ │ ├── data_1.parquet\n│ │ └── data_2.parquet\n│ └── month=2\n│ └── data_1.parquet\n└── year=2022\n ├── month=11\n │ ├── data_1.parquet\n │ └── data_2.parquet\n └── month=12\n └── data_1.parquet", "position": 1, "token_count": 207, "has_code": true, "section_hierarchy": ["Partitioned Writes"], "metadata": {"chunk_id": "data-partitioning-partitioned_writes-001", "document_id": "data-partitioning-partitioned_writes", "position": 1, "token_count": 207, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Partitioned Writes"], "file_path": "data/partitioning/partitioned_writes.md", "url": "/data/partitioning/partitioned_writes", "title": "Partitioned Writes", "category": null, "tags": [], "section_url": "/data/partitioning/partitioned_writes#partitioned-writes"}}
{"chunk_id": "data-partitioning-partitioned_writes-002", "document_id": "data-partitioning-partitioned_writes", "content": "```\n\nThe values of the partitions are automatically extracted from the data. Note that it can be very expensive to write a larger number of partitions as many files will be created. The ideal partition count depends on how large your dataset is.\n\nTo limit the maximum number of files the system can keep open before flushing to disk when writing using `PARTITION_BY`, use the `partitioned_write_max_open_files` configuration option (default: 100):\n\n```batch\nSET partitioned_write_max_open_files = 10;\n```\n\n> Bestpractice Writing data into many small partitions is expensive. It is generally recommended to have at least `100 MB` of data per partition.", "position": 2, "token_count": 153, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "data-partitioning-partitioned_writes-002", "document_id": "data-partitioning-partitioned_writes", "position": 2, "token_count": 153, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/partitioning/partitioned_writes.md", "url": "/data/partitioning/partitioned_writes", "title": "Partitioned Writes", "category": null, "tags": [], "section_url": "/data/partitioning/partitioned_writes"}}
{"chunk_id": "data-partitioning-partitioned_writes-003", "document_id": "data-partitioning-partitioned_writes", "content": "### Filename Pattern\n\nBy default, files will be named `data_0.parquet` or `data_0.csv`. With the flag `FILENAME_PATTERN` a pattern with `{i}` or `{uuid}` can be defined to create specific filenames:\n\n* `{i}` will be replaced by an index\n* `{uuid}` will be replaced by a 128 bits long UUID\n\nWrite a table to a Hive partitioned dataset of .parquet files, with an index in the filename:\n\n```sql\nCOPY orders TO 'orders'\n(FORMAT parquet, PARTITION_BY (year, month), OVERWRITE_OR_IGNORE, FILENAME_PATTERN 'orders_{i}');\n```\n\nWrite a table to a Hive partitioned dataset of .parquet files, with unique filenames:\n\n```sql\nCOPY orders TO 'orders'\n(FORMAT parquet, PARTITION_BY (year, month), OVERWRITE_OR_IGNORE, FILENAME_PATTERN 'file_{uuid}');\n```", "position": 3, "token_count": 237, "has_code": true, "section_hierarchy": ["Filename Pattern"], "metadata": {"chunk_id": "data-partitioning-partitioned_writes-003", "document_id": "data-partitioning-partitioned_writes", "position": 3, "token_count": 237, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Filename Pattern"], "file_path": "data/partitioning/partitioned_writes.md", "url": "/data/partitioning/partitioned_writes", "title": "Partitioned Writes", "category": null, "tags": [], "section_url": "/data/partitioning/partitioned_writes#filename-pattern"}}
{"chunk_id": "data-partitioning-partitioned_writes-004", "document_id": "data-partitioning-partitioned_writes", "content": "### Overwriting\n\nBy default the partitioned write will not allow overwriting existing directories.\nOn a local file system, the `OVERWRITE` and `OVERWRITE_OR_IGNORE` options remove the existing directories.\nOn remote file systems, overwriting is not supported.\n\n### Appending\n\nTo append to an existing Hive partitioned directory structure, use the `APPEND` option:\n\n```sql\nCOPY orders TO 'orders'\n(FORMAT parquet, PARTITION_BY (year, month), APPEND);\n```\n\nUsing the `APPEND` option result in a behavior similar the `OVERWRITE_OR_IGNORE, FILENAME_PATTERN '{uuid}'` options,\nbut DuckDB performs an extra check for whether the file already exists and then regenerates the UUID in the rare event that it does (to avoid clashes).\n\n### Handling Slashes in Columns\n\nTo handle slashes in column names, use Percent-Encoding implemented by the [`url_encode` function]({% link docs/stable/sql/functions/text.md %}#url_encodestring).", "position": 4, "token_count": 245, "has_code": true, "section_hierarchy": ["Handling Slashes in Columns"], "metadata": {"chunk_id": "data-partitioning-partitioned_writes-004", "document_id": "data-partitioning-partitioned_writes", "position": 4, "token_count": 245, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Handling Slashes in Columns"], "file_path": "data/partitioning/partitioned_writes.md", "url": "/data/partitioning/partitioned_writes", "title": "Partitioned Writes", "category": null, "tags": [], "section_url": "/data/partitioning/partitioned_writes#handling-slashes-in-columns"}}
