{"chunk_id": "data-csv-overview-000", "document_id": "data-csv-overview", "content": "## Examples\n\nThe following examples use the [`flights.csv`]({% link data/flights.csv %}) file.\n\nRead a CSV file from disk, auto-infer options:\n\n```sql\nSELECT * FROM 'flights.csv';\n```\n\nUse the `read_csv` function with custom options:\n\n```sql\nSELECT *\nFROM read_csv('flights.csv',\n delim = '|',\n header = true,\n columns = {\n 'FlightDate': 'DATE',\n 'UniqueCarrier': 'VARCHAR',\n 'OriginCityName': 'VARCHAR',\n 'DestCityName': 'VARCHAR'\n });\n```\n\nRead a CSV from stdin, auto-infer options:\n\n```batch\ncat flights.csv | duckdb -c \"SELECT * FROM read_csv('/dev/stdin')\"", "position": 0, "token_count": 207, "has_code": true, "section_hierarchy": ["Examples"], "metadata": {"chunk_id": "data-csv-overview-000", "document_id": "data-csv-overview", "position": 0, "token_count": 207, "has_code": true, "overlap_with_previous": false, "section_hierarchy": ["Examples"], "file_path": "data/csv/overview.md", "url": "/data/csv/overview", "title": "CSV Import", "category": null, "tags": [], "section_url": "/data/csv/overview#examples"}}
{"chunk_id": "data-csv-overview-001", "document_id": "data-csv-overview", "content": "```\n\nRead a CSV file into a table:\n\n```sql\nCREATE TABLE ontime (\n FlightDate DATE,\n UniqueCarrier VARCHAR,\n OriginCityName VARCHAR,\n DestCityName VARCHAR\n);\nCOPY ontime FROM 'flights.csv';\n```\n\nAlternatively, create a table without specifying the schema manually using a [`CREATE TABLE ... AS SELECT` statement]({% link docs/stable/sql/statements/create_table.md %}#create-table--as-select-ctas):\n\n```sql\nCREATE TABLE ontime AS\n SELECT * FROM 'flights.csv';\n```\n\nWe can use the [`FROM`-first syntax]({% link docs/stable/sql/query_syntax/from.md %}#from-first-syntax) to omit `SELECT *`.\n\n```sql\nCREATE TABLE ontime AS\n FROM 'flights.csv';\n```", "position": 1, "token_count": 213, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "data-csv-overview-001", "document_id": "data-csv-overview", "position": 1, "token_count": 213, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/csv/overview.md", "url": "/data/csv/overview", "title": "CSV Import", "category": null, "tags": [], "section_url": "/data/csv/overview"}}
{"chunk_id": "data-csv-overview-002", "document_id": "data-csv-overview", "content": "## CSV Loading\n\nCSV loading, i.e., importing CSV files to the database, is a very common, and yet surprisingly tricky, task. While CSVs seem simple on the surface, there are a lot of inconsistencies found within CSV files that can make loading them a challenge. CSV files come in many different varieties, are often corrupt, and do not have a schema. The CSV reader needs to cope with all of these different situations.\n\nThe DuckDB CSV reader can automatically infer which configuration flags to use by analyzing the CSV file using the [CSV sniffer]({% post_url 2023-10-27-csv-sniffer %}). This will work correctly in most situations, and should be the first option attempted. In rare situations where the CSV reader cannot figure out the correct configuration it is possible to manually configure the CSV reader to correctly parse the CSV file. See the [auto detection page]({% link docs/stable/data/csv/auto_detection.md %}) for more information.", "position": 2, "token_count": 240, "has_code": false, "section_hierarchy": ["CSV Loading"], "metadata": {"chunk_id": "data-csv-overview-002", "document_id": "data-csv-overview", "position": 2, "token_count": 240, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["CSV Loading"], "file_path": "data/csv/overview.md", "url": "/data/csv/overview", "title": "CSV Import", "category": null, "tags": [], "section_url": "/data/csv/overview#csv-loading"}}
{"chunk_id": "data-csv-overview-003", "document_id": "data-csv-overview", "content": "## Parameters\n\nBelow are parameters that can be passed to the [`read_csv` function](#csv-functions). Where meaningfully applicable, these parameters can also be passed to the [`COPY` statement]({% link docs/stable/sql/statements/copy.md %}#copy-to).", "position": 3, "token_count": 74, "has_code": false, "section_hierarchy": ["Parameters"], "metadata": {"chunk_id": "data-csv-overview-003", "document_id": "data-csv-overview", "position": 3, "token_count": 74, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Parameters"], "file_path": "data/csv/overview.md", "url": "/data/csv/overview", "title": "CSV Import", "category": null, "tags": [], "section_url": "/data/csv/overview#parameters"}}
{"chunk_id": "data-csv-overview-004", "document_id": "data-csv-overview", "content": "| Name | Description | Type | Default |\n|:--|:-----|:-|:-|\n| `all_varchar` | Skip type detection and assume all columns are of type `VARCHAR`. This option is only supported by the `read_csv` function. | `BOOL` | `false` |\n| `allow_quoted_nulls` | Allow the conversion of quoted values to `NULL` values | `BOOL` | `true` |\n| `auto_detect` | [Auto detect CSV parameters]({% link docs/stable/data/csv/auto_detection.md %}). | `BOOL` | `true` |\n| `auto_type_candidates` | Types that the sniffer uses when detecting column types. The `VARCHAR` type is always included as a fallback option. See [example](#auto_type_candidates-details). | `TYPE[]` | [default types](#auto_type_candidates-details) |", "position": 4, "token_count": 230, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "data-csv-overview-004", "document_id": "data-csv-overview", "position": 4, "token_count": 230, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/csv/overview.md", "url": "/data/csv/overview", "title": "CSV Import", "category": null, "tags": [], "section_url": "/data/csv/overview"}}
{"chunk_id": "data-csv-overview-005", "document_id": "data-csv-overview", "content": "| `buffer_size` | Size of the buffers used to read files, in bytes. Must be large enough to hold four lines and can significantly impact performance. | `BIGINT` | `16 * max_line_size` |\n| `columns` | Column names and types, as a struct (e.g., `{'col1': 'INTEGER', 'col2': 'VARCHAR'}`). Using this option disables auto detection of the schema. | `STRUCT` | (empty) |\n| `comment` | Character used to initiate comments. Lines starting with a comment character (optionally preceded by space characters) are completely ignored; other lines containing a comment character are parsed only up to that point. | `VARCHAR` | (empty) |", "position": 5, "token_count": 177, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "data-csv-overview-005", "document_id": "data-csv-overview", "position": 5, "token_count": 177, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/csv/overview.md", "url": "/data/csv/overview", "title": "CSV Import", "category": null, "tags": [], "section_url": "/data/csv/overview"}}
{"chunk_id": "data-csv-overview-006", "document_id": "data-csv-overview", "content": "| `compression` | Method used to compress CSV files. By default this is detected automatically from the file extension (e.g., `t.csv.gz` will use gzip, `t.csv` will use `none`). Options are `none`, `gzip`, `zstd`. | `VARCHAR` | `auto` |\n| `dateformat` | [Date format]({% link docs/stable/sql/functions/dateformat.md %}) used when parsing and writing dates. | `VARCHAR` | (empty) |\n| `date_format` | Alias for `dateformat`; only available in the `COPY` statement. | `VARCHAR` | (empty) |\n| `decimal_separator` | Decimal separator for numbers. | `VARCHAR` | `.` |", "position": 6, "token_count": 202, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "data-csv-overview-006", "document_id": "data-csv-overview", "position": 6, "token_count": 202, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/csv/overview.md", "url": "/data/csv/overview", "title": "CSV Import", "category": null, "tags": [], "section_url": "/data/csv/overview"}}
{"chunk_id": "data-csv-overview-007", "document_id": "data-csv-overview", "content": "| `decimal_separator` | Decimal separator for numbers. | `VARCHAR` | `.` |\n| `delim` | Delimiter character used to separate columns within each line, e.g., `,` `;` `\\t`. The delimiter character can be up to 4 bytes, e.g., ðŸ¦†. Alias for `sep`. | `VARCHAR` | `,` |\n| `delimiter` | Alias for `delim`; only available in the `COPY` statement. | `VARCHAR` | `,` |\n| `escape` | String used to escape the `quote` character within quoted values. | `VARCHAR` | `\"` |\n| `encoding` | Encoding used by the CSV file. Options are `utf-8`, `utf-16`, `latin-1`. Not available in the `COPY` statement (which always uses `utf-8`). | `VARCHAR` | `utf-8` |", "position": 7, "token_count": 231, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "data-csv-overview-007", "document_id": "data-csv-overview", "position": 7, "token_count": 231, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/csv/overview.md", "url": "/data/csv/overview", "title": "CSV Import", "category": null, "tags": [], "section_url": "/data/csv/overview"}}
{"chunk_id": "data-csv-overview-008", "document_id": "data-csv-overview", "content": "| `filename` | Add path of the containing file to each row, as a string column named `filename`. Relative or absolute paths are returned depending on the path or glob pattern provided to `read_csv`, not just filenames. Since DuckDB v1.3.0, the `filename` column is added automatically as a virtual column and this option is only kept for compatibility reasons. | `BOOL` | `false` |\n| `force_not_null` | Do not match values in the specified columns against the `NULL` string. In the default case where the `NULL` string is empty, this means that empty values are read as zero-length strings instead of `NULL`s. | `VARCHAR[]` | `[]` |\n| `header` | First line of each file contains the column names. | `BOOL` | `false` |", "position": 8, "token_count": 196, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "data-csv-overview-008", "document_id": "data-csv-overview", "position": 8, "token_count": 196, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/csv/overview.md", "url": "/data/csv/overview", "title": "CSV Import", "category": null, "tags": [], "section_url": "/data/csv/overview"}}
{"chunk_id": "data-csv-overview-009", "document_id": "data-csv-overview", "content": "| `header` | First line of each file contains the column names. | `BOOL` | `false` |\n| `hive_partitioning` | Interpret the path as a [Hive partitioned path]({% link docs/stable/data/partitioning/hive_partitioning.md %}). | `BOOL` | (auto-detected) |\n| `ignore_errors` | Ignore any parsing errors encountered. | `BOOL` | `false` |\n| `max_line_size` or `maximum_line_size`. Not available in the `COPY` statement. | Maximum line size, in bytes. | `BIGINT` | 2000000 |\n| `names` or `column_names` | Column names, as a list. See [example]({% link docs/stable/data/csv/tips.md %}#provide-names-if-the-file-does-not-contain-a-header). | `VARCHAR[]` | (empty) |", "position": 9, "token_count": 226, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "data-csv-overview-009", "document_id": "data-csv-overview", "position": 9, "token_count": 226, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/csv/overview.md", "url": "/data/csv/overview", "title": "CSV Import", "category": null, "tags": [], "section_url": "/data/csv/overview"}}
{"chunk_id": "data-csv-overview-010", "document_id": "data-csv-overview", "content": "| `new_line` | New line character(s). Options are `'\\r'`,`'\\n'`, or `'\\r\\n'`. The CSV parser only distinguishes between single-character and double-character line delimiters. Therefore, it does not differentiate between `'\\r'` and `'\\n'`.| `VARCHAR` | (empty) |\n| `normalize_names` | Normalize column names. This removes any non-alphanumeric characters from them. Column names that are reserved SQL keywords are prefixed with an underscore character (`_`). | `BOOL` | `false` |\n| `null_padding` | Pad the remaining columns on the right with `NULL` values when a line lacks columns. | `BOOL` | `false` |\n| `nullstr` or `null` | Strings that represent a `NULL` value. | `VARCHAR` or `VARCHAR[]` | (empty) |", "position": 10, "token_count": 227, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "data-csv-overview-010", "document_id": "data-csv-overview", "position": 10, "token_count": 227, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/csv/overview.md", "url": "/data/csv/overview", "title": "CSV Import", "category": null, "tags": [], "section_url": "/data/csv/overview"}}
{"chunk_id": "data-csv-overview-011", "document_id": "data-csv-overview", "content": "| `nullstr` or `null` | Strings that represent a `NULL` value. | `VARCHAR` or `VARCHAR[]` | (empty) |\n| `parallel` | Use the parallel CSV reader. | `BOOL` | `true` |\n| `quote` | String used to quote values. | `VARCHAR` | `\"` |\n| `rejects_scan` | Name of the [temporary table where information on faulty scans is stored]({% link docs/stable/data/csv/reading_faulty_csv_files.md %}#reject-scans). | `VARCHAR` | `reject_scans` |\n| `rejects_table` | Name of the [temporary table where information on faulty lines is stored]({% link docs/stable/data/csv/reading_faulty_csv_files.md %}#reject-errors). | `VARCHAR` | `reject_errors` |", "position": 11, "token_count": 217, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "data-csv-overview-011", "document_id": "data-csv-overview", "position": 11, "token_count": 217, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/csv/overview.md", "url": "/data/csv/overview", "title": "CSV Import", "category": null, "tags": [], "section_url": "/data/csv/overview"}}
{"chunk_id": "data-csv-overview-012", "document_id": "data-csv-overview", "content": "| `rejects_limit` | Upper limit on the number of faulty lines per file that are recorded in the rejects table. Setting this to `0` means that no limit is applied. | `BIGINT` | `0` |\n| `sample_size` | Number of sample lines for [auto detection of parameters]({% link docs/stable/data/csv/auto_detection.md %}). | `BIGINT` | 20480 |\n| `sep` | Delimiter character used to separate columns within each line, e.g., `,` `;` `\\t`. The delimiter character can be up to 4 bytes, e.g., ðŸ¦†. Alias for `delim`. | `VARCHAR` | `,` |\n| `skip` | Number of lines to skip at the start of each file. | `BIGINT` | 0 |\n| `store_rejects` | Skip any lines with errors and store them in the rejects table. | `BOOL` | `false` |", "position": 12, "token_count": 225, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "data-csv-overview-012", "document_id": "data-csv-overview", "position": 12, "token_count": 225, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/csv/overview.md", "url": "/data/csv/overview", "title": "CSV Import", "category": null, "tags": [], "section_url": "/data/csv/overview"}}
{"chunk_id": "data-csv-overview-013", "document_id": "data-csv-overview", "content": "| `store_rejects` | Skip any lines with errors and store them in the rejects table. | `BOOL` | `false` |\n| `strict_mode` | Enforces the strictness level of the CSV Reader. When set to `true`, the parser will throw an error upon encountering any issues. When set to `false`, the parser will attempt to read structurally incorrect files. It is important to note that reading structurally incorrect files can cause ambiguity; therefore, this option should be used with caution. | `BOOL` | `true` |\n| `thousands` | Character used to identify thousands separators in numeric values. It must be a single character and different from the `decimal_separator` option.| `VARCHAR` | (empty) |\n| `timestampformat` | [Timestamp format]({% link docs/stable/sql/functions/dateformat.md %}) used when parsing and writing timestamps. | `VARCHAR` | (empty) |", "position": 13, "token_count": 228, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "data-csv-overview-013", "document_id": "data-csv-overview", "position": 13, "token_count": 228, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/csv/overview.md", "url": "/data/csv/overview", "title": "CSV Import", "category": null, "tags": [], "section_url": "/data/csv/overview"}}
{"chunk_id": "data-csv-overview-014", "document_id": "data-csv-overview", "content": "| `timestamp_format` | Alias for `timestampformat`; only available in the `COPY` statement. | `VARCHAR` | (empty) |\n| `types` or `dtypes` or `column_types` | Column types, as either a list (by position) or a struct (by name). See [example]({% link docs/stable/data/csv/tips.md %}#override-the-types-of-specific-columns). | `VARCHAR[]` or `STRUCT` | (empty) |\n| `union_by_name` | Align columns from different files [by column name]({% link docs/stable/data/multiple_files/combining_schemas.md %}#union-by-name) instead of position. Using this option increases memory consumption. | `BOOL` | `false` |", "position": 14, "token_count": 207, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "data-csv-overview-014", "document_id": "data-csv-overview", "position": 14, "token_count": 207, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/csv/overview.md", "url": "/data/csv/overview", "title": "CSV Import", "category": null, "tags": [], "section_url": "/data/csv/overview"}}
{"chunk_id": "data-csv-overview-015", "document_id": "data-csv-overview", "content": "> Tip DuckDB's CSV reader supports `UTF-8` (default), `UTF-16` and `Latin-1` encodings as well as many other `encoding` options\n> natively through the `encoding` extension, for details see [All Supported Encodings]({% link docs/stable/core_extensions/encodings.md%}#all-supported-encodings).\n> To convert files with different encodings, we recommend using the [`iconv` command-line tool](https://linux.die.net/man/1/iconv).\n>\n> ```batch\n> iconv -f ISO-8859-2 -t UTF-8 input.csv > input-utf-8.csv\n>", "position": 15, "token_count": 172, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "data-csv-overview-015", "document_id": "data-csv-overview", "position": 15, "token_count": 172, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/csv/overview.md", "url": "/data/csv/overview", "title": "CSV Import", "category": null, "tags": [], "section_url": "/data/csv/overview"}}
{"chunk_id": "data-csv-overview-016", "document_id": "data-csv-overview", "content": "```", "position": 16, "token_count": 5, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "data-csv-overview-016", "document_id": "data-csv-overview", "position": 16, "token_count": 5, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/csv/overview.md", "url": "/data/csv/overview", "title": "CSV Import", "category": null, "tags": [], "section_url": "/data/csv/overview"}}
{"chunk_id": "data-csv-overview-017", "document_id": "data-csv-overview", "content": "### `auto_type_candidates` Details\n\nThe `auto_type_candidates` option lets you specify the data types that should be considered by the CSV reader for [column data type detection]({% link docs/stable/data/csv/auto_detection.md %}#type-detection).\nUsage example:\n\n```sql\nSELECT * FROM read_csv('csv_file.csv', auto_type_candidates = ['BIGINT', 'DATE']);\n```\n\nThe default value for the `auto_type_candidates` option is `['SQLNULL', 'BOOLEAN', 'BIGINT', 'DOUBLE', 'TIME', 'DATE', 'TIMESTAMP', 'VARCHAR']`.", "position": 17, "token_count": 175, "has_code": true, "section_hierarchy": ["`auto_type_candidates` Details"], "metadata": {"chunk_id": "data-csv-overview-017", "document_id": "data-csv-overview", "position": 17, "token_count": 175, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["`auto_type_candidates` Details"], "file_path": "data/csv/overview.md", "url": "/data/csv/overview", "title": "CSV Import", "category": null, "tags": [], "section_url": "/data/csv/overview#autotypecandidates-details"}}
{"chunk_id": "data-csv-overview-018", "document_id": "data-csv-overview", "content": "## CSV Functions\n\nThe `read_csv` automatically attempts to figure out the correct configuration of the CSV reader using the [CSV sniffer]({% post_url 2023-10-27-csv-sniffer %}). It also automatically deduces types of columns. If the CSV file has a header, it will use the names found in that header to name the columns. Otherwise, the columns will be named `column0, column1, column2, ...`. An example with the [`flights.csv`]({% link data/flights.csv %}) file:\n\n```sql\nSELECT * FROM read_csv('flights.csv');", "position": 18, "token_count": 160, "has_code": true, "section_hierarchy": ["CSV Functions"], "metadata": {"chunk_id": "data-csv-overview-018", "document_id": "data-csv-overview", "position": 18, "token_count": 160, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["CSV Functions"], "file_path": "data/csv/overview.md", "url": "/data/csv/overview", "title": "CSV Import", "category": null, "tags": [], "section_url": "/data/csv/overview#csv-functions"}}
{"chunk_id": "data-csv-overview-019", "document_id": "data-csv-overview", "content": "```\n\n| FlightDate | UniqueCarrier | OriginCityName | DestCityName |\n|------------|---------------|----------------|-----------------|\n| 1988-01-01 | AA | New York, NY | Los Angeles, CA |\n| 1988-01-02 | AA | New York, NY | Los Angeles, CA |\n| 1988-01-03 | AA | New York, NY | Los Angeles, CA |\n\nThe path can either be a relative path (relative to the current working directory) or an absolute path.\n\nWe can use `read_csv` to create a persistent table as well:\n\n```sql\nCREATE TABLE ontime AS\n SELECT * FROM read_csv('flights.csv');\nDESCRIBE ontime;", "position": 19, "token_count": 211, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "data-csv-overview-019", "document_id": "data-csv-overview", "position": 19, "token_count": 211, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/csv/overview.md", "url": "/data/csv/overview", "title": "CSV Import", "category": null, "tags": [], "section_url": "/data/csv/overview"}}
{"chunk_id": "data-csv-overview-020", "document_id": "data-csv-overview", "content": "```\n\n| column_name | column_type | null | key | default | extra |\n|----------------|-------------|------|------|---------|-------|\n| FlightDate | DATE | YES | NULL | NULL | NULL |\n| UniqueCarrier | VARCHAR | YES | NULL | NULL | NULL |\n| OriginCityName | VARCHAR | YES | NULL | NULL | NULL |\n| DestCityName | VARCHAR | YES | NULL | NULL | NULL |\n\n```sql\nSELECT * FROM read_csv('flights.csv', sample_size = 20_000);\n```\n\nIf we set `delim` / `sep`, `quote`, `escape`, or `header` explicitly, we can bypass the automatic detection of this particular parameter:\n\n```sql\nSELECT * FROM read_csv('flights.csv', header = true);", "position": 20, "token_count": 244, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "data-csv-overview-020", "document_id": "data-csv-overview", "position": 20, "token_count": 244, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/csv/overview.md", "url": "/data/csv/overview", "title": "CSV Import", "category": null, "tags": [], "section_url": "/data/csv/overview"}}
{"chunk_id": "data-csv-overview-021", "document_id": "data-csv-overview", "content": "```\n\nMultiple files can be read at once by providing a glob or a list of files. Refer to the [multiple files section]({% link docs/stable/data/multiple_files/overview.md %}) for more information.", "position": 21, "token_count": 57, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "data-csv-overview-021", "document_id": "data-csv-overview", "position": 21, "token_count": 57, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/csv/overview.md", "url": "/data/csv/overview", "title": "CSV Import", "category": null, "tags": [], "section_url": "/data/csv/overview"}}
{"chunk_id": "data-csv-overview-022", "document_id": "data-csv-overview", "content": "## Writing Using the `COPY` Statement\n\nThe [`COPY` statement]({% link docs/stable/sql/statements/copy.md %}#copy-to) can be used to load data from a CSV file into a table. This statement has the same syntax as the one used in PostgreSQL. To load the data using the `COPY` statement, we must first create a table with the correct schema (which matches the order of the columns in the CSV file and uses types that fit the values in the CSV file). `COPY` detects the CSV's configuration options automatically.\n\n```sql\nCREATE TABLE ontime (\n flightdate DATE,\n uniquecarrier VARCHAR,\n origincityname VARCHAR,\n destcityname VARCHAR\n);\nCOPY ontime FROM 'flights.csv';\nSELECT * FROM ontime;", "position": 22, "token_count": 189, "has_code": true, "section_hierarchy": ["Writing Using the `COPY` Statement"], "metadata": {"chunk_id": "data-csv-overview-022", "document_id": "data-csv-overview", "position": 22, "token_count": 189, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Writing Using the `COPY` Statement"], "file_path": "data/csv/overview.md", "url": "/data/csv/overview", "title": "CSV Import", "category": null, "tags": [], "section_url": "/data/csv/overview#writing-using-the-copy-statement"}}
{"chunk_id": "data-csv-overview-023", "document_id": "data-csv-overview", "content": "```\n\n| flightdate | uniquecarrier | origincityname | destcityname |\n|------------|---------------|----------------|-----------------|\n| 1988-01-01 | AA | New York, NY | Los Angeles, CA |\n| 1988-01-02 | AA | New York, NY | Los Angeles, CA |\n| 1988-01-03 | AA | New York, NY | Los Angeles, CA |\n\nIf we want to manually specify the CSV format, we can do so using the configuration options of `COPY`.\n\n```sql\nCREATE TABLE ontime (flightdate DATE, uniquecarrier VARCHAR, origincityname VARCHAR, destcityname VARCHAR);\nCOPY ontime FROM 'flights.csv' (DELIMITER '|', HEADER);\nSELECT * FROM ontime;\n```", "position": 23, "token_count": 234, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "data-csv-overview-023", "document_id": "data-csv-overview", "position": 23, "token_count": 234, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/csv/overview.md", "url": "/data/csv/overview", "title": "CSV Import", "category": null, "tags": [], "section_url": "/data/csv/overview"}}
{"chunk_id": "data-csv-overview-024", "document_id": "data-csv-overview", "content": "## Reading Faulty CSV Files\n\nDuckDB supports reading erroneous CSV files. For details, see the [Reading Faulty CSV Files page]({% link docs/stable/data/csv/reading_faulty_csv_files.md %}).\n\n## Order Preservation\n\nThe CSV reader respects the `preserve_insertion_order` [configuration option]({% link docs/stable/configuration/overview.md %}) to [preserve insertion order]({% link docs/stable/sql/dialect/order_preservation.md %}).\nWhen `true` (the default), the order of the rows in the result set returned by the CSV reader is the same as the order of the corresponding lines read from the file(s).\nWhen `false`, there is no guarantee that the order is preserved.\n\n## Writing CSV Files\n\nDuckDB can write CSV files using the [`COPY ... TO` statement]({% link docs/stable/sql/statements/copy.md %}#copy--to).", "position": 24, "token_count": 234, "has_code": false, "section_hierarchy": ["Writing CSV Files"], "metadata": {"chunk_id": "data-csv-overview-024", "document_id": "data-csv-overview", "position": 24, "token_count": 234, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Writing CSV Files"], "file_path": "data/csv/overview.md", "url": "/data/csv/overview", "title": "CSV Import", "category": null, "tags": [], "section_url": "/data/csv/overview#writing-csv-files"}}
