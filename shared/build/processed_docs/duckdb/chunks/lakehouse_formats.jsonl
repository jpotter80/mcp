{"chunk_id": "lakehouse_formats-000", "document_id": "lakehouse_formats", "content": "Lakehouse formats, often referred to as open table formats, are specifications for storing data in object storage while maintaining some guarantees such as ACID transactions or keeping snapshot history. Over time, multiple lakehouse formats have emerged, each one with its own unique approach to managing its metadata (a.k.a. catalog). In this page, we will go over the support that DuckDB offers for some of these formats as well as some workarounds that you can use to still use DuckDB and get close to full interoperability with these formats.", "position": 0, "token_count": 114, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "lakehouse_formats-000", "document_id": "lakehouse_formats", "position": 0, "token_count": 114, "has_code": false, "overlap_with_previous": false, "section_hierarchy": [], "file_path": "lakehouse_formats.md", "url": "/lakehouse_formats", "title": "Lakehouse Formats", "category": null, "tags": [], "section_url": "/lakehouse_formats"}}
{"chunk_id": "lakehouse_formats-001", "document_id": "lakehouse_formats", "content": "## DuckDB Lakehouse Support Matrix\n\nDuckDB supports Iceberg, Delta and DuckLake as first-class citizens. The following matrix represents what DuckDB natively supports out of the box through core extensions.", "position": 1, "token_count": 44, "has_code": false, "section_hierarchy": ["DuckDB Lakehouse Support Matrix"], "metadata": {"chunk_id": "lakehouse_formats-001", "document_id": "lakehouse_formats", "position": 1, "token_count": 44, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["DuckDB Lakehouse Support Matrix"], "file_path": "lakehouse_formats.md", "url": "/lakehouse_formats", "title": "Lakehouse Formats", "category": null, "tags": [], "section_url": "/lakehouse_formats#duckdb-lakehouse-support-matrix"}}
{"chunk_id": "lakehouse_formats-002", "document_id": "lakehouse_formats", "content": "| | DuckLake | Iceberg | Delta |\n| ---------------------------- | :-------------------------------------------------------------------- | :---------------------------------------------------------------------- | :--------------------------------------------------------- |", "position": 2, "token_count": 243, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "lakehouse_formats-002", "document_id": "lakehouse_formats", "position": 2, "token_count": 243, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "lakehouse_formats.md", "url": "/lakehouse_formats", "title": "Lakehouse Formats", "category": null, "tags": [], "section_url": "/lakehouse_formats"}}
{"chunk_id": "lakehouse_formats-003", "document_id": "lakehouse_formats", "content": "| Extension | [`ducklake`](https://ducklake.select/docs/stable/duckdb/introduction) | [`iceberg`]({% link docs/stable/core_extensions/iceberg/overview.md %}) | [`delta`]({% link docs/stable/core_extensions/delta.md %}) |\n| Read | ✅ | ✅ | ✅ |\n| Write | ✅ | ✅ | ✅ |\n| Deletes | ✅ | ✅ | ❌ |\n| Updates | ✅ | ✅ | ❌ |\n| Upserting | ✅ | ❌ | ❌ |\n| Create table | ✅ | ✅ | ❌ |\n| Create table with partitions | ✅ | ❌ | ❌ |\n| Attaching to a catalog | ✅ | ✅ | ✅ `*` |\n| Rename table | ✅ | ❌ | ❌ |\n| Rename columns | ✅ | ❌ | ❌ |\n| Add/drop columns | ✅ | ❌ | ❌ |", "position": 3, "token_count": 208, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "lakehouse_formats-003", "document_id": "lakehouse_formats", "position": 3, "token_count": 208, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "lakehouse_formats.md", "url": "/lakehouse_formats", "title": "Lakehouse Formats", "category": null, "tags": [], "section_url": "/lakehouse_formats"}}
{"chunk_id": "lakehouse_formats-004", "document_id": "lakehouse_formats", "content": "| Rename table | ✅ | ❌ | ❌ |\n| Rename columns | ✅ | ❌ | ❌ |\n| Add/drop columns | ✅ | ❌ | ❌ |\n| Alter column type | ✅ | ❌ | ❌ |\n| Compaction and maintenance | ✅ | ❌ | ❌ |\n| Encryption | ✅ | ❌ | ❌ |\n| Manage table properties | ✅ | ❌ | ❌ |\n| Time travel | ✅ | ✅ | ✅ |\n| Query table changes | ✅ | ❌ | ❌ |", "position": 4, "token_count": 100, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "lakehouse_formats-004", "document_id": "lakehouse_formats", "position": 4, "token_count": 100, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "lakehouse_formats.md", "url": "/lakehouse_formats", "title": "Lakehouse Formats", "category": null, "tags": [], "section_url": "/lakehouse_formats"}}
{"chunk_id": "lakehouse_formats-005", "document_id": "lakehouse_formats", "content": "`*` Through the `uc_catalog` extension\n\nDuckDB aims to build native extensions with minimal dependencies. The `iceberg` extension for example, has dependencies on third-party Iceberg libraries, which means all data and metadata operations are implemented natively in the DuckDB extension. For the `delta` extension, we use the [`delta-kernel-rs` project](https://github.com/delta-io/delta-kernel-rs), which is meant to be a lightweight platform for engines to build delta integrations that are as close to native as possible.\n\n> **Why do native implementations matter?** Native implementations allow DuckDB to do more performance optimizations such as complex filter pushdowns (with file-level and row-group level pruning) and improve memory management.", "position": 5, "token_count": 176, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "lakehouse_formats-005", "document_id": "lakehouse_formats", "position": 5, "token_count": 176, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "lakehouse_formats.md", "url": "/lakehouse_formats", "title": "Lakehouse Formats", "category": null, "tags": [], "section_url": "/lakehouse_formats"}}
{"chunk_id": "lakehouse_formats-006", "document_id": "lakehouse_formats", "content": "## Workarounds for Unsupported Features\n\nIf the DuckDB core extension does not cover your use case, you can still use DuckDB to process the data and use an external library to help you with the unsupported operations. If you are using the Python client, there are some very good off-the-self libraries that can help you. This examples will have one thing in common, they use Arrow as an efficient, zero-copy data interface with DuckDB.\n\n### Using PyIceberg with DuckDB\n\nIn this example, we will use [PyIceberg](https://py.iceberg.apache.org/) to create and alter the schema of a table and DuckDB to read and write to the table.\n\nClick here to see the full example.\n\n```python\nfrom pyiceberg.catalog import load_catalog\nfrom pyiceberg.schema import Schema\nfrom pyiceberg.types import (\n TimestampType,\n FloatType,\n DoubleType,\n StringType,\n NestedField,\n)\nimport duckdb", "position": 6, "token_count": 227, "has_code": true, "section_hierarchy": ["Using PyIceberg with DuckDB"], "metadata": {"chunk_id": "lakehouse_formats-006", "document_id": "lakehouse_formats", "position": 6, "token_count": 227, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Using PyIceberg with DuckDB"], "file_path": "lakehouse_formats.md", "url": "/lakehouse_formats", "title": "Lakehouse Formats", "category": null, "tags": [], "section_url": "/lakehouse_formats#using-pyiceberg-with-duckdb"}}
{"chunk_id": "lakehouse_formats-007", "document_id": "lakehouse_formats", "content": "# Create a table with PyIceberg\ncatalog = load_catalog(\n \"docs\",\n **{\n \"uri\": \"http://127.0.0.1:8181\",\n \"s3.endpoint\": \"http://127.0.0.1:9000\",\n \"py-io-impl\": \"pyiceberg.io.pyarrow.PyArrowFileIO\",\n \"s3.access-key-id\": \"admin\",\n \"s3.secret-access-key\": \"password\",\n }\n)\nschema = Schema(\n NestedField(field_id=1, name=\"datetime\", field_type=TimestampType(), required=True),", "position": 7, "token_count": 175, "has_code": false, "section_hierarchy": ["Create a table with PyIceberg"], "metadata": {"chunk_id": "lakehouse_formats-007", "document_id": "lakehouse_formats", "position": 7, "token_count": 175, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Create a table with PyIceberg"], "file_path": "lakehouse_formats.md", "url": "/lakehouse_formats", "title": "Lakehouse Formats", "category": null, "tags": [], "section_url": "/lakehouse_formats#create-a-table-with-pyiceberg"}}
{"chunk_id": "lakehouse_formats-008", "document_id": "lakehouse_formats", "content": ")\nschema = Schema(\n NestedField(field_id=1, name=\"datetime\", field_type=TimestampType(), required=True),\n NestedField(field_id=2, name=\"symbol\", field_type=StringType(), required=True),\n NestedField(field_id=3, name=\"bid\", field_type=FloatType(), required=False),\n NestedField(field_id=4, name=\"ask\", field_type=DoubleType(), required=False)\n)\ncatalog.create_table(\n identifier=\"default.bids\",\n schema=schema,\n partition_spec=partition_spec,\n)", "position": 8, "token_count": 163, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "lakehouse_formats-008", "document_id": "lakehouse_formats", "position": 8, "token_count": 163, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "lakehouse_formats.md", "url": "/lakehouse_formats", "title": "Lakehouse Formats", "category": null, "tags": [], "section_url": "/lakehouse_formats"}}
{"chunk_id": "lakehouse_formats-009", "document_id": "lakehouse_formats", "content": "# Write and read the table with DuckDB\nwith duckdb.connect() as conn:\n conn.execute(\"\"\"\n CREATE SECRET (\n TYPE S3,\n KEY_ID 'admin',\n SECRET 'password',\n ENDPOINT '127.0.0.1:9000',\n URL_STYLE 'path',\n USE_SSL false\n );\n ATTACH '' AS my_datalake (\n TYPE ICEBERG,\n CLIENT_ID 'admin',\n CLIENT_SECRET 'password',\n ENDPOINT 'http://127.0.0.1:8181'\n );\n \"\"\")\n conn.execute(\"\"\"\n INSERT INTO my_datalake.default.bids VALUES ('2024-01-01 10:00:00', 'AAPL', 150.0, 150.5);\n \"\"\")\n conn.sql(\"SELECT * FROM my_datalake.default.bids;\").show()", "position": 9, "token_count": 209, "has_code": false, "section_hierarchy": ["Write and read the table with DuckDB"], "metadata": {"chunk_id": "lakehouse_formats-009", "document_id": "lakehouse_formats", "position": 9, "token_count": 209, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Write and read the table with DuckDB"], "file_path": "lakehouse_formats.md", "url": "/lakehouse_formats", "title": "Lakehouse Formats", "category": null, "tags": [], "section_url": "/lakehouse_formats#write-and-read-the-table-with-duckdb"}}
{"chunk_id": "lakehouse_formats-010", "document_id": "lakehouse_formats", "content": "# Alter schema with PyIceberg\ntable = catalog.load_table(\"default.bids\")\nwith table.update_schema() as update:\n update.add_column(\"retries\", IntegerType(), \"Number of retries to place the bid\")\n```\n\n### Using delta-rs with DuckDB\n\nIn this example, we create a Delta table with the `delta-rs` Python binding, then we use the `delta` extension of DuckDB to read it. We also showcase how to do other read operations with DuckDB, like reading the change data feed using the Arrow zero-copy integration. This operation can also be lazy if reading bigger data by using [Arrow Datasets](https://delta-io.github.io/delta-rs/integrations/delta-lake-arrow/).\n\nClick here to see the full example.\n\n```python\nimport deltalake as dl\nimport pyarrow as pa", "position": 10, "token_count": 209, "has_code": true, "section_hierarchy": ["Using delta-rs with DuckDB"], "metadata": {"chunk_id": "lakehouse_formats-010", "document_id": "lakehouse_formats", "position": 10, "token_count": 209, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Using delta-rs with DuckDB"], "file_path": "lakehouse_formats.md", "url": "/lakehouse_formats", "title": "Lakehouse Formats", "category": null, "tags": [], "section_url": "/lakehouse_formats#using-delta-rs-with-duckdb"}}
{"chunk_id": "lakehouse_formats-011", "document_id": "lakehouse_formats", "content": "# Create a delta table and read it with DuckDB Delta extension\ndl.write_deltalake(\n \"tmp/some_table\",\n pa.table({\n \"id\": [1, 2, 3],\n \"value\": [\"a\", \"b\", \"c\"]\n })\n)\nwith duckdb.connect() as conn:\n conn.execute(\"\"\"\n INSTALL delta;\n LOAD delta;\n \"\"\")\n conn.sql(\"\"\"\n SELECT * FROM delta_scan('tmp/some_table')\n \"\"\").show()", "position": 11, "token_count": 129, "has_code": false, "section_hierarchy": ["Create a delta table and read it with DuckDB Delta extension"], "metadata": {"chunk_id": "lakehouse_formats-011", "document_id": "lakehouse_formats", "position": 11, "token_count": 129, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Create a delta table and read it with DuckDB Delta extension"], "file_path": "lakehouse_formats.md", "url": "/lakehouse_formats", "title": "Lakehouse Formats", "category": null, "tags": [], "section_url": "/lakehouse_formats#create-a-delta-table-and-read-it-with-duckdb-delta-extension"}}
{"chunk_id": "lakehouse_formats-012", "document_id": "lakehouse_formats", "content": "# Append some data and read the data change feed using the PyArrow integration\ndl.write_deltalake(\n \"tmp/some_table\",\n pa.table({\n \"id\": [4, 5],\n \"value\": [\"d\", \"e\"]\n }),\n mode=\"append\"\n)\ntable = dl.DeltaTable(\"tmp/some_table\").load_cdf(starting_version=1, ending_version=2)\nwith duckdb.connect() as conn:\n conn.register(\"t\", table)\n conn.sql(\"SELECT * FROM t\").show()\n```", "position": 12, "token_count": 148, "has_code": true, "section_hierarchy": ["Append some data and read the data change feed using the PyArrow integration"], "metadata": {"chunk_id": "lakehouse_formats-012", "document_id": "lakehouse_formats", "position": 12, "token_count": 148, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Append some data and read the data change feed using the PyArrow integration"], "file_path": "lakehouse_formats.md", "url": "/lakehouse_formats", "title": "Lakehouse Formats", "category": null, "tags": [], "section_url": "/lakehouse_formats#append-some-data-and-read-the-data-change-feed-using-the-pyarrow-integration"}}
