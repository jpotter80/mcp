{"chunk_id": "guides-network_cloud_storage-s3_export-000", "document_id": "guides-network_cloud_storage-s3_export", "content": "To write a Parquet file to S3, the [`httpfs` extension]({% link docs/stable/core_extensions/httpfs/overview.md %}) is required. This can be installed using the `INSTALL` SQL command. This only needs to be run once.\n\n```sql\nINSTALL httpfs;\n```\n\nTo load the `httpfs` extension for usage, use the `LOAD` SQL command:\n\n```sql\nLOAD httpfs;\n```\n\nAfter loading the `httpfs` extension, set up the credentials to write data. Note that the `region` parameter should match the region of the bucket you want to access.\n\n```sql\nCREATE SECRET (\n TYPE s3,\n KEY_ID '⟨AKIAIOSFODNN7EXAMPLE⟩',\n SECRET '⟨wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY⟩',\n REGION '⟨us-east-1⟩'\n);", "position": 0, "token_count": 211, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "guides-network_cloud_storage-s3_export-000", "document_id": "guides-network_cloud_storage-s3_export", "position": 0, "token_count": 211, "has_code": true, "overlap_with_previous": false, "section_hierarchy": [], "file_path": "guides/network_cloud_storage/s3_export.md", "url": "/guides/network_cloud_storage/s3_export", "title": "S3 Parquet Export", "category": null, "tags": [], "section_url": "/guides/network_cloud_storage/s3_export"}}
{"chunk_id": "guides-network_cloud_storage-s3_export-001", "document_id": "guides-network_cloud_storage-s3_export", "content": "```\n\n> Tip If you get an IO Error (`Connection error for HTTP HEAD`), configure the endpoint explicitly via `ENDPOINT 's3.⟨your-region⟩.amazonaws.com'`{:.language-sql .highlight}.\n\nAlternatively, use the [`aws` extension]({% link docs/stable/core_extensions/aws.md %}) to retrieve the credentials automatically:\n\n```sql\nCREATE SECRET (\n TYPE s3,\n PROVIDER credential_chain\n);\n```\n\nAfter the `httpfs` extension is set up and the S3 credentials are correctly configured, Parquet files can be written to S3 using the following command:\n\n```sql\nCOPY ⟨table_name⟩ TO 's3://⟨s3-bucket⟩/⟨filename⟩.parquet';", "position": 1, "token_count": 187, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "guides-network_cloud_storage-s3_export-001", "document_id": "guides-network_cloud_storage-s3_export", "position": 1, "token_count": 187, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "guides/network_cloud_storage/s3_export.md", "url": "/guides/network_cloud_storage/s3_export", "title": "S3 Parquet Export", "category": null, "tags": [], "section_url": "/guides/network_cloud_storage/s3_export"}}
{"chunk_id": "guides-network_cloud_storage-s3_export-002", "document_id": "guides-network_cloud_storage-s3_export", "content": "```\n\nSimilarly, Google Cloud Storage (GCS) is supported through the Interoperability API.\nYou need to create [HMAC keys](https://console.cloud.google.com/storage/settings;tab=interoperability) and provide the credentials as follows:\n\n```sql\nCREATE SECRET (\n TYPE gcs,\n KEY_ID '⟨AKIAIOSFODNN7EXAMPLE⟩',\n SECRET '⟨wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY⟩'\n);\n```\n\nAfter setting up the GCS credentials, you can export using:\n\n```sql\nCOPY ⟨table_name⟩ TO 'gs://⟨gcs_bucket⟩/⟨filename⟩.parquet';\n```", "position": 2, "token_count": 171, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "guides-network_cloud_storage-s3_export-002", "document_id": "guides-network_cloud_storage-s3_export", "position": 2, "token_count": 171, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "guides/network_cloud_storage/s3_export.md", "url": "/guides/network_cloud_storage/s3_export", "title": "S3 Parquet Export", "category": null, "tags": [], "section_url": "/guides/network_cloud_storage/s3_export"}}
