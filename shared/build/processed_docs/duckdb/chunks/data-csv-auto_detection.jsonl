{"chunk_id": "data-csv-auto_detection-000", "document_id": "data-csv-auto_detection", "content": "When using `read_csv`, the system tries to automatically infer how to read the CSV file using the [CSV sniffer]({% post_url 2023-10-27-csv-sniffer %}).\nThis step is necessary because CSV files are not self-describing and come in many different dialects. The auto-detection works roughly as follows:\n\n* Detect the dialect of the CSV file (delimiter, quoting rule, escape)\n* Detect the types of each of the columns\n* Detect whether or not the file has a header row\n\nBy default the system will try to auto-detect all options. However, options can be individually overridden by the user. This can be useful in case the system makes a mistake. For example, if the delimiter is chosen incorrectly, we can override it by calling the `read_csv` with an explicit delimiter (e.g., `read_csv('file.csv', delim = '|')`).", "position": 0, "token_count": 223, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "data-csv-auto_detection-000", "document_id": "data-csv-auto_detection", "position": 0, "token_count": 223, "has_code": false, "overlap_with_previous": false, "section_hierarchy": [], "file_path": "data/csv/auto_detection.md", "url": "/data/csv/auto_detection", "title": "CSV Auto Detection", "category": null, "tags": [], "section_url": "/data/csv/auto_detection"}}
{"chunk_id": "data-csv-auto_detection-001", "document_id": "data-csv-auto_detection", "content": "## Sample Size\n\nThe type detection works by operating on a sample of the file.\nThe size of the sample can be modified by setting the `sample_size` parameter.\nThe default sample size is 20,480 rows.\nSetting the `sample_size` parameter to `-1` means the entire file is read for sampling:\n\n```sql\nSELECT * FROM read_csv('my_csv_file.csv', sample_size = -1);\n```\n\nThe way sampling is performed depends on the type of file. If we are reading from a regular file on disk, we will jump into the file and try to sample from different locations in the file.\nIf we are reading from a file in which we cannot jump – such as a `.gz` compressed CSV file or `stdin` – samples are taken only from the beginning of the file.", "position": 1, "token_count": 186, "has_code": true, "section_hierarchy": ["Sample Size"], "metadata": {"chunk_id": "data-csv-auto_detection-001", "document_id": "data-csv-auto_detection", "position": 1, "token_count": 186, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Sample Size"], "file_path": "data/csv/auto_detection.md", "url": "/data/csv/auto_detection", "title": "CSV Auto Detection", "category": null, "tags": [], "section_url": "/data/csv/auto_detection#sample-size"}}
{"chunk_id": "data-csv-auto_detection-002", "document_id": "data-csv-auto_detection", "content": "## `sniff_csv` Function\n\nIt is possible to run the CSV sniffer as a separate step using the `sniff_csv(filename)` function, which returns the detected CSV properties as a table with a single row.\nThe `sniff_csv` function accepts an optional `sample_size` parameter to configure the number of rows sampled.\n\n```sql\nFROM sniff_csv('my_file.csv');\nFROM sniff_csv('my_file.csv', sample_size = 1000);", "position": 2, "token_count": 124, "has_code": true, "section_hierarchy": ["`sniff_csv` Function"], "metadata": {"chunk_id": "data-csv-auto_detection-002", "document_id": "data-csv-auto_detection", "position": 2, "token_count": 124, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["`sniff_csv` Function"], "file_path": "data/csv/auto_detection.md", "url": "/data/csv/auto_detection", "title": "CSV Auto Detection", "category": null, "tags": [], "section_url": "/data/csv/auto_detection#sniffcsv-function"}}
{"chunk_id": "data-csv-auto_detection-003", "document_id": "data-csv-auto_detection", "content": "```", "position": 3, "token_count": 5, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "data-csv-auto_detection-003", "document_id": "data-csv-auto_detection", "position": 3, "token_count": 5, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/csv/auto_detection.md", "url": "/data/csv/auto_detection", "title": "CSV Auto Detection", "category": null, "tags": [], "section_url": "/data/csv/auto_detection"}}
{"chunk_id": "data-csv-auto_detection-004", "document_id": "data-csv-auto_detection", "content": "| Column name | Description | Example |\n|--------------------|-----------------------------------------------|-------------------------------------------------------------------|\n| `Delimiter` | Delimiter | `,` |\n| `Quote` | Quote character | `\"` |\n| `Escape` | Escape | `\\` |\n| `NewLineDelimiter` | New-line delimiter | `\\r\\n` |\n| `Comment` | Comment character | `#` |", "position": 4, "token_count": 221, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "data-csv-auto_detection-004", "document_id": "data-csv-auto_detection", "position": 4, "token_count": 221, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/csv/auto_detection.md", "url": "/data/csv/auto_detection", "title": "CSV Auto Detection", "category": null, "tags": [], "section_url": "/data/csv/auto_detection"}}
{"chunk_id": "data-csv-auto_detection-005", "document_id": "data-csv-auto_detection", "content": "| `NewLineDelimiter` | New-line delimiter | `\\r\\n` |\n| `Comment` | Comment character | `#` |\n| `SkipRows` | Number of rows skipped | 1 |\n| `HasHeader` | Whether the CSV has a header | `true` |\n| `Columns` | Column types encoded as a `LIST` of `STRUCT`s | `({'name': 'VARCHAR', 'age': 'BIGINT'})` |\n| `DateFormat` | Date format | `%d/%m/%Y` |\n| `TimestampFormat` | Timestamp Format | `%Y-%m-%dT%H:%M:%S.%f` |\n| `UserArguments` | Arguments used to invoke `sniff_csv` | `sample_size = 1000` |", "position": 5, "token_count": 202, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "data-csv-auto_detection-005", "document_id": "data-csv-auto_detection", "position": 5, "token_count": 202, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/csv/auto_detection.md", "url": "/data/csv/auto_detection", "title": "CSV Auto Detection", "category": null, "tags": [], "section_url": "/data/csv/auto_detection"}}
{"chunk_id": "data-csv-auto_detection-006", "document_id": "data-csv-auto_detection", "content": "| `UserArguments` | Arguments used to invoke `sniff_csv` | `sample_size = 1000` |\n| `Prompt` | Prompt ready to be used to read the CSV | `FROM read_csv('my_file.csv', auto_detect=false, delim=',', ...)` |", "position": 6, "token_count": 82, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "data-csv-auto_detection-006", "document_id": "data-csv-auto_detection", "position": 6, "token_count": 82, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/csv/auto_detection.md", "url": "/data/csv/auto_detection", "title": "CSV Auto Detection", "category": null, "tags": [], "section_url": "/data/csv/auto_detection"}}
{"chunk_id": "data-csv-auto_detection-007", "document_id": "data-csv-auto_detection", "content": "### Prompt\n\nThe `Prompt` column contains a SQL command with the configurations detected by the sniffer.\n\n```sql\n-- use line mode in CLI to get the full command\n.mode line\nSELECT Prompt FROM sniff_csv('my_file.csv');\n```\n\n```text\nPrompt = FROM read_csv('my_file.csv', auto_detect=false, delim=',', quote='\"', escape='\"', new_line='\\n', skip=0, header=true, columns={...});\n```", "position": 7, "token_count": 140, "has_code": true, "section_hierarchy": ["Prompt"], "metadata": {"chunk_id": "data-csv-auto_detection-007", "document_id": "data-csv-auto_detection", "position": 7, "token_count": 140, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Prompt"], "file_path": "data/csv/auto_detection.md", "url": "/data/csv/auto_detection", "title": "CSV Auto Detection", "category": null, "tags": [], "section_url": "/data/csv/auto_detection#prompt"}}
{"chunk_id": "data-csv-auto_detection-008", "document_id": "data-csv-auto_detection", "content": "## Detection Steps", "position": 8, "token_count": 6, "has_code": false, "section_hierarchy": ["Detection Steps"], "metadata": {"chunk_id": "data-csv-auto_detection-008", "document_id": "data-csv-auto_detection", "position": 8, "token_count": 6, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Detection Steps"], "file_path": "data/csv/auto_detection.md", "url": "/data/csv/auto_detection", "title": "CSV Auto Detection", "category": null, "tags": [], "section_url": "/data/csv/auto_detection#detection-steps"}}
{"chunk_id": "data-csv-auto_detection-009", "document_id": "data-csv-auto_detection", "content": "### Dialect Detection\n\nDialect detection works by attempting to parse the samples using the set of considered values. The detected dialect is the dialect that has (1) a consistent number of columns for each row, and (2) the highest number of columns for each row.\n\nThe following dialects are considered for automatic dialect detection.\n\n| Parameters | Considered values |\n|------------|-----------------------|\n| `delim` | `,` `|` `;` `\\t` |\n| `quote` | `\"` `'` (empty) |\n| `escape` | `\"` `'` `\\` (empty) |\n\nConsider the example file [`flights.csv`]({% link data/flights.csv %}):", "position": 9, "token_count": 190, "has_code": false, "section_hierarchy": ["Dialect Detection"], "metadata": {"chunk_id": "data-csv-auto_detection-009", "document_id": "data-csv-auto_detection", "position": 9, "token_count": 190, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Dialect Detection"], "file_path": "data/csv/auto_detection.md", "url": "/data/csv/auto_detection", "title": "CSV Auto Detection", "category": null, "tags": [], "section_url": "/data/csv/auto_detection#dialect-detection"}}
{"chunk_id": "data-csv-auto_detection-010", "document_id": "data-csv-auto_detection", "content": "Consider the example file [`flights.csv`]({% link data/flights.csv %}):\n\n```csv\nFlightDate|UniqueCarrier|OriginCityName|DestCityName\n1988-01-01|AA|New York, NY|Los Angeles, CA\n1988-01-02|AA|New York, NY|Los Angeles, CA\n1988-01-03|AA|New York, NY|Los Angeles, CA", "position": 10, "token_count": 99, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "data-csv-auto_detection-010", "document_id": "data-csv-auto_detection", "position": 10, "token_count": 99, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/csv/auto_detection.md", "url": "/data/csv/auto_detection", "title": "CSV Auto Detection", "category": null, "tags": [], "section_url": "/data/csv/auto_detection"}}
{"chunk_id": "data-csv-auto_detection-011", "document_id": "data-csv-auto_detection", "content": "```\n\nIn this file, the dialect detection works as follows:\n\n* If we split by a `|` every row is split into `4` columns\n* If we split by a `,` rows 2-4 are split into `3` columns, while the first row is split into `1` column\n* If we split by `;`, every row is split into `1` column\n* If we split by `\\t`, every row is split into `1` column\n\nIn this example – the system selects the `|` as the delimiter. All rows are split into the same amount of columns, and there is more than one column per row meaning the delimiter was actually found in the CSV file.", "position": 11, "token_count": 154, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "data-csv-auto_detection-011", "document_id": "data-csv-auto_detection", "position": 11, "token_count": 154, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/csv/auto_detection.md", "url": "/data/csv/auto_detection", "title": "CSV Auto Detection", "category": null, "tags": [], "section_url": "/data/csv/auto_detection"}}
{"chunk_id": "data-csv-auto_detection-012", "document_id": "data-csv-auto_detection", "content": "### Type Detection\n\nAfter detecting the dialect, the system will attempt to figure out the types of each of the columns. Note that this step is only performed if we are calling `read_csv`. In case of the `COPY` statement the types of the table that we are copying into will be used instead.\n\nThe type detection works by attempting to convert the values in each column to the candidate types. If the conversion is unsuccessful, the candidate type is removed from the set of candidate types for that column. After all samples have been handled – the remaining candidate type with the highest priority is chosen. The default set of candidate types is given below, in order of priority:\n\n| Types |\n|-------------|\n| NULL |\n| BOOLEAN |\n| TIME |\n| DATE |\n| TIMESTAMP |\n| TIMESTAMPTZ |\n| BIGINT |\n| DOUBLE |\n| VARCHAR |", "position": 12, "token_count": 194, "has_code": false, "section_hierarchy": ["Type Detection"], "metadata": {"chunk_id": "data-csv-auto_detection-012", "document_id": "data-csv-auto_detection", "position": 12, "token_count": 194, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Type Detection"], "file_path": "data/csv/auto_detection.md", "url": "/data/csv/auto_detection", "title": "CSV Auto Detection", "category": null, "tags": [], "section_url": "/data/csv/auto_detection#type-detection"}}
{"chunk_id": "data-csv-auto_detection-013", "document_id": "data-csv-auto_detection", "content": "Everything can be cast to `VARCHAR`, therefore, this type has the lowest priority meaning that all columns are converted to `VARCHAR` as a fallback if they cannot be cast to anything else.\nIn [`flights.csv`]({% link data/flights.csv %}) the `FlightDate` column will be cast to a `DATE`, while the other columns will be cast to `VARCHAR`.\n\nThe set of candidate types that should be considered by the CSV reader can be specified explicitly using the [`auto_type_candidates`]({% link docs/stable/data/csv/overview.md %}#auto_type_candidates-details) option. `VARCHAR` as the fallback type will always be considered as a candidate type whether you specify it or not.\n\nHere are all additional candidate types that may be specified using the `auto_type_candidates` option, in order of priority:\n\n| Types |\n|-----------|\n| TINYINT |\n| SMALLINT |\n| INTEGER |\n| DECIMAL |\n| FLOAT |", "position": 13, "token_count": 240, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "data-csv-auto_detection-013", "document_id": "data-csv-auto_detection", "position": 13, "token_count": 240, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/csv/auto_detection.md", "url": "/data/csv/auto_detection", "title": "CSV Auto Detection", "category": null, "tags": [], "section_url": "/data/csv/auto_detection"}}
{"chunk_id": "data-csv-auto_detection-014", "document_id": "data-csv-auto_detection", "content": "| Types |\n|-----------|\n| TINYINT |\n| SMALLINT |\n| INTEGER |\n| DECIMAL |\n| FLOAT |\n\nEven though the set of data types that can be automatically detected may appear quite limited, the CSV reader can be configured to read arbitrarily complex types by using the `types`-option described in the next section.\n\nType detection can be entirely disabled by using the `all_varchar` option. If this is set all columns will remain as `VARCHAR` (as they originally occur in the CSV file).\n\nNote that using quote characters vs. no quote characters (e.g., `\"42\"` and `42`) does not make a difference for type detection.\nQuoted fields will not be converted to `VARCHAR`, instead, the sniffer will try to find the type candidate with the highest priority.", "position": 14, "token_count": 190, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "data-csv-auto_detection-014", "document_id": "data-csv-auto_detection", "position": 14, "token_count": 190, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/csv/auto_detection.md", "url": "/data/csv/auto_detection", "title": "CSV Auto Detection", "category": null, "tags": [], "section_url": "/data/csv/auto_detection"}}
{"chunk_id": "data-csv-auto_detection-015", "document_id": "data-csv-auto_detection", "content": "#### Overriding Type Detection\n\nThe detected types can be individually overridden using the `types` option. This option takes either of two options:\n\n* A list of type definitions (e.g., `types = ['INTEGER', 'VARCHAR', 'DATE']`). This overrides the types of the columns in-order of occurrence in the CSV file.\n* Alternatively, `types` takes a `name` → `type` map which overrides options of individual columns (e.g., `types = {'quarter': 'INTEGER'}`).", "position": 15, "token_count": 132, "has_code": false, "section_hierarchy": ["Overriding Type Detection"], "metadata": {"chunk_id": "data-csv-auto_detection-015", "document_id": "data-csv-auto_detection", "position": 15, "token_count": 132, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Overriding Type Detection"], "file_path": "data/csv/auto_detection.md", "url": "/data/csv/auto_detection", "title": "CSV Auto Detection", "category": null, "tags": [], "section_url": "/data/csv/auto_detection#overriding-type-detection"}}
{"chunk_id": "data-csv-auto_detection-016", "document_id": "data-csv-auto_detection", "content": "The set of column types that may be specified using the `types` option is not as limited as the types available for the `auto_type_candidates` option: any valid type definition is acceptable to the `types`-option. (To get a valid type definition, use the [`typeof()`]({% link docs/stable/sql/functions/utility.md %}#typeofexpression) function, or use the `column_type` column of the [`DESCRIBE`]({% link docs/stable/guides/meta/describe.md %}) result.)\n\nThe `sniff_csv()` function's `Column` field returns a struct with column names and types that can be used as a basis for overriding types.", "position": 16, "token_count": 171, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "data-csv-auto_detection-016", "document_id": "data-csv-auto_detection", "position": 16, "token_count": 171, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/csv/auto_detection.md", "url": "/data/csv/auto_detection", "title": "CSV Auto Detection", "category": null, "tags": [], "section_url": "/data/csv/auto_detection"}}
{"chunk_id": "data-csv-auto_detection-017", "document_id": "data-csv-auto_detection", "content": "## Header Detection\n\nHeader detection works by checking if the candidate header row deviates from the other rows in the file in terms of types. For example, in [`flights.csv`]({% link data/flights.csv %}), we can see that the header row consists of only `VARCHAR` columns – whereas the values contain a `DATE` value for the `FlightDate` column. As such – the system defines the first row as the header row and extracts the column names from the header row.\n\nIn files that do not have a header row, the column names are generated as `column0`, `column1`, etc.\n\nNote that headers cannot be detected correctly if all columns are of type `VARCHAR` – as in this case the system cannot distinguish the header row from the other rows in the file. In this case, the system assumes the file has a header. This can be overridden by setting the `header` option to `false`.", "position": 17, "token_count": 211, "has_code": false, "section_hierarchy": ["Header Detection"], "metadata": {"chunk_id": "data-csv-auto_detection-017", "document_id": "data-csv-auto_detection", "position": 17, "token_count": 211, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Header Detection"], "file_path": "data/csv/auto_detection.md", "url": "/data/csv/auto_detection", "title": "CSV Auto Detection", "category": null, "tags": [], "section_url": "/data/csv/auto_detection#header-detection"}}
{"chunk_id": "data-csv-auto_detection-018", "document_id": "data-csv-auto_detection", "content": "### Dates and Timestamps\n\nDuckDB supports the [ISO 8601 format](https://en.wikipedia.org/wiki/ISO_8601) format by default for timestamps, dates and times. Unfortunately, not all dates and times are formatted using this standard. For that reason, the CSV reader also supports the `dateformat` and `timestampformat` options. Using this format the user can specify a [format string]({% link docs/stable/sql/functions/dateformat.md %}) that specifies how the date or timestamp should be read.", "position": 18, "token_count": 137, "has_code": false, "section_hierarchy": ["Dates and Timestamps"], "metadata": {"chunk_id": "data-csv-auto_detection-018", "document_id": "data-csv-auto_detection", "position": 18, "token_count": 137, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Dates and Timestamps"], "file_path": "data/csv/auto_detection.md", "url": "/data/csv/auto_detection", "title": "CSV Auto Detection", "category": null, "tags": [], "section_url": "/data/csv/auto_detection#dates-and-timestamps"}}
{"chunk_id": "data-csv-auto_detection-019", "document_id": "data-csv-auto_detection", "content": "As part of the auto-detection, the system tries to figure out if dates and times are stored in a different representation. This is not always possible – as there are ambiguities in the representation. For example, the date `01-02-2000` can be parsed as either January 2nd or February 1st. Often these ambiguities can be resolved. For example, if we later encounter the date `21-02-2000` then we know that the format must have been `DD-MM-YYYY`. `MM-DD-YYYY` is no longer possible as there is no 21nd month.\n\nIf the ambiguities cannot be resolved by looking at the data the system has a list of preferences for which date format to use. If the system chooses incorrectly, the user can specify the `dateformat` and `timestampformat` options manually.\n\nThe system considers the following formats for dates (`dateformat`). Higher entries are chosen over lower entries in case of ambiguities (i.e., ISO 8601 is preferred over `MM-DD-YYYY`).", "position": 19, "token_count": 245, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "data-csv-auto_detection-019", "document_id": "data-csv-auto_detection", "position": 19, "token_count": 245, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/csv/auto_detection.md", "url": "/data/csv/auto_detection", "title": "CSV Auto Detection", "category": null, "tags": [], "section_url": "/data/csv/auto_detection"}}
{"chunk_id": "data-csv-auto_detection-020", "document_id": "data-csv-auto_detection", "content": "| dateformat |\n|------------|\n| ISO 8601 |\n| %y-%m-%d |\n| %Y-%m-%d |\n| %d-%m-%y |\n| %d-%m-%Y |\n| %m-%d-%y |\n| %m-%d-%Y |\n\nThe system considers the following formats for timestamps (`timestampformat`). Higher entries are chosen over lower entries in case of ambiguities.", "position": 20, "token_count": 121, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "data-csv-auto_detection-020", "document_id": "data-csv-auto_detection", "position": 20, "token_count": 121, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/csv/auto_detection.md", "url": "/data/csv/auto_detection", "title": "CSV Auto Detection", "category": null, "tags": [], "section_url": "/data/csv/auto_detection"}}
{"chunk_id": "data-csv-auto_detection-021", "document_id": "data-csv-auto_detection", "content": "The system considers the following formats for timestamps (`timestampformat`). Higher entries are chosen over lower entries in case of ambiguities.\n\n| timestampformat |\n|----------------------|\n| ISO 8601 |\n| %y-%m-%d %H:%M:%S |\n| %Y-%m-%d %H:%M:%S |\n| %d-%m-%y %H:%M:%S |\n| %d-%m-%Y %H:%M:%S |\n| %m-%d-%y %I:%M:%S %p |\n| %m-%d-%Y %I:%M:%S %p |\n| %Y-%m-%d %H:%M:%S.%f |", "position": 21, "token_count": 206, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "data-csv-auto_detection-021", "document_id": "data-csv-auto_detection", "position": 21, "token_count": 206, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/csv/auto_detection.md", "url": "/data/csv/auto_detection", "title": "CSV Auto Detection", "category": null, "tags": [], "section_url": "/data/csv/auto_detection"}}
