{"chunk_id": "guides-performance-how_to_tune_workloads-000", "document_id": "guides-performance-how_to_tune_workloads", "content": "## The `preserve_insertion_order` Option\n\nWhen importing or exporting datasets (from/to the Parquet or CSV formats), which are much larger than the available memory, an out of memory error may occur:\n\n```console\nOut of Memory Error: failed to allocate data of size ... (.../... used)\n```\n\nIn these cases, consider setting the [`preserve_insertion_order` configuration option]({% link docs/stable/configuration/overview.md %}) to `false`:\n\n```sql\nSET preserve_insertion_order = false;\n```\n\nThis allows the systems to re-order any results that do not contain `ORDER BY` clauses, potentially reducing memory usage.", "position": 0, "token_count": 166, "has_code": true, "section_hierarchy": ["The `preserve_insertion_order` Option"], "metadata": {"chunk_id": "guides-performance-how_to_tune_workloads-000", "document_id": "guides-performance-how_to_tune_workloads", "position": 0, "token_count": 166, "has_code": true, "overlap_with_previous": false, "section_hierarchy": ["The `preserve_insertion_order` Option"], "file_path": "guides/performance/how_to_tune_workloads.md", "url": "/guides/performance/how_to_tune_workloads", "title": "Tuning Workloads", "category": null, "tags": [], "section_url": "/guides/performance/how_to_tune_workloads#the-preserveinsertionorder-option"}}
{"chunk_id": "guides-performance-how_to_tune_workloads-001", "document_id": "guides-performance-how_to_tune_workloads", "content": "## Parallelism (Multi-Core Processing)\n\n### The Effect of Row Groups on Parallelism\n\nDuckDB parallelizes the workload based on _[row groups]({% link docs/stable/internals/storage.md %}#row-groups),_ i.e., groups of rows that are stored together at the storage level.\nThe default row group size in DuckDB's database format is 122,880 rows.\nParallelism starts at the level of row groups, therefore, for a query to run on _k_ threads, it needs to scan at least _k_ \\* 122,880 rows.\n\nThe row group size can be specified as an option of the `ATTACH` statement:\n\n```sql\nATTACH '/tmp/somefile.db' AS db (ROW_GROUP_SIZE 16384);\n```\n\nThe [performance considerations when chosing `ROW_GROUP_SIZE` for Parquet files]({% link docs/stable/data/parquet/tips.md %}#selecting-a-row_group_size) apply verbatim to DuckDB's own database format.", "position": 1, "token_count": 245, "has_code": true, "section_hierarchy": ["The Effect of Row Groups on Parallelism"], "metadata": {"chunk_id": "guides-performance-how_to_tune_workloads-001", "document_id": "guides-performance-how_to_tune_workloads", "position": 1, "token_count": 245, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["The Effect of Row Groups on Parallelism"], "file_path": "guides/performance/how_to_tune_workloads.md", "url": "/guides/performance/how_to_tune_workloads", "title": "Tuning Workloads", "category": null, "tags": [], "section_url": "/guides/performance/how_to_tune_workloads#the-effect-of-row-groups-on-parallelism"}}
{"chunk_id": "guides-performance-how_to_tune_workloads-002", "document_id": "guides-performance-how_to_tune_workloads", "content": "### Too Many Threads\n\nNote that in certain cases DuckDB may launch _too many threads_ (e.g., due to HyperThreading), which can lead to slowdowns. In these cases, it’s worth manually limiting the number of threads using [`SET threads = X`]({% link docs/stable/configuration/pragmas.md %}#threads).", "position": 2, "token_count": 89, "has_code": false, "section_hierarchy": ["Too Many Threads"], "metadata": {"chunk_id": "guides-performance-how_to_tune_workloads-002", "document_id": "guides-performance-how_to_tune_workloads", "position": 2, "token_count": 89, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Too Many Threads"], "file_path": "guides/performance/how_to_tune_workloads.md", "url": "/guides/performance/how_to_tune_workloads", "title": "Tuning Workloads", "category": null, "tags": [], "section_url": "/guides/performance/how_to_tune_workloads#too-many-threads"}}
{"chunk_id": "guides-performance-how_to_tune_workloads-003", "document_id": "guides-performance-how_to_tune_workloads", "content": "## Larger-than-Memory Workloads (Out-of-Core Processing)\n\nA key strength of DuckDB is support for larger-than-memory workloads, i.e., it is able to process datasets that are larger than the available system memory (also known as _out-of-core processing_).\nIt can also run queries where the intermediate results cannot fit into memory.\nThis section explains the prerequisites, scope, and known limitations of larger-than-memory processing in DuckDB.", "position": 3, "token_count": 113, "has_code": false, "section_hierarchy": ["Larger-than-Memory Workloads (Out-of-Core Processing)"], "metadata": {"chunk_id": "guides-performance-how_to_tune_workloads-003", "document_id": "guides-performance-how_to_tune_workloads", "position": 3, "token_count": 113, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Larger-than-Memory Workloads (Out-of-Core Processing)"], "file_path": "guides/performance/how_to_tune_workloads.md", "url": "/guides/performance/how_to_tune_workloads", "title": "Tuning Workloads", "category": null, "tags": [], "section_url": "/guides/performance/how_to_tune_workloads#larger-than-memory-workloads-out-of-core-processing"}}
{"chunk_id": "guides-performance-how_to_tune_workloads-004", "document_id": "guides-performance-how_to_tune_workloads", "content": "### Spilling to Disk\n\nLarger-than-memory workloads are supported by spilling to disk.\nWith the default configuration, DuckDB creates the `⟨database_file_name⟩.tmp`{:.language-sql .highlight} temporary directory (in persistent mode) or the `.tmp`{:.language-sql .highlight} directory (in in-memory mode). This directory can be changed using the [`temp_directory` configuration option]({% link docs/stable/configuration/pragmas.md %}#temp-directory-for-spilling-data-to-disk), e.g.:\n\n```sql\nSET temp_directory = '/path/to/temp_dir.tmp/';\n```", "position": 4, "token_count": 171, "has_code": true, "section_hierarchy": ["Spilling to Disk"], "metadata": {"chunk_id": "guides-performance-how_to_tune_workloads-004", "document_id": "guides-performance-how_to_tune_workloads", "position": 4, "token_count": 171, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Spilling to Disk"], "file_path": "guides/performance/how_to_tune_workloads.md", "url": "/guides/performance/how_to_tune_workloads", "title": "Tuning Workloads", "category": null, "tags": [], "section_url": "/guides/performance/how_to_tune_workloads#spilling-to-disk"}}
{"chunk_id": "guides-performance-how_to_tune_workloads-005", "document_id": "guides-performance-how_to_tune_workloads", "content": "### Blocking Operators\n\nSome operators cannot output a single row until the last row of their input has been seen.\nThese are called _blocking operators_ as they require their entire input to be buffered,\nand are the most memory-intensive operators in relational database systems.\nThe main blocking operators are the following:\n\n- _grouping:_ [`GROUP BY`]({% link docs/stable/sql/query_syntax/groupby.md %})\n- _joining:_ [`JOIN`]({% link docs/stable/sql/query_syntax/from.md %}#joins)\n- _sorting:_ [`ORDER BY`]({% link docs/stable/sql/query_syntax/orderby.md %})\n- _windowing:_ [`OVER ... (PARTITION BY ... ORDER BY ...)`]({% link docs/stable/sql/functions/window_functions.md %})\n\nDuckDB supports larger-than-memory processing for all of these operators.", "position": 5, "token_count": 225, "has_code": false, "section_hierarchy": ["Blocking Operators"], "metadata": {"chunk_id": "guides-performance-how_to_tune_workloads-005", "document_id": "guides-performance-how_to_tune_workloads", "position": 5, "token_count": 225, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Blocking Operators"], "file_path": "guides/performance/how_to_tune_workloads.md", "url": "/guides/performance/how_to_tune_workloads", "title": "Tuning Workloads", "category": null, "tags": [], "section_url": "/guides/performance/how_to_tune_workloads#blocking-operators"}}
{"chunk_id": "guides-performance-how_to_tune_workloads-006", "document_id": "guides-performance-how_to_tune_workloads", "content": "### Limitations\n\nDuckDB strives to always complete workloads even if they are larger-than-memory.\nThat said, there are some limitations at the moment:", "position": 6, "token_count": 37, "has_code": false, "section_hierarchy": ["Limitations"], "metadata": {"chunk_id": "guides-performance-how_to_tune_workloads-006", "document_id": "guides-performance-how_to_tune_workloads", "position": 6, "token_count": 37, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Limitations"], "file_path": "guides/performance/how_to_tune_workloads.md", "url": "/guides/performance/how_to_tune_workloads", "title": "Tuning Workloads", "category": null, "tags": [], "section_url": "/guides/performance/how_to_tune_workloads#limitations"}}
{"chunk_id": "guides-performance-how_to_tune_workloads-007", "document_id": "guides-performance-how_to_tune_workloads", "content": "- If multiple blocking operators appear in the same query, DuckDB may still throw an out-of-memory exception due to the complex interplay of these operators.\n- Some [aggregate functions]({% link docs/stable/sql/functions/aggregates.md %}), such as `list()` and `string_agg()`, do not support offloading to disk.\n- [Aggregate functions that use sorting]({% link docs/stable/sql/functions/aggregates.md %}#order-by-clause-in-aggregate-functions) are holistic, i.e., they need all inputs before the aggregation can start. As DuckDB cannot yet offload some complex intermediate aggregate states to disk, these functions can cause an out-of-memory exception when run on large datasets.\n- The `PIVOT` operation [internally uses the `list()` function]({% link docs/stable/sql/statements/pivot.md %}#internals), therefore it is subject to the same limitation.", "position": 7, "token_count": 231, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "guides-performance-how_to_tune_workloads-007", "document_id": "guides-performance-how_to_tune_workloads", "position": 7, "token_count": 231, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "guides/performance/how_to_tune_workloads.md", "url": "/guides/performance/how_to_tune_workloads", "title": "Tuning Workloads", "category": null, "tags": [], "section_url": "/guides/performance/how_to_tune_workloads"}}
{"chunk_id": "guides-performance-how_to_tune_workloads-008", "document_id": "guides-performance-how_to_tune_workloads", "content": "## Profiling\n\nIf your queries are not performing as well as expected, it’s worth studying their query plans:\n\n- Use [`EXPLAIN`]({% link docs/stable/guides/meta/explain.md %}) to print the physical query plan without running the query.\n- Use [`EXPLAIN ANALYZE`]({% link docs/stable/guides/meta/explain_analyze.md %}) to run and profile the query. This will show the CPU time that each step in the query takes. Note that due to multi-threading, adding up the individual times will be larger than the total query processing time.\n\nQuery plans can point to the root of performance issues. A few general directions:\n\n- Avoid nested loop joins in favor of hash joins.\n- A scan that does not include a filter pushdown for a filter condition that is later applied performs unnecessary IO. Try rewriting the query to apply a pushdown.\n- Bad join orders where the cardinality of an operator explodes to billions of tuples should be avoided at all costs.", "position": 8, "token_count": 224, "has_code": false, "section_hierarchy": ["Profiling"], "metadata": {"chunk_id": "guides-performance-how_to_tune_workloads-008", "document_id": "guides-performance-how_to_tune_workloads", "position": 8, "token_count": 224, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Profiling"], "file_path": "guides/performance/how_to_tune_workloads.md", "url": "/guides/performance/how_to_tune_workloads", "title": "Tuning Workloads", "category": null, "tags": [], "section_url": "/guides/performance/how_to_tune_workloads#profiling"}}
{"chunk_id": "guides-performance-how_to_tune_workloads-009", "document_id": "guides-performance-how_to_tune_workloads", "content": "## Prepared Statements\n\n[Prepared statements]({% link docs/stable/sql/query_syntax/prepared_statements.md %}) can improve performance when running the same query many times, but with different parameters. When a statement is prepared, it completes several of the initial portions of the query execution process (parsing, planning, etc.) and caches their output. When it is executed, those steps can be skipped, improving performance. This is beneficial mostly for repeatedly running small queries (with a runtime of < 100ms) with different sets of parameters.\n\nNote that it is not a primary design goal for DuckDB to quickly execute many small queries concurrently. Rather, it is optimized for running larger, less frequent queries.", "position": 9, "token_count": 159, "has_code": false, "section_hierarchy": ["Prepared Statements"], "metadata": {"chunk_id": "guides-performance-how_to_tune_workloads-009", "document_id": "guides-performance-how_to_tune_workloads", "position": 9, "token_count": 159, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Prepared Statements"], "file_path": "guides/performance/how_to_tune_workloads.md", "url": "/guides/performance/how_to_tune_workloads", "title": "Tuning Workloads", "category": null, "tags": [], "section_url": "/guides/performance/how_to_tune_workloads#prepared-statements"}}
{"chunk_id": "guides-performance-how_to_tune_workloads-010", "document_id": "guides-performance-how_to_tune_workloads", "content": "## Querying Remote Files\n\nDuckDB uses synchronous IO when reading remote files. This means that each DuckDB thread can make at most one HTTP request at a time. If a query must make many small requests over the network, increasing DuckDB's [`threads` setting]({% link docs/stable/configuration/pragmas.md %}#threads) to larger than the total number of CPU cores (approx. 2-5 times CPU cores) can improve parallelism and performance.", "position": 10, "token_count": 109, "has_code": false, "section_hierarchy": ["Querying Remote Files"], "metadata": {"chunk_id": "guides-performance-how_to_tune_workloads-010", "document_id": "guides-performance-how_to_tune_workloads", "position": 10, "token_count": 109, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Querying Remote Files"], "file_path": "guides/performance/how_to_tune_workloads.md", "url": "/guides/performance/how_to_tune_workloads", "title": "Tuning Workloads", "category": null, "tags": [], "section_url": "/guides/performance/how_to_tune_workloads#querying-remote-files"}}
{"chunk_id": "guides-performance-how_to_tune_workloads-011", "document_id": "guides-performance-how_to_tune_workloads", "content": "### Avoid Reading Unnecessary Data\n\nThe main bottleneck in workloads reading remote files is likely to be the IO. This means that minimizing the unnecessarily read data can be highly beneficial.\n\nSome basic SQL tricks can help with this:\n\n- Avoid `SELECT *`. Instead, only select columns that are actually used. DuckDB will try to only download the data it actually needs.\n- Apply filters on remote Parquet files when possible. DuckDB can use these filters to reduce the amount of data that is scanned.\n- Either [sort]({% link docs/stable/sql/query_syntax/orderby.md %}) or [partition]({% link docs/stable/data/partitioning/partitioned_writes.md %}) data by columns that are regularly used for filters: this increases the effectiveness of the filters in reducing IO.\n\nTo inspect how much remote data is transferred for a query, [`EXPLAIN ANALYZE`]({% link docs/stable/guides/meta/explain_analyze.md %}) can be used to print out the total number of requests and total data transferred for queries on remote files.", "position": 11, "token_count": 248, "has_code": false, "section_hierarchy": ["Avoid Reading Unnecessary Data"], "metadata": {"chunk_id": "guides-performance-how_to_tune_workloads-011", "document_id": "guides-performance-how_to_tune_workloads", "position": 11, "token_count": 248, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Avoid Reading Unnecessary Data"], "file_path": "guides/performance/how_to_tune_workloads.md", "url": "/guides/performance/how_to_tune_workloads", "title": "Tuning Workloads", "category": null, "tags": [], "section_url": "/guides/performance/how_to_tune_workloads#avoid-reading-unnecessary-data"}}
{"chunk_id": "guides-performance-how_to_tune_workloads-012", "document_id": "guides-performance-how_to_tune_workloads", "content": "### Caching\n\nStarting with version 1.3.0, DuckDB supports caching remote data. To inspect the content of the external file cache, run:\n\n```sql\nFROM duckdb_external_file_cache();\n```", "position": 12, "token_count": 55, "has_code": true, "section_hierarchy": ["Caching"], "metadata": {"chunk_id": "guides-performance-how_to_tune_workloads-012", "document_id": "guides-performance-how_to_tune_workloads", "position": 12, "token_count": 55, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Caching"], "file_path": "guides/performance/how_to_tune_workloads.md", "url": "/guides/performance/how_to_tune_workloads", "title": "Tuning Workloads", "category": null, "tags": [], "section_url": "/guides/performance/how_to_tune_workloads#caching"}}
{"chunk_id": "guides-performance-how_to_tune_workloads-013", "document_id": "guides-performance-how_to_tune_workloads", "content": "## Best Practices for Using Connections\n\nDuckDB will perform best when reusing the same database connection many times. Disconnecting and reconnecting on every query will incur some overhead, which can reduce performance when running many small queries. DuckDB also caches some data and metadata in memory, and that cache is lost when the last open connection is closed. Frequently, a single connection will work best, but a connection pool may also be used.\n\nUsing multiple connections can parallelize some operations, although it is typically not necessary. DuckDB does attempt to parallelize as much as possible within each individual query, but it is not possible to parallelize in all cases. Making multiple connections can process more operations concurrently. This can be more helpful if DuckDB is not CPU limited, but instead bottlenecked by another resource like network transfer speed.", "position": 13, "token_count": 173, "has_code": false, "section_hierarchy": ["Best Practices for Using Connections"], "metadata": {"chunk_id": "guides-performance-how_to_tune_workloads-013", "document_id": "guides-performance-how_to_tune_workloads", "position": 13, "token_count": 173, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Best Practices for Using Connections"], "file_path": "guides/performance/how_to_tune_workloads.md", "url": "/guides/performance/how_to_tune_workloads", "title": "Tuning Workloads", "category": null, "tags": [], "section_url": "/guides/performance/how_to_tune_workloads#best-practices-for-using-connections"}}
{"chunk_id": "guides-performance-how_to_tune_workloads-014", "document_id": "guides-performance-how_to_tune_workloads", "content": "## Persistent vs. In-Memory Tables\n\nDuckDB supports [lightweight compression techniques]({% post_url 2022-10-28-lightweight-compression %}). By default, compression is only applied on persistent (on-disk) databases and not on in-memory tables.\n\nIn some cases, this can result in counter-intuitive performance results where queries are faster on on-disk tables compared to in-memory ones. Let's take Q1 of the [TPC-H workload]({% link docs/stable/core_extensions/tpch.md %}) for example on the SF30 dataset:\n\n```sql\nCALL dbgen(sf = 30);\n.timer on\nPRAGMA tpch(1);", "position": 14, "token_count": 165, "has_code": true, "section_hierarchy": ["Persistent vs. In-Memory Tables"], "metadata": {"chunk_id": "guides-performance-how_to_tune_workloads-014", "document_id": "guides-performance-how_to_tune_workloads", "position": 14, "token_count": 165, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Persistent vs. In-Memory Tables"], "file_path": "guides/performance/how_to_tune_workloads.md", "url": "/guides/performance/how_to_tune_workloads", "title": "Tuning Workloads", "category": null, "tags": [], "section_url": "/guides/performance/how_to_tune_workloads#persistent-vs-in-memory-tables"}}
{"chunk_id": "guides-performance-how_to_tune_workloads-015", "document_id": "guides-performance-how_to_tune_workloads", "content": "```\n\nWe run this script using three DuckDB prompts:\n\n| Database setup | DuckDB prompt | Execution time |\n| --------------------------- | ----------------------------------------------------------- | -------------: |\n| In-memory DB (uncompressed) | `duckdb` | 4.22 s |\n| In-memory DB (compressed) | `duckdb -cmd \"ATTACH ':memory:' AS db (COMPRESS); USE db;\"` | 0.55 s |\n| Persistent DB (compressed) | `duckdb tpch-sf30.db` | 0.56 s |\n\nWe can observe that the compressed databases about 8× faster compared to the uncompressed in-memory database.", "position": 15, "token_count": 239, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "guides-performance-how_to_tune_workloads-015", "document_id": "guides-performance-how_to_tune_workloads", "position": 15, "token_count": 239, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "guides/performance/how_to_tune_workloads.md", "url": "/guides/performance/how_to_tune_workloads", "title": "Tuning Workloads", "category": null, "tags": [], "section_url": "/guides/performance/how_to_tune_workloads"}}
