{"chunk_id": "guides-performance-import-000", "document_id": "guides-performance-import", "content": "## Recommended Import Methods\n\nWhen importing data from other systems to DuckDB, there are several considerations to take into account.\nWe recommend importing using the following order:", "position": 0, "token_count": 36, "has_code": false, "section_hierarchy": ["Recommended Import Methods"], "metadata": {"chunk_id": "guides-performance-import-000", "document_id": "guides-performance-import", "position": 0, "token_count": 36, "has_code": false, "overlap_with_previous": false, "section_hierarchy": ["Recommended Import Methods"], "file_path": "guides/performance/import.md", "url": "/guides/performance/import", "title": "Data Import", "category": null, "tags": [], "section_url": "/guides/performance/import#recommended-import-methods"}}
{"chunk_id": "guides-performance-import-001", "document_id": "guides-performance-import", "content": "1. For systems which are supported by a DuckDB scanner extension, it's preferable to use the scanner. DuckDB currently offers scanners for [MySQL]({% link docs/stable/guides/database_integration/mysql.md %}), [PostgreSQL]({% link docs/stable/guides/database_integration/postgres.md %}), and [SQLite]({% link docs/stable/guides/database_integration/sqlite.md %}).\n2. If there is a bulk export feature in the data source system, export the data to Parquet or CSV format, then load it using DuckDB's [Parquet]({% link docs/stable/guides/file_formats/parquet_import.md %}) or [CSV loader]({% link docs/stable/guides/file_formats/csv_import.md %}).", "position": 1, "token_count": 217, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "guides-performance-import-001", "document_id": "guides-performance-import", "position": 1, "token_count": 217, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "guides/performance/import.md", "url": "/guides/performance/import", "title": "Data Import", "category": null, "tags": [], "section_url": "/guides/performance/import"}}
{"chunk_id": "guides-performance-import-002", "document_id": "guides-performance-import", "content": "3. If the approaches above are not applicable, consider using the DuckDB [appender]({% link docs/stable/data/appender.md %}), currently available in the C, C++, Go, Java, and Rust APIs.", "position": 2, "token_count": 61, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "guides-performance-import-002", "document_id": "guides-performance-import", "position": 2, "token_count": 61, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "guides/performance/import.md", "url": "/guides/performance/import", "title": "Data Import", "category": null, "tags": [], "section_url": "/guides/performance/import"}}
{"chunk_id": "guides-performance-import-003", "document_id": "guides-performance-import", "content": "## Methods to Avoid\n\nIf possible, avoid looping row-by-row (tuple-at-a-time) in favor of bulk operations.\nPerforming row-by-row inserts (even with prepared statements) is detrimental to performance and will result in slow load times.\n\n> Bestpractice Unless your data is small (<100k rows), avoid using inserts in loops.", "position": 3, "token_count": 84, "has_code": false, "section_hierarchy": ["Methods to Avoid"], "metadata": {"chunk_id": "guides-performance-import-003", "document_id": "guides-performance-import", "position": 3, "token_count": 84, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Methods to Avoid"], "file_path": "guides/performance/import.md", "url": "/guides/performance/import", "title": "Data Import", "category": null, "tags": [], "section_url": "/guides/performance/import#methods-to-avoid"}}
