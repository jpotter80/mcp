{"chunk_id": "data-parquet-overview-000", "document_id": "data-parquet-overview", "content": "## Examples\n\nRead a single Parquet file:\n\n```sql\nSELECT * FROM 'test.parquet';\n```\n\nFigure out which columns/types are in a Parquet file:\n\n```sql\nDESCRIBE SELECT * FROM 'test.parquet';\n```\n\nCreate a table from a Parquet file:\n\n```sql\nCREATE TABLE test AS\n SELECT * FROM 'test.parquet';\n```\n\nIf the file does not end in `.parquet`, use the `read_parquet` function:\n\n```sql\nSELECT *\nFROM read_parquet('test.parq');\n```\n\nUse list parameter to read three Parquet files and treat them as a single table:\n\n```sql\nSELECT *\nFROM read_parquet(['file1.parquet', 'file2.parquet', 'file3.parquet']);\n```\n\nRead all files that match the glob pattern:\n\n```sql\nSELECT *\nFROM 'test/*.parquet';", "position": 0, "token_count": 222, "has_code": true, "section_hierarchy": ["Examples"], "metadata": {"chunk_id": "data-parquet-overview-000", "document_id": "data-parquet-overview", "position": 0, "token_count": 222, "has_code": true, "overlap_with_previous": false, "section_hierarchy": ["Examples"], "file_path": "data/parquet/overview.md", "url": "/data/parquet/overview", "title": "Reading and Writing Parquet Files", "category": null, "tags": [], "section_url": "/data/parquet/overview#examples"}}
{"chunk_id": "data-parquet-overview-001", "document_id": "data-parquet-overview", "content": "```\n\nRead all files that match the glob pattern:\n\n```sql\nSELECT *\nFROM 'test/*.parquet';\n```\n\nRead all files that match the glob pattern, and include the `filename` virtual column that specifies which file each row came from (this column is available by default without a configuration options since DuckDB v1.3.0):\n\n```sql\nSELECT *, filename\nFROM read_parquet('test/*.parquet');\n```\n\nUse a list of globs to read all Parquet files from two specific folders:\n\n```sql\nSELECT *\nFROM read_parquet(['folder1/*.parquet', 'folder2/*.parquet']);\n```\n\nRead over HTTPS:\n\n```sql\nSELECT *\nFROM read_parquet('https://some.url/some_file.parquet');", "position": 1, "token_count": 205, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "data-parquet-overview-001", "document_id": "data-parquet-overview", "position": 1, "token_count": 205, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/parquet/overview.md", "url": "/data/parquet/overview", "title": "Reading and Writing Parquet Files", "category": null, "tags": [], "section_url": "/data/parquet/overview"}}
{"chunk_id": "data-parquet-overview-002", "document_id": "data-parquet-overview", "content": "```\n\nRead over HTTPS:\n\n```sql\nSELECT *\nFROM read_parquet('https://some.url/some_file.parquet');\n```\n\nQuery the [metadata of a Parquet file]({% link docs/stable/data/parquet/metadata.md %}#parquet-metadata):\n\n```sql\nSELECT *\nFROM parquet_metadata('test.parquet');\n```\n\nQuery the [file metadata of a Parquet file]({% link docs/stable/data/parquet/metadata.md %}#parquet-file-metadata):\n\n```sql\nSELECT *\nFROM parquet_file_metadata('test.parquet');\n```\n\nQuery the [key-value metadata of a Parquet file]({% link docs/stable/data/parquet/metadata.md %}#parquet-key-value-metadata):\n\n```sql\nSELECT *\nFROM parquet_kv_metadata('test.parquet');", "position": 2, "token_count": 231, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "data-parquet-overview-002", "document_id": "data-parquet-overview", "position": 2, "token_count": 231, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/parquet/overview.md", "url": "/data/parquet/overview", "title": "Reading and Writing Parquet Files", "category": null, "tags": [], "section_url": "/data/parquet/overview"}}
{"chunk_id": "data-parquet-overview-003", "document_id": "data-parquet-overview", "content": "```\n\nQuery the [schema of a Parquet file]({% link docs/stable/data/parquet/metadata.md %}#parquet-schema):\n\n```sql\nSELECT *\nFROM parquet_schema('test.parquet');\n```\n\nWrite the results of a query to a Parquet file using the default compression (Snappy):\n\n```sql\nCOPY\n (SELECT * FROM tbl)\n TO 'result-snappy.parquet'\n (FORMAT parquet);\n```\n\nWrite the results from a query to a Parquet file with specific compression and row group size:\n\n```sql\nCOPY\n (FROM generate_series(100_000))\n TO 'test.parquet'\n (FORMAT parquet, COMPRESSION zstd, ROW_GROUP_SIZE 100_000);\n```\n\nExport the table contents of the entire database as parquet:\n\n```sql\nEXPORT DATABASE 'target_directory' (FORMAT parquet);\n```", "position": 3, "token_count": 215, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "data-parquet-overview-003", "document_id": "data-parquet-overview", "position": 3, "token_count": 215, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/parquet/overview.md", "url": "/data/parquet/overview", "title": "Reading and Writing Parquet Files", "category": null, "tags": [], "section_url": "/data/parquet/overview"}}
{"chunk_id": "data-parquet-overview-004", "document_id": "data-parquet-overview", "content": "## Parquet Files\n\nParquet files are compressed columnar files that are efficient to load and process. DuckDB provides support for both reading and writing Parquet files in an efficient manner, as well as support for pushing filters and projections into the Parquet file scans.\n\n> Parquet datasets differ based on the number of files, the size of individual files, the compression algorithm used, row group size, etc. These have a significant effect on performance. Please consult the [Performance Guide]({% link docs/stable/guides/performance/file_formats.md %}) for details.", "position": 4, "token_count": 126, "has_code": false, "section_hierarchy": ["Parquet Files"], "metadata": {"chunk_id": "data-parquet-overview-004", "document_id": "data-parquet-overview", "position": 4, "token_count": 126, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Parquet Files"], "file_path": "data/parquet/overview.md", "url": "/data/parquet/overview", "title": "Reading and Writing Parquet Files", "category": null, "tags": [], "section_url": "/data/parquet/overview#parquet-files"}}
{"chunk_id": "data-parquet-overview-005", "document_id": "data-parquet-overview", "content": "## `read_parquet` Function\n\n| Function | Description | Example |\n|:--|:--|:-----|\n| `read_parquet(path_or_list_of_paths)` | Read Parquet file(s) | `SELECT * FROM read_parquet('test.parquet');` |\n| `parquet_scan(path_or_list_of_paths)` | Alias for `read_parquet` | `SELECT * FROM parquet_scan('test.parquet');` |\n\nIf your file ends in `.parquet`, the function syntax is optional. The system will automatically infer that you are reading a Parquet file:\n\n```sql\nSELECT * FROM 'test.parquet';\n```\n\nMultiple files can be read at once by providing a glob or a list of files. Refer to the [multiple files section]({% link docs/stable/data/multiple_files/overview.md %}) for more information.", "position": 5, "token_count": 228, "has_code": true, "section_hierarchy": ["`read_parquet` Function"], "metadata": {"chunk_id": "data-parquet-overview-005", "document_id": "data-parquet-overview", "position": 5, "token_count": 228, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["`read_parquet` Function"], "file_path": "data/parquet/overview.md", "url": "/data/parquet/overview", "title": "Reading and Writing Parquet Files", "category": null, "tags": [], "section_url": "/data/parquet/overview#readparquet-function"}}
{"chunk_id": "data-parquet-overview-006", "document_id": "data-parquet-overview", "content": "### Parameters\n\nThere are a number of options exposed that can be passed to the `read_parquet` function or the [`COPY` statement]({% link docs/stable/sql/statements/copy.md %}).", "position": 6, "token_count": 54, "has_code": false, "section_hierarchy": ["Parameters"], "metadata": {"chunk_id": "data-parquet-overview-006", "document_id": "data-parquet-overview", "position": 6, "token_count": 54, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Parameters"], "file_path": "data/parquet/overview.md", "url": "/data/parquet/overview", "title": "Reading and Writing Parquet Files", "category": null, "tags": [], "section_url": "/data/parquet/overview#parameters"}}
{"chunk_id": "data-parquet-overview-007", "document_id": "data-parquet-overview", "content": "| Name | Description | Type | Default |\n|:--|:-----|:-|:-|\n| `binary_as_string` | Parquet files generated by legacy writers do not correctly set the `UTF8` flag for strings, causing string columns to be loaded as `BLOB` instead. Set this to true to load binary columns as strings. | `BOOL` | `false` |\n| `encryption_config` | Configuration for [Parquet encryption]({% link docs/stable/data/parquet/encryption.md %}). | `STRUCT` | - |\n| `filename` | Whether or not an extra `filename` column should be included in the result. Since DuckDB v1.3.0, the `filename` column is added automatically as a virtual column and this option is only kept for compatibility reasons. | `BOOL` | `false` |", "position": 7, "token_count": 206, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "data-parquet-overview-007", "document_id": "data-parquet-overview", "position": 7, "token_count": 206, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/parquet/overview.md", "url": "/data/parquet/overview", "title": "Reading and Writing Parquet Files", "category": null, "tags": [], "section_url": "/data/parquet/overview"}}
{"chunk_id": "data-parquet-overview-008", "document_id": "data-parquet-overview", "content": "| `file_row_number` | Whether or not to include the `file_row_number` column. | `BOOL` | `false` |\n| `hive_partitioning` | Whether or not to interpret the path as a [Hive partitioned path]({% link docs/stable/data/partitioning/hive_partitioning.md %}). | `BOOL` | (auto-detected) |\n| `union_by_name` | Whether the columns of multiple schemas should be [unified by name]({% link docs/stable/data/multiple_files/combining_schemas.md %}), rather than by position. | `BOOL` | `false` |", "position": 8, "token_count": 160, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "data-parquet-overview-008", "document_id": "data-parquet-overview", "position": 8, "token_count": 160, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/parquet/overview.md", "url": "/data/parquet/overview", "title": "Reading and Writing Parquet Files", "category": null, "tags": [], "section_url": "/data/parquet/overview"}}
{"chunk_id": "data-parquet-overview-009", "document_id": "data-parquet-overview", "content": "## Partial Reading\n\nDuckDB supports projection pushdown into the Parquet file itself. That is to say, when querying a Parquet file, only the columns required for the query are read. This allows you to read only the part of the Parquet file that you are interested in. This will be done automatically by DuckDB.\n\nDuckDB also supports filter pushdown into the Parquet reader. When you apply a filter to a column that is scanned from a Parquet file, the filter will be pushed down into the scan, and can even be used to skip parts of the file using the built-in zonemaps. Note that this will depend on whether or not your Parquet file contains zonemaps.\n\nFilter and projection pushdown provide significant performance benefits. See [our blog post “Querying Parquet with Precision Using DuckDB”]({% post_url 2021-06-25-querying-parquet %}) for more information.", "position": 9, "token_count": 200, "has_code": false, "section_hierarchy": ["Partial Reading"], "metadata": {"chunk_id": "data-parquet-overview-009", "document_id": "data-parquet-overview", "position": 9, "token_count": 200, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Partial Reading"], "file_path": "data/parquet/overview.md", "url": "/data/parquet/overview", "title": "Reading and Writing Parquet Files", "category": null, "tags": [], "section_url": "/data/parquet/overview#partial-reading"}}
{"chunk_id": "data-parquet-overview-010", "document_id": "data-parquet-overview", "content": "## Inserts and Views\n\nYou can also insert the data into a table or create a table from the Parquet file directly. This will load the data from the Parquet file and insert it into the database:\n\nInsert the data from the Parquet file in the table:\n\n```sql\nINSERT INTO people\n SELECT * FROM read_parquet('test.parquet');\n```\n\nCreate a table directly from a Parquet file:\n\n```sql\nCREATE TABLE people AS\n SELECT * FROM read_parquet('test.parquet');\n```\n\nIf you wish to keep the data stored inside the Parquet file, but want to query the Parquet file directly, you can create a view over the `read_parquet` function. You can then query the Parquet file as if it were a built-in table:\n\nCreate a view over the Parquet file:\n\n```sql\nCREATE VIEW people AS\n SELECT * FROM read_parquet('test.parquet');\n```\n\nQuery the Parquet file:\n\n```sql\nSELECT * FROM people;\n```", "position": 10, "token_count": 231, "has_code": true, "section_hierarchy": ["Inserts and Views"], "metadata": {"chunk_id": "data-parquet-overview-010", "document_id": "data-parquet-overview", "position": 10, "token_count": 231, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Inserts and Views"], "file_path": "data/parquet/overview.md", "url": "/data/parquet/overview", "title": "Reading and Writing Parquet Files", "category": null, "tags": [], "section_url": "/data/parquet/overview#inserts-and-views"}}
{"chunk_id": "data-parquet-overview-011", "document_id": "data-parquet-overview", "content": "## Writing to Parquet Files\n\nDuckDB also has support for writing to Parquet files using the `COPY` statement syntax. See the [`COPY` Statement page]({% link docs/stable/sql/statements/copy.md %}) for details, including all possible parameters for the `COPY` statement.\n\nWrite a query to a Snappy-compressed Parquet file:\n\n```sql\nCOPY\n (SELECT * FROM tbl)\n TO 'result-snappy.parquet'\n (FORMAT parquet);\n```\n\nWrite `tbl` to a zstd-compressed Parquet file:\n\n```sql\nCOPY tbl\n TO 'result-zstd.parquet'\n (FORMAT parquet, COMPRESSION zstd);\n```\n\nWrite `tbl` to a zstd-compressed Parquet file with the lowest compression level yielding the fastest compression:\n\n```sql\nCOPY tbl\n TO 'result-zstd.parquet'\n (FORMAT parquet, COMPRESSION zstd, COMPRESSION_LEVEL 1);", "position": 11, "token_count": 221, "has_code": true, "section_hierarchy": ["Writing to Parquet Files"], "metadata": {"chunk_id": "data-parquet-overview-011", "document_id": "data-parquet-overview", "position": 11, "token_count": 221, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Writing to Parquet Files"], "file_path": "data/parquet/overview.md", "url": "/data/parquet/overview", "title": "Reading and Writing Parquet Files", "category": null, "tags": [], "section_url": "/data/parquet/overview#writing-to-parquet-files"}}
{"chunk_id": "data-parquet-overview-012", "document_id": "data-parquet-overview", "content": "```\n\nWrite to Parquet file with [key-value metadata]({% link docs/stable/data/parquet/metadata.md %}):\n\n```sql\nCOPY (\n SELECT\n 42 AS number,\n true AS is_even\n) TO 'kv_metadata.parquet' (\n FORMAT parquet,\n KV_METADATA {\n number: 'Answer to life, universe, and everything',\n is_even: 'not ''odd''' -- single quotes in values must be escaped\n }\n);\n```\n\nWrite a CSV file to an uncompressed Parquet file:\n\n```sql\nCOPY\n 'test.csv'\n TO 'result-uncompressed.parquet'\n (FORMAT parquet, COMPRESSION uncompressed);\n```\n\nWrite a query to a Parquet file with zstd-compression and row group size:\n\n```sql\nCOPY\n (FROM generate_series(100_000))\n TO 'row-groups-zstd.parquet'\n (FORMAT parquet, COMPRESSION zstd, ROW_GROUP_SIZE 100_000);", "position": 12, "token_count": 231, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "data-parquet-overview-012", "document_id": "data-parquet-overview", "position": 12, "token_count": 231, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/parquet/overview.md", "url": "/data/parquet/overview", "title": "Reading and Writing Parquet Files", "category": null, "tags": [], "section_url": "/data/parquet/overview"}}
{"chunk_id": "data-parquet-overview-013", "document_id": "data-parquet-overview", "content": "```\n\nWrite data to an LZ4-compressed Parquet file:\n\n```sql\nCOPY\n (FROM generate_series(100_000))\n TO 'result-lz4.parquet'\n (FORMAT parquet, COMPRESSION lz4);\n```\n\nOr, equivalently:\n\n```sql\nCOPY\n (FROM generate_series(100_000))\n TO 'result-lz4.parquet'\n (FORMAT parquet, COMPRESSION lz4_raw);\n```\n\nWrite data to a Brotli-compressed Parquet file:\n\n```sql\nCOPY\n (FROM generate_series(100_000))\n TO 'result-brotli.parquet'\n (FORMAT parquet, COMPRESSION brotli);\n```\n\nTo configure the page size of Parquet file's dictionary pages, use the `STRING_DICTIONARY_PAGE_SIZE_LIMIT` option (default: 1 MB):\n\n```sql\nCOPY\n lineitem\n TO 'lineitem-with-custom-dictionary-size.parquet'\n (FORMAT parquet, STRING_DICTIONARY_PAGE_SIZE_LIMIT 100_000);", "position": 13, "token_count": 243, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "data-parquet-overview-013", "document_id": "data-parquet-overview", "position": 13, "token_count": 243, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/parquet/overview.md", "url": "/data/parquet/overview", "title": "Reading and Writing Parquet Files", "category": null, "tags": [], "section_url": "/data/parquet/overview"}}
{"chunk_id": "data-parquet-overview-014", "document_id": "data-parquet-overview", "content": "```\n\nDuckDB's `EXPORT` command can be used to export an entire database to a series of Parquet files. See the [“`EXPORT` statement” page]({% link docs/stable/sql/statements/export.md %}) for more details:\n\nExport the table contents of the entire database as Parquet:\n\n```sql\nEXPORT DATABASE 'target_directory' (FORMAT parquet);\n```", "position": 14, "token_count": 95, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "data-parquet-overview-014", "document_id": "data-parquet-overview", "position": 14, "token_count": 95, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/parquet/overview.md", "url": "/data/parquet/overview", "title": "Reading and Writing Parquet Files", "category": null, "tags": [], "section_url": "/data/parquet/overview"}}
{"chunk_id": "data-parquet-overview-015", "document_id": "data-parquet-overview", "content": "## Encryption\n\nDuckDB supports reading and writing [encrypted Parquet files]({% link docs/stable/data/parquet/encryption.md %}).\n\n## Supported Features\n\nThe list of supported Parquet features is available in the [Parquet documentation's “Implementation status” page](https://parquet.apache.org/docs/file-format/implementationstatus/).\n\n## Installing and Loading the Parquet Extension\n\nThe support for Parquet files is enabled via extension. The `parquet` extension is bundled with almost all clients. However, if your client does not bundle the `parquet` extension, the extension must be installed separately:\n\n```sql\nINSTALL parquet;\n```", "position": 15, "token_count": 159, "has_code": true, "section_hierarchy": ["Installing and Loading the Parquet Extension"], "metadata": {"chunk_id": "data-parquet-overview-015", "document_id": "data-parquet-overview", "position": 15, "token_count": 159, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Installing and Loading the Parquet Extension"], "file_path": "data/parquet/overview.md", "url": "/data/parquet/overview", "title": "Reading and Writing Parquet Files", "category": null, "tags": [], "section_url": "/data/parquet/overview#installing-and-loading-the-parquet-extension"}}
