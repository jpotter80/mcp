{"chunk_id": "data-partitioning-hive_partitioning-000", "document_id": "data-partitioning-hive_partitioning", "content": "## Examples\n\nRead data from a Hive partitioned dataset:\n\n```sql\nSELECT *\nFROM read_parquet('orders/*/*/*.parquet', hive_partitioning = true);\n```\n\nWrite a table to a Hive partitioned dataset:\n\n```sql\nCOPY orders\nTO 'orders' (FORMAT parquet, PARTITION_BY (year, month));\n```\n\nNote that the `PARTITION_BY` options cannot use expressions. You can produce columns on the fly using the following syntax:\n\n```sql\nCOPY (SELECT *, year(timestamp) AS year, month(timestamp) AS month FROM services)\nTO 'test' (PARTITION_BY (year, month));", "position": 0, "token_count": 159, "has_code": true, "section_hierarchy": ["Examples"], "metadata": {"chunk_id": "data-partitioning-hive_partitioning-000", "document_id": "data-partitioning-hive_partitioning", "position": 0, "token_count": 159, "has_code": true, "overlap_with_previous": false, "section_hierarchy": ["Examples"], "file_path": "data/partitioning/hive_partitioning.md", "url": "/data/partitioning/hive_partitioning", "title": "Hive Partitioning", "category": null, "tags": [], "section_url": "/data/partitioning/hive_partitioning#examples"}}
{"chunk_id": "data-partitioning-hive_partitioning-001", "document_id": "data-partitioning-hive_partitioning", "content": "```\n\nWhen reading, the partition columns are read from the directory structure and\ncan be included or excluded depending on the `hive_partitioning` parameter.\n\n```sql\nFROM read_parquet('test/*/*/*.parquet', hive_partitioning = false); -- will not include year, month columns\nFROM read_parquet('test/*/*/*.parquet', hive_partitioning = true); -- will include year, month partition columns\n```", "position": 1, "token_count": 113, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "data-partitioning-hive_partitioning-001", "document_id": "data-partitioning-hive_partitioning", "position": 1, "token_count": 113, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/partitioning/hive_partitioning.md", "url": "/data/partitioning/hive_partitioning", "title": "Hive Partitioning", "category": null, "tags": [], "section_url": "/data/partitioning/hive_partitioning"}}
{"chunk_id": "data-partitioning-hive_partitioning-002", "document_id": "data-partitioning-hive_partitioning", "content": "## Hive Partitioning\n\nHive partitioning is a [partitioning strategy](https://en.wikipedia.org/wiki/Partition_(database)) that is used to split a table into multiple files based on **partition keys**. The files are organized into folders. Within each folder, the **partition key** has a value that is determined by the name of the folder.\n\nBelow is an example of a Hive partitioned file hierarchy. The files are partitioned on two keys (`year` and `month`).\n\n```text\norders\n├── year=2021\n│ ├── month=1\n│ │ ├── file1.parquet\n│ │ └── file2.parquet\n│ └── month=2\n│ └── file3.parquet\n└── year=2022\n ├── month=11\n │ ├── file4.parquet\n │ └── file5.parquet\n └── month=12\n └── file6.parquet\n```\n\nFiles stored in this hierarchy can be read using the `hive_partitioning` flag.\n\n```sql\nSELECT *\nFROM read_parquet('orders/*/*/*.parquet', hive_partitioning = true);", "position": 2, "token_count": 247, "has_code": true, "section_hierarchy": ["Hive Partitioning"], "metadata": {"chunk_id": "data-partitioning-hive_partitioning-002", "document_id": "data-partitioning-hive_partitioning", "position": 2, "token_count": 247, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Hive Partitioning"], "file_path": "data/partitioning/hive_partitioning.md", "url": "/data/partitioning/hive_partitioning", "title": "Hive Partitioning", "category": null, "tags": [], "section_url": "/data/partitioning/hive_partitioning#hive-partitioning"}}
{"chunk_id": "data-partitioning-hive_partitioning-003", "document_id": "data-partitioning-hive_partitioning", "content": "```\n\nWhen we specify the `hive_partitioning` flag, the values of the columns will be read from the directories.", "position": 3, "token_count": 30, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "data-partitioning-hive_partitioning-003", "document_id": "data-partitioning-hive_partitioning", "position": 3, "token_count": 30, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "data/partitioning/hive_partitioning.md", "url": "/data/partitioning/hive_partitioning", "title": "Hive Partitioning", "category": null, "tags": [], "section_url": "/data/partitioning/hive_partitioning"}}
{"chunk_id": "data-partitioning-hive_partitioning-004", "document_id": "data-partitioning-hive_partitioning", "content": "### Filter Pushdown\n\nFilters on the partition keys are automatically pushed down into the files. This way the system skips reading files that are not necessary to answer a query. For example, consider the following query on the above dataset:\n\n```sql\nSELECT *\nFROM read_parquet('orders/*/*/*.parquet', hive_partitioning = true)\nWHERE year = 2022\n AND month = 11;\n```\n\nWhen executing this query, only the following files will be read:\n\n```text\norders\n└── year=2022\n └── month=11\n ├── file4.parquet\n └── file5.parquet\n```\n\n### Auto-detection\n\nBy default the system tries to infer if the provided files are in a hive partitioned hierarchy. And if so, the `hive_partitioning` flag is enabled automatically. The auto-detection will look at the names of the folders and search for a `'key' = 'value'` pattern. This behavior can be overridden by using the `hive_partitioning` configuration option:\n\n```sql\nSET hive_partitioning = false;\n```", "position": 4, "token_count": 242, "has_code": true, "section_hierarchy": ["Auto-detection"], "metadata": {"chunk_id": "data-partitioning-hive_partitioning-004", "document_id": "data-partitioning-hive_partitioning", "position": 4, "token_count": 242, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Auto-detection"], "file_path": "data/partitioning/hive_partitioning.md", "url": "/data/partitioning/hive_partitioning", "title": "Hive Partitioning", "category": null, "tags": [], "section_url": "/data/partitioning/hive_partitioning#auto-detection"}}
{"chunk_id": "data-partitioning-hive_partitioning-005", "document_id": "data-partitioning-hive_partitioning", "content": "### Hive Types\n\n`hive_types` is a way to specify the logical types of the hive partitions in a struct:\n\n```sql\nSELECT *\nFROM read_parquet(\n 'dir/**/*.parquet',\n hive_partitioning = true,\n hive_types = {'release': DATE, 'orders': BIGINT}\n);\n```\n\n`hive_types` will be auto-detected for the following types: `DATE`, `TIMESTAMP` and `BIGINT`. To switch off the auto-detection, the flag `hive_types_autocast = 0` can be set.\n\n### Writing Partitioned Files\n\nSee the [Partitioned Writes]({% link docs/stable/data/partitioning/partitioned_writes.md %}) section.", "position": 5, "token_count": 178, "has_code": true, "section_hierarchy": ["Writing Partitioned Files"], "metadata": {"chunk_id": "data-partitioning-hive_partitioning-005", "document_id": "data-partitioning-hive_partitioning", "position": 5, "token_count": 178, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Writing Partitioned Files"], "file_path": "data/partitioning/hive_partitioning.md", "url": "/data/partitioning/hive_partitioning", "title": "Hive Partitioning", "category": null, "tags": [], "section_url": "/data/partitioning/hive_partitioning#writing-partitioned-files"}}
