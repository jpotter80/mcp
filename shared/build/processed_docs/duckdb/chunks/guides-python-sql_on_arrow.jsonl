{"chunk_id": "guides-python-sql_on_arrow-000", "document_id": "guides-python-sql_on_arrow", "content": "DuckDB can query multiple different types of Apache Arrow objects.\n\n## Apache Arrow Tables\n\n[Arrow Tables](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html) stored in local variables can be queried as if they are regular tables within DuckDB.\n\n```python\nimport duckdb\nimport pyarrow as pa\n\n# connect to an in-memory database\ncon = duckdb.connect()\n\nmy_arrow_table = pa.Table.from_pydict({'i': [1, 2, 3, 4],\n 'j': [\"one\", \"two\", \"three\", \"four\"]})", "position": 0, "token_count": 151, "has_code": true, "section_hierarchy": ["connect to an in-memory database"], "metadata": {"chunk_id": "guides-python-sql_on_arrow-000", "document_id": "guides-python-sql_on_arrow", "position": 0, "token_count": 151, "has_code": true, "overlap_with_previous": false, "section_hierarchy": ["connect to an in-memory database"], "file_path": "guides/python/sql_on_arrow.md", "url": "/guides/python/sql_on_arrow", "title": "SQL on Apache Arrow", "category": null, "tags": [], "section_url": "/guides/python/sql_on_arrow#connect-to-an-in-memory-database"}}
{"chunk_id": "guides-python-sql_on_arrow-001", "document_id": "guides-python-sql_on_arrow", "content": "# query the Apache Arrow Table \"my_arrow_table\" and return as an Arrow Table\nresults = con.execute(\"SELECT * FROM my_arrow_table WHERE i = 2\").fetch_arrow_table()\n```\n\n## Apache Arrow Datasets\n\n[Arrow Datasets](https://arrow.apache.org/docs/python/dataset.html) stored as variables can also be queried as if they were regular tables.\nDatasets are useful to point towards directories of Parquet files to analyze large datasets.\nDuckDB will push column selections and row filters down into the dataset scan operation so that only the necessary data is pulled into memory.\n\n```python\nimport duckdb\nimport pyarrow as pa\nimport tempfile\nimport pathlib\nimport pyarrow.parquet as pq\nimport pyarrow.dataset as ds", "position": 1, "token_count": 190, "has_code": true, "section_hierarchy": ["Apache Arrow Datasets"], "metadata": {"chunk_id": "guides-python-sql_on_arrow-001", "document_id": "guides-python-sql_on_arrow", "position": 1, "token_count": 190, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Apache Arrow Datasets"], "file_path": "guides/python/sql_on_arrow.md", "url": "/guides/python/sql_on_arrow", "title": "SQL on Apache Arrow", "category": null, "tags": [], "section_url": "/guides/python/sql_on_arrow#apache-arrow-datasets"}}
{"chunk_id": "guides-python-sql_on_arrow-002", "document_id": "guides-python-sql_on_arrow", "content": "# connect to an in-memory database\ncon = duckdb.connect()\n\nmy_arrow_table = pa.Table.from_pydict({'i': [1, 2, 3, 4],\n 'j': [\"one\", \"two\", \"three\", \"four\"]})\n\n# create example Parquet files and save in a folder\nbase_path = pathlib.Path(tempfile.gettempdir())\n(base_path / \"parquet_folder\").mkdir(exist_ok = True)\npq.write_to_dataset(my_arrow_table, str(base_path / \"parquet_folder\"))\n\n# link to Parquet files using an Arrow Dataset\nmy_arrow_dataset = ds.dataset(str(base_path / 'parquet_folder/'))", "position": 2, "token_count": 198, "has_code": false, "section_hierarchy": ["link to Parquet files using an Arrow Dataset"], "metadata": {"chunk_id": "guides-python-sql_on_arrow-002", "document_id": "guides-python-sql_on_arrow", "position": 2, "token_count": 198, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["link to Parquet files using an Arrow Dataset"], "file_path": "guides/python/sql_on_arrow.md", "url": "/guides/python/sql_on_arrow", "title": "SQL on Apache Arrow", "category": null, "tags": [], "section_url": "/guides/python/sql_on_arrow#link-to-parquet-files-using-an-arrow-dataset"}}
{"chunk_id": "guides-python-sql_on_arrow-003", "document_id": "guides-python-sql_on_arrow", "content": "# query the Apache Arrow Dataset \"my_arrow_dataset\" and return as an Arrow Table\nresults = con.execute(\"SELECT * FROM my_arrow_dataset WHERE i = 2\").fetch_arrow_table()\n```\n\n## Apache Arrow Scanners\n\n[Arrow Scanners](https://arrow.apache.org/docs/python/generated/pyarrow.dataset.Scanner.html) stored as variables can also be queried as if they were regular tables. Scanners read over a dataset and select specific columns or apply row-wise filtering. This is similar to how DuckDB pushes column selections and filters down into an Arrow Dataset, but using Arrow compute operations instead. Arrow can use asynchronous IO to quickly access files.\n\n```python\nimport duckdb\nimport pyarrow as pa\nimport tempfile\nimport pathlib\nimport pyarrow.parquet as pq\nimport pyarrow.dataset as ds\nimport pyarrow.compute as pc", "position": 3, "token_count": 217, "has_code": true, "section_hierarchy": ["Apache Arrow Scanners"], "metadata": {"chunk_id": "guides-python-sql_on_arrow-003", "document_id": "guides-python-sql_on_arrow", "position": 3, "token_count": 217, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Apache Arrow Scanners"], "file_path": "guides/python/sql_on_arrow.md", "url": "/guides/python/sql_on_arrow", "title": "SQL on Apache Arrow", "category": null, "tags": [], "section_url": "/guides/python/sql_on_arrow#apache-arrow-scanners"}}
{"chunk_id": "guides-python-sql_on_arrow-004", "document_id": "guides-python-sql_on_arrow", "content": "# connect to an in-memory database\ncon = duckdb.connect()\n\nmy_arrow_table = pa.Table.from_pydict({'i': [1, 2, 3, 4],\n 'j': [\"one\", \"two\", \"three\", \"four\"]})\n\n# create example Parquet files and save in a folder\nbase_path = pathlib.Path(tempfile.gettempdir())\n(base_path / \"parquet_folder\").mkdir(exist_ok = True)\npq.write_to_dataset(my_arrow_table, str(base_path / \"parquet_folder\"))\n\n# link to Parquet files using an Arrow Dataset\nmy_arrow_dataset = ds.dataset(str(base_path / 'parquet_folder/'))\n\n# define the filter to be applied while scanning", "position": 4, "token_count": 207, "has_code": false, "section_hierarchy": ["define the filter to be applied while scanning"], "metadata": {"chunk_id": "guides-python-sql_on_arrow-004", "document_id": "guides-python-sql_on_arrow", "position": 4, "token_count": 207, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["define the filter to be applied while scanning"], "file_path": "guides/python/sql_on_arrow.md", "url": "/guides/python/sql_on_arrow", "title": "SQL on Apache Arrow", "category": null, "tags": [], "section_url": "/guides/python/sql_on_arrow#define-the-filter-to-be-applied-while-scanning"}}
{"chunk_id": "guides-python-sql_on_arrow-005", "document_id": "guides-python-sql_on_arrow", "content": "# define the filter to be applied while scanning\n# equivalent to \"WHERE i = 2\"\nscanner_filter = (pc.field(\"i\") == pc.scalar(2))\n\narrow_scanner = ds.Scanner.from_dataset(my_arrow_dataset, filter = scanner_filter)\n\n# query the Apache Arrow scanner \"arrow_scanner\" and return as an Arrow Table\nresults = con.execute(\"SELECT * FROM arrow_scanner\").fetch_arrow_table()\n```\n\n## Apache Arrow RecordBatchReaders\n\n[Arrow RecordBatchReaders](https://arrow.apache.org/docs/python/generated/pyarrow.RecordBatchReader.html) are a reader for Arrow's streaming binary format and can also be queried directly as if they were tables. This streaming format is useful when sending Arrow data for tasks like interprocess communication or communicating between language runtimes.\n\n```python\nimport duckdb\nimport pyarrow as pa", "position": 5, "token_count": 220, "has_code": true, "section_hierarchy": ["Apache Arrow RecordBatchReaders"], "metadata": {"chunk_id": "guides-python-sql_on_arrow-005", "document_id": "guides-python-sql_on_arrow", "position": 5, "token_count": 220, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Apache Arrow RecordBatchReaders"], "file_path": "guides/python/sql_on_arrow.md", "url": "/guides/python/sql_on_arrow", "title": "SQL on Apache Arrow", "category": null, "tags": [], "section_url": "/guides/python/sql_on_arrow#apache-arrow-recordbatchreaders"}}
{"chunk_id": "guides-python-sql_on_arrow-006", "document_id": "guides-python-sql_on_arrow", "content": "# connect to an in-memory database\ncon = duckdb.connect()\n\nmy_recordbatch = pa.RecordBatch.from_pydict({'i': [1, 2, 3, 4],\n 'j': [\"one\", \"two\", \"three\", \"four\"]})\n\nmy_recordbatchreader = pa.ipc.RecordBatchReader.from_batches(my_recordbatch.schema, [my_recordbatch])\n\n# query the Apache Arrow RecordBatchReader \"my_recordbatchreader\" and return as an Arrow Table\nresults = con.execute(\"SELECT * FROM my_recordbatchreader WHERE i = 2\").fetch_arrow_table()\n```", "position": 6, "token_count": 175, "has_code": true, "section_hierarchy": ["query the Apache Arrow RecordBatchReader \"my_recordbatchreader\" and return as an Arrow Table"], "metadata": {"chunk_id": "guides-python-sql_on_arrow-006", "document_id": "guides-python-sql_on_arrow", "position": 6, "token_count": 175, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["query the Apache Arrow RecordBatchReader \"my_recordbatchreader\" and return as an Arrow Table"], "file_path": "guides/python/sql_on_arrow.md", "url": "/guides/python/sql_on_arrow", "title": "SQL on Apache Arrow", "category": null, "tags": [], "section_url": "/guides/python/sql_on_arrow#query-the-apache-arrow-recordbatchreader-myrecordbatchreader-and-return-as-an-arrow-table"}}
