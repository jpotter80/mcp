{"chunk_id": "guides-performance-working_with_huge_databases-000", "document_id": "guides-performance-working_with_huge_databases", "content": "This page contains information for working with huge DuckDB database files.\nWhile most DuckDB databases are well below 1 TB,\nin our [2024 user survey]({% post_url 2024-10-04-duckdb-user-survey-analysis %}#dataset-sizes), 1% of respondents used DuckDB files of 2 TB or more (corresponding to roughly 10 TB of CSV files).\n\nDuckDB's [native database format]({% link docs/stable/internals/storage.md %}) supports huge database files without any practical restrictions, however, there are a few things to keep in mind when working with huge database files.\n\n1. Object storage systems have lower limits on file sizes than block-based storage systems. For example, [AWS S3 limits the file size to 5 TB](https://aws.amazon.com/s3/faqs/).", "position": 0, "token_count": 199, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "guides-performance-working_with_huge_databases-000", "document_id": "guides-performance-working_with_huge_databases", "position": 0, "token_count": 199, "has_code": false, "overlap_with_previous": false, "section_hierarchy": [], "file_path": "guides/performance/working_with_huge_databases.md", "url": "/guides/performance/working_with_huge_databases", "title": "Working with Huge Databases", "category": null, "tags": [], "section_url": "/guides/performance/working_with_huge_databases"}}
{"chunk_id": "guides-performance-working_with_huge_databases-001", "document_id": "guides-performance-working_with_huge_databases", "content": "2. Checkpointing a DuckDB database can be slow. For example, checkpointing after adding a few rows to a table in the [TPC-H]({% link docs/stable/core_extensions/tpch.md %}) SF1000 database takes approximately 5 seconds.\n\n3. On block-based storage, the file has a big effect on performance when working with large files. On Linux, DuckDB performs best with XFS on large files.\n\nFor storing large amounts of data, consider using the [DuckLake lakehouse format](https://ducklake.select/).", "position": 1, "token_count": 130, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "guides-performance-working_with_huge_databases-001", "document_id": "guides-performance-working_with_huge_databases", "position": 1, "token_count": 130, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "guides/performance/working_with_huge_databases.md", "url": "/guides/performance/working_with_huge_databases", "title": "Working with Huge Databases", "category": null, "tags": [], "section_url": "/guides/performance/working_with_huge_databases"}}
