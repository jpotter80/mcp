{"chunk_id": "gpu-basics-000", "document_id": "gpu-basics", "content": "If you have any questions or feedback for this content, please post it in the\n[Modular forum thread\nhere](https://forum.modular.com/t/gpu-programming-manual/755).\n\nThis documentation aims to build your GPU programming knowledge from the ground\nup, starting with the lowest levels of the stack before progressing to\nhigher-level functionality. It's designed for a diverse audience, from\nexperienced GPU developers to programmers new to GPU coding. Mojo allows you to\nprogram NVIDIA GPUs, with direct access to low-level GPU primitives, while\nsharing types and functions that can also run on CPUs where applicable. If\nyou're experienced with [NVIDIA Compute Unified Device\nArchitecture](https://developer.nvidia.com/cuda-toolkit) (CUDA), what you'll\nlearn here will enable you to expand your reach as we release support for more\nhardware.", "position": 0, "token_count": 199, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "gpu-basics-000", "document_id": "gpu-basics", "position": 0, "token_count": 199, "has_code": false, "overlap_with_previous": false, "section_hierarchy": [], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics"}}
{"chunk_id": "gpu-basics-001", "document_id": "gpu-basics", "content": "## Introduction to massively parallel programming\n\nWe can no longer rely on new generations of CPUs to increase application\nperformance through improved clock speeds. Power demands and heat dissipation\nlimits have stalled that trend, pushing the hardware industry toward increasing\nthe number of physical cores. Modern consumer CPUs now boast 16 cores or more,\ncapable of running in parallel, which forces programmers to rethink how they\nmaximize performance. This shift is especially evident in AI applications, where\nperformance scales remarkably well with additional cores.\n\nNVIDIA's breakthrough came with CUDA, a general programming model that allows\ndevelopers to target both server and consumer GPUs for any application domain.\nThis vision sparked an AI revolution when Alex Krizhevsky, Ilya Sutskever, and\nGeoffrey Hinton trained AlexNet on consumer GPUs, significantly outperforming\ntraditional computer vision methods. GPUs pack thousands of cores, the NVIDIA\nH100 can run 16,896 threads in parallel in a single clock cycle, with over\n270,000 threads queued and ready to go. They're also engineered in a way where\nthe cost of scheduling threads is much lower compared to a traditional CPU.", "position": 1, "token_count": 240, "has_code": false, "section_hierarchy": ["Introduction to massively parallel programming"], "metadata": {"chunk_id": "gpu-basics-001", "document_id": "gpu-basics", "position": 1, "token_count": 240, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Introduction to massively parallel programming"], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics#introduction-to-massively-parallel-programming"}}
{"chunk_id": "gpu-basics-002", "document_id": "gpu-basics", "content": "Harnessing this hardware requires a new programming mindset. Mojo represents a\nchance to rethink GPU programming and make it more approachable. C/C++ is at the\ncore of GPU programming, but we've seen leaps in ergonomics and memory safety\nfrom systems programming languages in recent years. Mojo expands on Python's\nfamiliar syntax, adds direct access to low-level CPU and GPU intrinsics for\nsystems programming, and introduces ergonomic and safety improvements from\nmodern languages. This course aims to empower programmers with minimal\nspecialized knowledge to build high-performance, GPU-enabled applications. By\nlowering the barrier to entry, we aim to fuel more breakthroughs and accelerate\ninnovation.", "position": 2, "token_count": 145, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "gpu-basics-002", "document_id": "gpu-basics", "position": 2, "token_count": 145, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics"}}
{"chunk_id": "gpu-basics-003", "document_id": "gpu-basics", "content": "## Setup\n\nSystem requirements:\n\n[note]\nThese examples can run on many consumer NVIDIA GeForce GPUs, though they aren't\nofficially supported yet. Make sure you have the latest NVIDIA drivers.\n\nAll of these notebook cells are runnable through a VS Code extension. You can\ninstall\n[Markdown Lab](https://marketplace.visualstudio.com/items?itemName=jackos.mdlab),\nthen clone the repo that contains the markdown that generated this website:\n\n```sh\ngit clone git@github.com:modular/modular\ncd modular/mojo/docs/manual/gpu\n```\n\nAnd open `basics.mdx` to run the code cells interactively. If you don't have\n`mojo` on your system PATH it will automatically download and install it the\nfirst time you run a Mojo cell.\n\nIf you prefer the traditional approach using a CLI, first install\n[`pixi`](https://pixi.sh/latest/) if you don't have it:\n\n```sh\ncurl -fsSL https://pixi.sh/install.sh | sh", "position": 3, "token_count": 250, "has_code": true, "section_hierarchy": ["Setup"], "metadata": {"chunk_id": "gpu-basics-003", "document_id": "gpu-basics", "position": 3, "token_count": 250, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Setup"], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics#setup"}}
{"chunk_id": "gpu-basics-004", "document_id": "gpu-basics", "content": "```\n\nThen restart your terminal, create a project, install the `mojo` package, and\nenter the virtual environment:\n\n```sh\npixi init gpu-basics \\\n -c https://conda.modular.com/max-nightly/ -c conda-forge\n\ncd gpu-basics\n\npixi add mojo\n\npixi shell # enter virtual environment\n```\n\nYou can now create file such as `main.mojo` and put everything except the\nimports into a `def main`:\n\n```mojo :once\nfrom gpu import thread_idx\nfrom gpu.host import DeviceContext\n\ndef main():\n fn printing_kernel():\n print(\"GPU thread: [\", thread_idx.x, thread_idx.y, thread_idx.z, \"]\")\n\n var ctx = DeviceContext()\n\n ctx.enqueue_function[printing_kernel](grid_dim=1, block_dim=4)\n ctx.synchronize()", "position": 4, "token_count": 222, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "gpu-basics-004", "document_id": "gpu-basics", "position": 4, "token_count": 222, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics"}}
{"chunk_id": "gpu-basics-005", "document_id": "gpu-basics", "content": "```\n\n```text\nGPU thread: [ 0 0 0 ]\nGPU thread: [ 1 0 0 ]\nGPU thread: [ 2 0 0 ]\nGPU thread: [ 3 0 0 ]\n```\n\nThen compile and run the file using `mojo main.mojo`.\n\nWhen you're ready to exit the virtual environment run the command: `exit`.", "position": 5, "token_count": 81, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "gpu-basics-005", "document_id": "gpu-basics", "position": 5, "token_count": 81, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics"}}
{"chunk_id": "gpu-basics-006", "document_id": "gpu-basics", "content": "## Imports\n\nThese are all the imports required to run the examples, put this at the top of\nyour file if you're running from `mojo main.mojo`:\n\n```mojo\nfrom gpu import thread_idx, block_idx, warp, barrier\nfrom gpu.host import DeviceContext, DeviceBuffer, HostBuffer\nfrom gpu.memory import AddressSpace\nfrom memory import stack_allocation\nfrom layout import Layout, LayoutTensor\nfrom math import iota\nfrom sys import size_of\n```", "position": 6, "token_count": 112, "has_code": true, "section_hierarchy": ["Imports"], "metadata": {"chunk_id": "gpu-basics-006", "document_id": "gpu-basics", "position": 6, "token_count": 112, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Imports"], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics#imports"}}
{"chunk_id": "gpu-basics-007", "document_id": "gpu-basics", "content": "## Your first kernel\n\nIn the context of GPU programming, a kernel is a program that runs on each\nthread that you launch:\n\n```mojo\nfn printing_kernel():\n print(\"GPU thread: [\", thread_idx.x, thread_idx.y, thread_idx.z, \"]\")\n```\n\n[note]\nWe're using `fn` here without the `raises` keyword because a kernel function is\nnot allowed to raise an error condition. When you define a Mojo function with\n`def`, the compiler always assumes that the function *can* raise an error\ncondition. See [Functions](/mojo/manual/functions) more information.\n\nWe can pass this function as a parameter to `enqueue_function()` to compile it\nfor your attached GPU and launch it. First we need to get the\n[`DeviceContext`](/mojo/stdlib/gpu/host/device_context/DeviceContext) for your\nGPU:\n\n```mojo\nvar ctx = DeviceContext()", "position": 7, "token_count": 235, "has_code": true, "section_hierarchy": ["Your first kernel"], "metadata": {"chunk_id": "gpu-basics-007", "document_id": "gpu-basics", "position": 7, "token_count": 235, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Your first kernel"], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics#your-first-kernel"}}
{"chunk_id": "gpu-basics-008", "document_id": "gpu-basics", "content": "```\n\nNow we have the `DeviceContext` we can compile and launch the kernel:\n\n```mojo :once\nctx.enqueue_function[printing_kernel](grid_dim=1, block_dim=4)", "position": 8, "token_count": 55, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "gpu-basics-008", "document_id": "gpu-basics", "position": 8, "token_count": 55, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics"}}
{"chunk_id": "gpu-basics-009", "document_id": "gpu-basics", "content": "# Wait for the kernel to finish executing before handing back to CPU\nctx.synchronize()\n```\n\n```text\nGPU thread: [ 0 0 0 ]\nGPU thread: [ 1 0 0 ]\nGPU thread: [ 2 0 0 ]\nGPU thread: [ 3 0 0 ]\n```\n\n[note]\nThe term `kernel` in this context originated in the 1980s with the introduction\nof the [Single Program, Multiple\nData](https://en.wikipedia.org/wiki/Single_program,_multiple_data) (SPMD)\nparallel programming technique, which underpins ROCm and CUDA. In this approach,\na kernel executes concurrently across distinct elements of large data\nstructures.", "position": 9, "token_count": 154, "has_code": true, "section_hierarchy": ["Wait for the kernel to finish executing before handing back to CPU"], "metadata": {"chunk_id": "gpu-basics-009", "document_id": "gpu-basics", "position": 9, "token_count": 154, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Wait for the kernel to finish executing before handing back to CPU"], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics#wait-for-the-kernel-to-finish-executing-before-handing-back-to-cpu"}}
{"chunk_id": "gpu-basics-010", "document_id": "gpu-basics", "content": "## Threads\n\nBecause we passed `block_dim=4`, we launched 4 threads on the x dimension, the\nkernel code we wrote is executed on each thread. The printing can be out of\norder depending on which thread reaches that `print()` call first.\n\nNow add the y and z dimensions with `block_dim=(2, 2, 2)`:\n\n[note]\nFor the `grid_dim` and `block_dim` arguments you can use a single value or a\ntuple. A single value will launch N blocks/threads on the x dimension, while\nusing a tuple with up to three values will determine the (x, y, z) dimensions.\n\n```mojo :once\nctx.enqueue_function[printing_kernel](grid_dim=1, block_dim=(2, 2, 2))\nctx.synchronize()", "position": 10, "token_count": 190, "has_code": true, "section_hierarchy": ["Threads"], "metadata": {"chunk_id": "gpu-basics-010", "document_id": "gpu-basics", "position": 10, "token_count": 190, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Threads"], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics#threads"}}
{"chunk_id": "gpu-basics-011", "document_id": "gpu-basics", "content": "```\n\n```text\nGPU thread: [ 0 0 0 ]\nGPU thread: [ 1 0 0 ]\nGPU thread: [ 0 1 0 ]\nGPU thread: [ 1 1 0 ]\nGPU thread: [ 0 0 1 ]\nGPU thread: [ 1 0 1 ]\nGPU thread: [ 0 1 1 ]\nGPU thread: [ 1 1 1 ]\n```\n\nWe're now launching 8 (2x2x2) threads in total.", "position": 11, "token_count": 101, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "gpu-basics-011", "document_id": "gpu-basics", "position": 11, "token_count": 101, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics"}}
{"chunk_id": "gpu-basics-012", "document_id": "gpu-basics", "content": "## Host vs device\n\nYou'll see the word host which refers to the CPU that schedules work for the\ndevice, device refers to the accelerator which in this case is a GPU.\n\nWhen you're running device side code, it means that the host is scheduling the\noperation to execute asynchronously on the device. If your host-side code relies\non the outcome of device-side code, you need to call `ctx.synchronize()`. For\ninstance, printing from the CPU without first synchronizing might result in\nout-of-order output:\n\n```mojo :once\nctx.enqueue_function[printing_kernel](grid_dim=1, block_dim=4)\nprint(\"This might print before the GPU has completed its work\")\n```\n\n```text\nThis might print before the GPU has completed its work\nGPU thread: [ 0 0 0 ]\nGPU thread: [ 1 0 0 ]\nGPU thread: [ 2 0 0 ]\nGPU thread: [ 3 0 0 ]", "position": 12, "token_count": 226, "has_code": true, "section_hierarchy": ["Host vs device"], "metadata": {"chunk_id": "gpu-basics-012", "document_id": "gpu-basics", "position": 12, "token_count": 226, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Host vs device"], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics#host-vs-device"}}
{"chunk_id": "gpu-basics-013", "document_id": "gpu-basics", "content": "```\n\nIn the above example we failed to call `synchronize()` before printing on the\nhost, the device could be slightly slower to finish its work, so you might\nsee that output after the host output. Let's add a `synchronize()` call:\n\n```mojo :once\nctx.enqueue_function[printing_kernel](grid_dim=1, block_dim=4)\nctx.synchronize()\nprint(\"This will print after the GPU has completed its work\")\n```\n\n```text\nGPU thread: [ 0 0 0 ]\nGPU thread: [ 1 0 0 ]\nGPU thread: [ 2 0 0 ]\nGPU thread: [ 3 0 0 ]\nThis will print after the GPU has completed its work", "position": 13, "token_count": 175, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "gpu-basics-013", "document_id": "gpu-basics", "position": 13, "token_count": 175, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics"}}
{"chunk_id": "gpu-basics-014", "document_id": "gpu-basics", "content": "```\n\nAny method you run on a `DeviceContext` that interacts with the device, will run\nin the order that you called them. You only have to synchronize when you're\ndoing something from the host which is dependent on the results of the device.\n\nIn GPU programming with Mojo, when there's a tradeoff between GPU performance\nand safety or ergonomics, performance takes priority, aligning with the\nexpectations of kernel engineers. For instance while we could force\nsynchronization for each call that interacts with the device, this would come at\na performance cost where we want to run multiple functions asynchronously and\nsynchronize once.\n\nwarning Synchronization\n\nFor any methods or functions that interact with the device, you must synchronize\nbefore running CPU code that is dependent on it's execution. Multiple method or\nfunction calls for a single GPU is safe, as they are scheduled to run in the\norder you call them.", "position": 14, "token_count": 205, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "gpu-basics-014", "document_id": "gpu-basics", "position": 14, "token_count": 205, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics"}}
{"chunk_id": "gpu-basics-015", "document_id": "gpu-basics", "content": "Mojo enhances the safety and ergonomics of C++ GPU programming where it doesn't\nsacrifice performance. For example, ASAP destruction automatically frees buffer\nmemory on last use of the object, eliminating memory leaks and ensuring memory\nis released as early as possible. This is an evolution on modern memory\nmanagement solutions such as C++ RAII, which is scope based and may hold onto\nmemory for longer than expected, which is a precious resource in AI\napplications.", "position": 15, "token_count": 97, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "gpu-basics-015", "document_id": "gpu-basics", "position": 15, "token_count": 97, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics"}}
{"chunk_id": "gpu-basics-016", "document_id": "gpu-basics", "content": "## Blocks\n\nThis kernel demonstrates how blocks work:\n\n```mojo :once\nfn block_kernel():\n print(\n \"block: [\",\n block_idx.x,\n block_idx.y,\n block_idx.z,\n \"]\",\n \"thread: [\",\n thread_idx.x,\n thread_idx.y,\n thread_idx.z,\n \"]\"\n )\n\nctx.enqueue_function[block_kernel](grid_dim=(2, 2), block_dim=2)\nctx.synchronize()\n```\n\n```text\nblock: [ 0 0 0 ] thread: [ 0 0 0 ]\nblock: [ 0 0 0 ] thread: [ 1 0 0 ]\nblock: [ 0 1 0 ] thread: [ 0 0 0 ]\nblock: [ 0 1 0 ] thread: [ 1 0 0 ]\nblock: [ 1 1 0 ] thread: [ 0 0 0 ]\nblock: [ 1 1 0 ] thread: [ 1 0 0 ]\nblock: [ 1 0 0 ] thread: [ 0 0 0 ]\nblock: [ 1 0 0 ] thread: [ 1 0 0 ]", "position": 16, "token_count": 248, "has_code": true, "section_hierarchy": ["Blocks"], "metadata": {"chunk_id": "gpu-basics-016", "document_id": "gpu-basics", "position": 16, "token_count": 248, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Blocks"], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics#blocks"}}
{"chunk_id": "gpu-basics-017", "document_id": "gpu-basics", "content": "```\n\nWe're still launching 8 (2x2x2) threads, where there are 4 blocks, each with 2\nthreads. In GPU programming this grouping of blocks and threads is important,\neach block can have its own fast SRAM (Static Random Access Memory) which allows\nthreads to communicate. The threads within a block can also communicate through\nregisters, we'll cover this concept when we get to warps. For now the\nimportant information to internalize is:\n\n- `grid_dim` defines how many blocks are launched.\n- `block_dim` defines how many threads are launched in each block.", "position": 17, "token_count": 128, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "gpu-basics-017", "document_id": "gpu-basics", "position": 17, "token_count": 128, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics"}}
{"chunk_id": "gpu-basics-018", "document_id": "gpu-basics", "content": "## Tiles\n\nThe x, y, z dimensions of blocks are important for splitting up large jobs into\ntiles, so each thread can work on its own subset of the problem. Below is a\nvisualization for how a contiguous array of data can be split up into tiles, if\nwe have an array of UInt32 (Unsigned Integer 32bit) data like:\n\n```plaintext\n[ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ]\n```\n\nWe could split work up between threads and blocks, we're only going to use the x\ndimension for threads and blocks to get started:\n\n```plaintext\nThread | 0 1 2 3\n-------------------------\nblock 0 | [ 0 1 2 3 ]\nblock 1 | [ 4 5 6 7 ]\nblock 2 | [ 8 9 10 11 ]\nblock 3 | [ 12 13 14 15 ]", "position": 18, "token_count": 200, "has_code": true, "section_hierarchy": ["Tiles"], "metadata": {"chunk_id": "gpu-basics-018", "document_id": "gpu-basics", "position": 18, "token_count": 200, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Tiles"], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics#tiles"}}
{"chunk_id": "gpu-basics-019", "document_id": "gpu-basics", "content": "```\n\nIf you had a much larger data array you could further split it up into tiles,\ne.g. tile with widths [2, 2] at index (0, 0) would be:\n\n```plaintext\n[ 0 1 ]\n[ 4 5 ]\n```\n\nAnd index (2, 0) would be:\n\n```plaintext\n[ 2 3 ]\n[ 6 7 ]\n```\n\nThis is where you'd introduce the y dimension, later we'll begin working on\nimage data which is a tensor with 3 dimensions: (height, width, color_channels).\nFor now we're going to focus on how blocks and threads interact, splitting up an\narray into 1 row per block, and 1 value per thread.", "position": 19, "token_count": 156, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "gpu-basics-019", "document_id": "gpu-basics", "position": 19, "token_count": 156, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics"}}
{"chunk_id": "gpu-basics-020", "document_id": "gpu-basics", "content": "## Buffers\n\nFirst we'll initialize a contiguous array on the GPU:\n\n```mojo\nalias dtype = DType.uint32\nalias blocks = 4\nalias threads = 4\nalias elements_in = blocks * threads # one element per thread\n\nvar in_buffer = ctx.enqueue_create_buffer[dtype](elements_in)\n```\n\nCreating the GPU buffer is allocating _global memory_ which can be accessed from\nany block and thread inside a GPU kernel, this memory is relatively slow\ncompared to _shared memory_ which is shared between all of the threads in a\nblock, more on that later.\n\nWe can't access memory in a GPU address space from CPU to initialize the values\nunless we map it to host:\n\n```mojo\nwith in_buffer.map_to_host() as host_buffer:\n iota(host_buffer.unsafe_ptr(), elements_in)\n print(host_buffer)", "position": 20, "token_count": 208, "has_code": true, "section_hierarchy": ["Buffers"], "metadata": {"chunk_id": "gpu-basics-020", "document_id": "gpu-basics", "position": 20, "token_count": 208, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Buffers"], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics#buffers"}}
{"chunk_id": "gpu-basics-021", "document_id": "gpu-basics", "content": "```\n\n```text\nHostBuffer([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])\n```\n\nIf you're loading or storing values from a buffer allocated on GPU, mapping to\nhost ensures the values are copied into the CPU address space when the context\nmanager enters (start of the `with` block), and back to the GPU address space\nwhen the context manager exits (end of the `with` block). Note that\n`map_to_host()` will call `synchronize()` before writing the data back to CPU,\nso you don't have to call it separately.", "position": 21, "token_count": 157, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "gpu-basics-021", "document_id": "gpu-basics", "position": 21, "token_count": 157, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics"}}
{"chunk_id": "gpu-basics-022", "document_id": "gpu-basics", "content": "## Tensor indexing from threads\n\nNow that we have the data set up, we can wrap the data in a\n[LayoutTensor](/mojo/kernels/layout/layout_tensor/LayoutTensor/) so that we can\nreason about how to index into the array, allowing each thread to get its\ncorresponding value:\n\n```mojo :clear\nalias layout = Layout.row_major(blocks, threads)\n\nvar in_tensor = LayoutTensor[dtype, layout](in_buffer)", "position": 22, "token_count": 108, "has_code": true, "section_hierarchy": ["Tensor indexing from threads"], "metadata": {"chunk_id": "gpu-basics-022", "document_id": "gpu-basics", "position": 22, "token_count": 108, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Tensor indexing from threads"], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics#tensor-indexing-from-threads"}}
{"chunk_id": "gpu-basics-023", "document_id": "gpu-basics", "content": "```\n\nnote Memory Layout\n\n\"Row major\" means the values are stored sequentially in memory:\n\n[ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ]\n\n\"Column major\" means memory advances down each column first, then moves to the\nnext column. This layout is used in some GPU tiling kernels because it can align\nwith coalesced column accesses:\n\n[ 0 4 8 12 1 5 9 13 2 6 10 14 3 7 11 15 ]\n\n`LayoutTensor` is a view of the data in buffer, it does not own the underlying\nmemory. It's a powerful abstraction and offers many advanced methods which we'll\ndive into in later chapters.\n\nWe'll create an alias so that we don't have to repeat the type information for\neach kernel launch:\n\n```mojo :clear\nalias InTensor = LayoutTensor[dtype, layout, MutableAnyOrigin]", "position": 23, "token_count": 193, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "gpu-basics-023", "document_id": "gpu-basics", "position": 23, "token_count": 193, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics"}}
{"chunk_id": "gpu-basics-024", "document_id": "gpu-basics", "content": "```\n\nMore information on [origins here](/mojo/manual/values/lifetimes).\n\nInitially we'll just print the values to confirm it's indexing as we expect:\n\n```mojo :once\nfn print_values_kernel(in_tensor: InTensor):\n var bid = block_idx.x\n var tid = thread_idx.x\n print(\"block:\", bid, \"thread:\", tid, \"val:\", in_tensor[bid, tid])\n\nctx.enqueue_function[print_values_kernel](\n in_tensor, grid_dim=blocks, block_dim=threads,\n)\nctx.synchronize()", "position": 24, "token_count": 159, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "gpu-basics-024", "document_id": "gpu-basics", "position": 24, "token_count": 159, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics"}}
{"chunk_id": "gpu-basics-025", "document_id": "gpu-basics", "content": "```\n\n```text\nblock: 0 thread: 0 val: 0\nblock: 0 thread: 1 val: 1\nblock: 0 thread: 2 val: 2\nblock: 0 thread: 3 val: 3\nblock: 1 thread: 0 val: 4\nblock: 1 thread: 1 val: 5\nblock: 1 thread: 2 val: 6\nblock: 1 thread: 3 val: 7\nblock: 3 thread: 0 val: 12\nblock: 3 thread: 1 val: 13\nblock: 3 thread: 2 val: 14\nblock: 3 thread: 3 val: 15\nblock: 2 thread: 0 val: 8\nblock: 2 thread: 1 val: 9\nblock: 2 thread: 2 val: 10\nblock: 2 thread: 3 val: 11\n```\n\nAs in the visualization above, the block/thread is getting the corresponding\nvalue that we expect. You can see `block: 3 thread: 3` has the last value 15.\nTry experimenting with different `grid_dim`, `block_dim` and indexing values\nto see how the behavior changes.", "position": 25, "token_count": 219, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "gpu-basics-025", "document_id": "gpu-basics", "position": 25, "token_count": 219, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics"}}
{"chunk_id": "gpu-basics-026", "document_id": "gpu-basics", "content": "## Multiply kernel\n\nNow that we've verified we're getting the correct values when indexing, we'll\nlaunch a kernel to multiply each value:\n\n```mojo :once\nfn multiply_kernel[multiplier: Int](in_tensor: InTensor):\n in_tensor[block_idx.x, thread_idx.x] *= multiplier\n\nctx.enqueue_function[multiply_kernel[2]](\n in_tensor,\n grid_dim=blocks,\n block_dim=threads,\n)", "position": 26, "token_count": 123, "has_code": true, "section_hierarchy": ["Multiply kernel"], "metadata": {"chunk_id": "gpu-basics-026", "document_id": "gpu-basics", "position": 26, "token_count": 123, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Multiply kernel"], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics#multiply-kernel"}}
{"chunk_id": "gpu-basics-027", "document_id": "gpu-basics", "content": "# Map to host and print as 2D array\nwith in_buffer.map_to_host() as host_buffer:\n var host_tensor = LayoutTensor[dtype, layout](host_buffer)\n print(host_tensor)\n```\n\n```text\n0 2 4 6\n8 10 12 14\n16 18 20 22\n24 26 28 30\n```\n\nCongratulations! You've successfully run a kernel that modifies values from your\nGPU, and printed the result on your CPU. You can see above that each thread\nmultiplied a single value by 2 in parallel on the GPU, and copied the result\nback to the CPU.\n\n## Sum reduce output\n\nWe're going to set up a new buffer which will have all the reduced values with\nthe sum of each thread in the block:\n\n```plaintext\nOutput: [ block[0] block[1] block[2] block[3] ]\n```\n\nSet up the output buffer/tensor for the host and device:\n\n```mojo :clear\nvar out_buffer = ctx.enqueue_create_buffer[dtype](blocks)", "position": 27, "token_count": 235, "has_code": true, "section_hierarchy": ["Sum reduce output"], "metadata": {"chunk_id": "gpu-basics-027", "document_id": "gpu-basics", "position": 27, "token_count": 235, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Sum reduce output"], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics#sum-reduce-output"}}
{"chunk_id": "gpu-basics-028", "document_id": "gpu-basics", "content": "# Zero the values on the device as they'll be used to accumulate results\n_ = out_buffer.enqueue_fill(0)\n\nalias out_layout = Layout.row_major(blocks)\nalias OutTensor = LayoutTensor[dtype, out_layout, MutableAnyOrigin]\n\nvar out_tensor = OutTensor(out_buffer)\n```\n\nThe problem here is that we can't have all the threads summing their values into\nthe same index in the output buffer as that will introduce race conditions.\nWe're going to introduce new concepts to deal with this.", "position": 28, "token_count": 128, "has_code": true, "section_hierarchy": ["Zero the values on the device as they'll be used to accumulate results"], "metadata": {"chunk_id": "gpu-basics-028", "document_id": "gpu-basics", "position": 28, "token_count": 128, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Zero the values on the device as they'll be used to accumulate results"], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics#zero-the-values-on-the-device-as-theyll-be-used-to-accumulate-results"}}
{"chunk_id": "gpu-basics-029", "document_id": "gpu-basics", "content": "## Shared memory\n\nThis kernel uses shared memory to accumulate values. Shared memory is much\nfaster than global memory because it resides on-chip, closer to the processing\ncores, reducing latency and increasing bandwidth. It's not an optimal solution\nfor this kind of reduction operation, but it's a good way to introduce shared\nmemory in a simple example. We'll cover better solutions in the next sections.\n\n```mojo :once\nfn sum_reduce_kernel(\n in_tensor: InTensor, out_tensor: OutTensor\n):\n # This allocates memory to be shared between threads in a block prior to the\n # kernel launching. Each kernel gets a pointer to the allocated memory.\n var shared = stack_allocation[\n threads,\n Scalar[dtype],\n address_space = AddressSpace.SHARED,\n ]()\n\n # Place the corresponding value into shared memory\n shared[thread_idx.x] = in_tensor[block_idx.x, thread_idx.x][0]\n\n # Await all the threads to finish loading their values into shared memory\n barrier()", "position": 29, "token_count": 229, "has_code": true, "section_hierarchy": ["Shared memory"], "metadata": {"chunk_id": "gpu-basics-029", "document_id": "gpu-basics", "position": 29, "token_count": 229, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Shared memory"], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics#shared-memory"}}
{"chunk_id": "gpu-basics-030", "document_id": "gpu-basics", "content": "# Await all the threads to finish loading their values into shared memory\n barrier()\n\n # If this is the first thread, sum and write the result to global memory\n if thread_idx.x == 0:\n for i in range(threads):\n out_tensor[block_idx.x] += shared[i]\n\nctx.enqueue_function[sum_reduce_kernel](\n in_tensor,\n out_tensor,\n grid_dim=blocks,\n block_dim=threads,\n)", "position": 30, "token_count": 107, "has_code": false, "section_hierarchy": ["Await all the threads to finish loading their values into shared memory"], "metadata": {"chunk_id": "gpu-basics-030", "document_id": "gpu-basics", "position": 30, "token_count": 107, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Await all the threads to finish loading their values into shared memory"], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics#await-all-the-threads-to-finish-loading-their-values-into-shared-memory"}}
{"chunk_id": "gpu-basics-031", "document_id": "gpu-basics", "content": "# Copy the data back to the host and print out the buffer\nwith out_buffer.map_to_host() as host_buffer:\n print(host_buffer)\n```\n\n```text\nHostBuffer([6, 22, 38, 54])\n```\n\nFor our first block/tile we summed the values:\n\n```plaintext\nsum([ 0 1 2 3 ]) == 6\n```\n\nAnd the reduction resulted in the output having the sum of 6 in the first\nposition. Every tile/block has been reduced to:\n\n```plaintext\n[ 6 22 38 54]\n```", "position": 31, "token_count": 134, "has_code": true, "section_hierarchy": ["Copy the data back to the host and print out the buffer"], "metadata": {"chunk_id": "gpu-basics-031", "document_id": "gpu-basics", "position": 31, "token_count": 134, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Copy the data back to the host and print out the buffer"], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics#copy-the-data-back-to-the-host-and-print-out-the-buffer"}}
{"chunk_id": "gpu-basics-032", "document_id": "gpu-basics", "content": "## Sum multiple values from a single thread\n\nWe could skip using shared memory altogether by launching a single thread per\nblock. Each thread can load more than a single value, here we'll be launching\none thread per block, loading the 4 corresponding values from that block, and\nsumming them together:\n\n```mojo :once\nfn simd_reduce_kernel(\n in_tensor: InTensor, out_tensor: OutTensor\n):\n # The [4] means it loads 4 sequential values before doing the `reduce_add`\n out_tensor[block_idx.x] = in_tensor.load[4](block_idx.x, 0).reduce_add()\n\nctx.enqueue_function[simd_reduce_kernel](\n in_tensor,\n out_tensor,\n grid_dim=blocks,\n block_dim=1, # one thread per block\n)", "position": 32, "token_count": 192, "has_code": true, "section_hierarchy": ["Sum multiple values from a single thread"], "metadata": {"chunk_id": "gpu-basics-032", "document_id": "gpu-basics", "position": 32, "token_count": 192, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Sum multiple values from a single thread"], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics#sum-multiple-values-from-a-single-thread"}}
{"chunk_id": "gpu-basics-033", "document_id": "gpu-basics", "content": "# Ensure we have the same result\nwith out_buffer.map_to_host() as host_buffer:\n print(host_buffer)\n```\n\n```text\nHostBuffer([6, 22, 38, 54])\n```\n\nThis is cleaner and faster, instead of 4 threads writing to shared memory, we're\nusing 1 thread per block and summing them together without the intermediate\nstep. However, this can be even faster by launching one thread per value and\ndoing a single instruction in parallel using warps.", "position": 33, "token_count": 113, "has_code": true, "section_hierarchy": ["Ensure we have the same result"], "metadata": {"chunk_id": "gpu-basics-033", "document_id": "gpu-basics", "position": 33, "token_count": 113, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Ensure we have the same result"], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics#ensure-we-have-the-same-result"}}
{"chunk_id": "gpu-basics-034", "document_id": "gpu-basics", "content": "## Warps\n\nnote Warps\n\nWarp level instructions are an advanced concept, this section is to demonstrate\nthat these low-level primitives are available from Mojo. We'll go into more\ndepth on warps later, so don't worry if it doesn't make sense yet.\n\nA _warp_ is a group of threads (32 on NVIDIA GPUs) within a block. Threads\nwithin the same warp can synchronize their execution, and take advantage of\n[Single Instruction, Multiple\nThreads](https://en.wikipedia.org/wiki/Single_instruction,_multiple_threads)\n(SIMT). SIMT (GPU-focused) allows multiple threads to execute the same\ninstruction on different data with independent control flow and thread states,\nwhile SIMD (CPU-focused) applies a single instruction to multiple data elements\nsimultaneously with no thread independence.\n\nWe have only 4 threads within each block, well under the 32 limit, if this\nwasn't the case you'd have to do two reductions, one from each warp to shared\nmemory, then another from shared memory to the output buffer or tensor.", "position": 34, "token_count": 234, "has_code": false, "section_hierarchy": ["Warps"], "metadata": {"chunk_id": "gpu-basics-034", "document_id": "gpu-basics", "position": 34, "token_count": 234, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Warps"], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics#warps"}}
{"chunk_id": "gpu-basics-035", "document_id": "gpu-basics", "content": "Here is a simple warp reduction kernel:\n\n```mojo :once\nfn warp_reduce_kernel(\n in_tensor: InTensor, out_tensor: OutTensor\n):\n var value = in_tensor.load[1](block_idx.x, thread_idx.x)\n\n # Each thread gets the value from one thread higher, summing them as they go\n value = warp.sum(value)\n\n # Print each reduction step in the first block\n if block_idx.x == 0:\n print(\"thread:\", thread_idx.x, \"value:\", value)\n\n # Thread 0 has the reduced sum of the values from all the other threads\n if thread_idx.x == 0:\n out_tensor[block_idx.x] = value\n\nctx.enqueue_function[warp_reduce_kernel](\n in_tensor,\n out_tensor,\n grid_dim=blocks,\n block_dim=threads,\n)", "position": 35, "token_count": 209, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "gpu-basics-035", "document_id": "gpu-basics", "position": 35, "token_count": 209, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics"}}
{"chunk_id": "gpu-basics-036", "document_id": "gpu-basics", "content": "# Ensure we have the same result\nwith out_buffer.map_to_host() as host_buffer:\n print(host_buffer)\n```\n\n```text\nthread: 0 value: 6\nthread: 1 value: 6\nthread: 2 value: 6\nthread: 3 value: 6\nHostBuffer([6, 22, 38, 54])\n```\n\nYou can see in the output that the first block had the values [0 1 2 3] and was\nreduced from top to bottom (shuffle down) in this way, where the sum result of\none thread is passed to the next thread down:\n\n| Thread | value | next_value | result |\n|--------|-------|------------|--------|\n| 3 | 3 | N/A | 3 |\n| 2 | 2 | 3 | 5 |\n| 1 | 1 | 5 | 6 |\n| 0 | 0 | 6 | 6 |", "position": 36, "token_count": 218, "has_code": true, "section_hierarchy": ["Ensure we have the same result"], "metadata": {"chunk_id": "gpu-basics-036", "document_id": "gpu-basics", "position": 36, "token_count": 218, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Ensure we have the same result"], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics#ensure-we-have-the-same-result"}}
{"chunk_id": "gpu-basics-037", "document_id": "gpu-basics", "content": "## Exercise\n\nNow that we've covered some of the core primitives for GPU programming, here's\nan exercise to solidify your understanding. Feel free to revisit the examples as\nyou work through it the first time, then challenge yourself to write the code\nindependently. Experimenting with the code and observing the results is also a\nhighly valuable way to deepen your skills, don't hesitate to tweak things and\nsee what happens!\n\n1. Create a host buffer for the input of `DType` `Float32`, with 32 elements,\nand initialize the numbers ordered sequentially. Copy the host buffer to the\ndevice.\n2. Create a in_tensor that wraps the host buffer, with the dimensions (8, 4)\n3. Create an host and device buffer for the output of `DType` `Float32`, with 8\nelements, don't forget to zero the values with `enqueue_fill()`.\n4. Launch a GPU kernel with 8 blocks and 4 threads that reduce sums the values,\nusing your preferred method to write to the output buffer.\n5. Copy the device buffer to the host buffer, and print it out on the CPU.", "position": 37, "token_count": 247, "has_code": false, "section_hierarchy": ["Exercise"], "metadata": {"chunk_id": "gpu-basics-037", "document_id": "gpu-basics", "position": 37, "token_count": 247, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Exercise"], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics#exercise"}}
{"chunk_id": "gpu-basics-038", "document_id": "gpu-basics", "content": "<details>\n <summary>Click to expand answer.</summary>\n\n```mojo :reset\nfrom gpu import thread_idx, block_idx, warp\nfrom gpu.host import DeviceContext\nfrom layout import Layout, LayoutTensor\nfrom math import iota\n\nalias dtype = DType.float32\nalias blocks = 8\nalias threads = 4\nalias elements_in = blocks * threads", "position": 38, "token_count": 85, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "gpu-basics-038", "document_id": "gpu-basics", "position": 38, "token_count": 85, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics"}}
{"chunk_id": "gpu-basics-039", "document_id": "gpu-basics", "content": "# Create context\nvar ctx = DeviceContext()\n\n# Create buffers\nvar in_buffer = ctx.enqueue_create_buffer[dtype](elements_in)\nvar out_buffer = ctx.enqueue_create_buffer[dtype](blocks)\n\n# Fill in input values sequentially and copy to device\nwith in_buffer.map_to_host() as host_buffer:\n iota(host_buffer.unsafe_ptr(), elements_in)\n\n# Zero output buffer values\n_ = out_buffer.enqueue_fill(0)", "position": 39, "token_count": 129, "has_code": false, "section_hierarchy": ["Zero output buffer values"], "metadata": {"chunk_id": "gpu-basics-039", "document_id": "gpu-basics", "position": 39, "token_count": 129, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Zero output buffer values"], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics#zero-output-buffer-values"}}
{"chunk_id": "gpu-basics-040", "document_id": "gpu-basics", "content": "# Create the LayoutTensors\nalias layout = Layout.row_major(blocks, threads)\nalias InTensor = LayoutTensor[dtype, layout, MutableAnyOrigin]\nvar in_tensor = InTensor(in_buffer)\n\nalias out_layout = Layout.row_major(blocks)\nalias OutTensor = LayoutTensor[dtype, out_layout, MutableAnyOrigin]\nvar out_tensor = OutTensor(out_buffer)\n\nfn reduce_sum(in_tensor: InTensor, out_tensor: OutTensor):\n var value = in_tensor.load[1](block_idx.x, thread_idx.x)\n value = warp.sum(value)\n if thread_idx.x == 0:\n out_tensor[block_idx.x] = value\n\nctx.enqueue_function[reduce_sum](\n in_tensor,\n out_tensor,\n grid_dim=blocks,\n block_dim=threads,\n)\n\nwith out_buffer.map_to_host() as host_buffer:\n print(host_buffer)", "position": 40, "token_count": 241, "has_code": false, "section_hierarchy": ["Create the LayoutTensors"], "metadata": {"chunk_id": "gpu-basics-040", "document_id": "gpu-basics", "position": 40, "token_count": 241, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Create the LayoutTensors"], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics#create-the-layouttensors"}}
{"chunk_id": "gpu-basics-041", "document_id": "gpu-basics", "content": "```\n\n```text\nHostBuffer([6.0, 22.0, 38.0, 54.0, 70.0, 86.0, 102.0, 118.0])\n```\n\n</details>", "position": 41, "token_count": 54, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "gpu-basics-041", "document_id": "gpu-basics", "position": 41, "token_count": 54, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics"}}
{"chunk_id": "gpu-basics-042", "document_id": "gpu-basics", "content": "## Next steps\n\nTo continue learning how to write Mojo for GPUs, check out the following\nresources:\n\n- [GPU puzzles](https://builds.modular.com/puzzles/):\n Learn how to program for GPUs by solving increasingly challenging puzzles.\n\n- [Build custom ops for GPUs](/max/tutorials/build-custom-ops): Learn to write\n custom ops for MAX graphs that can execute on both CPUs and GPUs.\n\n- [GPU programming examples in\n GitHub](https://github.com/modular/modular/tree/main/examples/mojo/gpu-functions):\n A variety of GPU code examples, from simple to complex.", "position": 42, "token_count": 152, "has_code": false, "section_hierarchy": ["Next steps"], "metadata": {"chunk_id": "gpu-basics-042", "document_id": "gpu-basics", "position": 42, "token_count": 152, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Next steps"], "file_path": "gpu/basics.mdx", "url": "https://docs.modular.com/mojo/manual/gpu/basics", "title": "Basics of GPU programming with Mojo", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/gpu/basics#next-steps"}}
