{"chunk_id": "layout-tensors-000", "document_id": "layout-tensors", "content": "A [`LayoutTensor`](/mojo/kernels/layout/layout_tensor/LayoutTensor/)\nprovides a view of multi-dimensional data stored in a linear\narray. `LayoutTensor` abstracts the logical organization of multi-dimensional\ndata from its actual arrangement in memory. You can generate new tensor \"views\"\nof the same data without copying the underlying data.\nThis facilitates essential patterns for writing performant computational\nalgorithms, such as:\n\n- Extracting tiles (sub-tensors) from existing tensors. This is especially\n valuable on the GPU, allowing a thread block to load a tile into shared\n memory, for faster access and more efficient caching.\n- Vectorizing tensors—reorganizing them into multi-element vectors for more\n performant memory loads and stores.\n- Partitioning a tensor into thread-local fragments to distribute work across a\n thread block.\n\n`LayoutTensor` is especially valuable for writing GPU kernels, and a number of\nits APIs are GPU-specific. However, `LayoutTensor` can also be used for\nCPU-based algorithms.\n\nA `LayoutTensor` consists of three main properties:", "position": 0, "token_count": 240, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-000", "document_id": "layout-tensors", "position": 0, "token_count": 240, "has_code": false, "overlap_with_previous": false, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
{"chunk_id": "layout-tensors-001", "document_id": "layout-tensors", "content": "A `LayoutTensor` consists of three main properties:\n\n* A [layout](/mojo/manual/layout/layouts), defining how the elements are\n laid out in memory.\n* A `DType`, defining the data type stored in the tensor.\n* A pointer to memory where the data is stored.\n\nFigure 1 shows the relationship between the layout and the storage.\n\n<figure>\n\n![](../images/layout/tensors/layout-tensor-indexing-simple.png#light)\n![](../images/layout/tensors/layout-tensor-indexing-simple-dark.png#dark)\n\n<figcaption><b>Figure 1.</b> Layout and storage for a 2D tensor</figcaption>\n\n</figure>\n\nFigure 1 shows a 2D column-major layout, and the corresponding linear array of\nstorage. The values shown inside the layout are offsets into the storage: so the\ncoordinates (0, 1) correspond to offset 2 in the storage.", "position": 1, "token_count": 220, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-001", "document_id": "layout-tensors", "position": 1, "token_count": 220, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
{"chunk_id": "layout-tensors-002", "document_id": "layout-tensors", "content": "Figure 1 shows a 2D column-major layout, and the corresponding linear array of\nstorage. The values shown inside the layout are offsets into the storage: so the\ncoordinates (0, 1) correspond to offset 2 in the storage.\n\nBecause `LayoutTensor` is a view, creating a new tensor based on an existing\ntensor doesn't require copying the underlying data. So you can easily create a\nnew view, representing a tile (sub-tensor), or accessing the elements in a\ndifferent order. These views all access the same data, so changing the stored\ndata in one view changes the data seen by all of the views.\n\nEach element in a tensor can be either a single (scalar) value or a SIMD vector\nof values. For a vectorized layout, you can specify an *element layout* that\ndetermines how the vector elements are laid out in memory. For more information,\nsee [Vectorizing tensors](#vectorizing-tensors).", "position": 2, "token_count": 200, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-002", "document_id": "layout-tensors", "position": 2, "token_count": 200, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
{"chunk_id": "layout-tensors-003", "document_id": "layout-tensors", "content": "## Accessing tensor elements\n\nFor tensors with simple row-major or column-major layouts, you can address a\nlayout tensor like a multidimensional array to access elements:\n\n```mojo\nelement = tensor2d[x, y]\ntensor2d[x, y] = z\n```\n\nThe number of indices passed to the subscript operator must match the number of\ncoordinates required by the tensor. For simple layouts, this is the same as the\nlayout's *rank*: two for a 2D tensor, three for a 3D tensor, and so on. If the\nnumber of indices is incorrect, you may see a cryptic runtime error.\n\n```mojo", "position": 3, "token_count": 143, "has_code": true, "section_hierarchy": ["Accessing tensor elements"], "metadata": {"chunk_id": "layout-tensors-003", "document_id": "layout-tensors", "position": 3, "token_count": 143, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Accessing tensor elements"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#accessing-tensor-elements"}}
{"chunk_id": "layout-tensors-004", "document_id": "layout-tensors", "content": "# Indexing into a 2D tensor requires two indices\nel1 = tensor2d[x, y] # Works\nel2 = tensor2d[x] # Runtime error\n```\n\nFor more complicated \"nested\" layouts, such as tiled layouts, the\nnumber of indices **doesn't** match the rank of the tensor. For details, see\n[Tensor indexing and nested layouts](#tensor-indexing-and-nested-layouts).\n\nThe `__getitem__()` method returns a SIMD vector of elements, and the compiler\ncan't statically determine the size of the vector (which is the size of the\ntensor's element layout). This can cause type checking errors at compile time,\nbecause some APIs can only accept scalar values (SIMD vectors of width 1). For\nexample, consider the following code:\n\n```mojo\ni: Int = SIMD[DType.int32, width](15)", "position": 4, "token_count": 214, "has_code": true, "section_hierarchy": ["Indexing into a 2D tensor requires two indices"], "metadata": {"chunk_id": "layout-tensors-004", "document_id": "layout-tensors", "position": 4, "token_count": 214, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Indexing into a 2D tensor requires two indices"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#indexing-into-a-2d-tensor-requires-two-indices"}}
{"chunk_id": "layout-tensors-005", "document_id": "layout-tensors", "content": "```\n\nIf `width` is 1, the vector can be implicitly converted to an `Int`, but if\n`width` is any other value, the vector can't be implicitly converted. If `width`\nisn't known at compile time, this produces an error.\n\nIf your tensor stores scalar values, you can work around this by explicitly\ntaking the first item in the vector:\n\n```mojo\nvar element = tensor[row, col][0] # element is guaranteed to be a scalar value\n```\n\nYou can also access elements using the `load()` and `store()` methods, which\nlet you specify the vector width explicitly:\n\n```mojo\nvar elements: SIMD[DType.float32, 4]\nvar elements = tensor.load[4](row, col)\nelements = elements * 2\ntensor.store(row, col, elements)\n```", "position": 5, "token_count": 194, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-005", "document_id": "layout-tensors", "position": 5, "token_count": 194, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
{"chunk_id": "layout-tensors-006", "document_id": "layout-tensors", "content": "### Tensor indexing and nested layouts\n\nA tensor's layout may have nested modes (or sub-layouts), as described in\n[Introduction to layouts](/mojo/manual/layout/layouts#modes). These layouts have\none or more of their dimensions divided into sub-layouts. For example, Figure 2\nshows a tensor with a nested layout:\n\n<figure>\n\n![](../images/layout/tensors/layout-tensor-indexing-nested.png#light)\n![](../images/layout/tensors/layout-tensor-indexing-nested-dark.png#dark)\n\n<figcaption><b>Figure 2.</b> Tensor with nested layout</figcaption>\n\n</figure>", "position": 6, "token_count": 178, "has_code": false, "section_hierarchy": ["Tensor indexing and nested layouts"], "metadata": {"chunk_id": "layout-tensors-006", "document_id": "layout-tensors", "position": 6, "token_count": 178, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Tensor indexing and nested layouts"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#tensor-indexing-and-nested-layouts"}}
{"chunk_id": "layout-tensors-007", "document_id": "layout-tensors", "content": "<figcaption><b>Figure 2.</b> Tensor with nested layout</figcaption>\n\n</figure>\n\nFigure 2 shows a tensor with a tile-major nested layout. Instead of being\naddressed with a single coordinate on each axis, it has a pair of coordinates\nper axis. For example, the coordinates `((1, 0), (0, 1))` map to the offset 6.\n\nYou can't pass nested coordinates to the subscript operator `[]`, but you can\npass a flattened version of the coordinates. For example:\n\n```mojo", "position": 7, "token_count": 129, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-007", "document_id": "layout-tensors", "position": 7, "token_count": 129, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
{"chunk_id": "layout-tensors-008", "document_id": "layout-tensors", "content": "# retrieve the value at ((1, 0), (0, 1))\nelement = nested_tensor[1, 0, 0, 1][0]\n```\n\nThe number of indices passed to the subscript operator must match the *flattened\nrank* of the tensor.\n\nYou can't currently use the `load()` and `store()` methods for tensors with\nnested layouts. However, these methods are usually used on tensors that have\nbeen [*tiled*](#tiling-tensors), which yields a tensor with a simple layout.", "position": 8, "token_count": 126, "has_code": true, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))"], "metadata": {"chunk_id": "layout-tensors-008", "document_id": "layout-tensors", "position": 8, "token_count": 126, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["retrieve the value at ((1, 0), (0, 1))"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#retrieve-the-value-at-1-0-0-1"}}
{"chunk_id": "layout-tensors-009", "document_id": "layout-tensors", "content": "## Creating a LayoutTensor\n\nThere are several ways to create a `LayoutTensor`, depending on where the tensor\ndata resides:\n\n* On the CPU.\n* In GPU global memory.\n* In GPU shared or local memory.\n\nIn addition to methods for creating a tensor from scratch, `LayoutTensor`\nprovides a number of methods for producing a new view of an existing tensor.\n\nnote No bounds checking\n\nThe `LayoutTensor` constructors don't do any bounds-checking to verify\nthat the allocated memory is large enough to hold all of the elements specified\nin the layout. It's up to the user to ensure that the proper amount of space is\nallocated.", "position": 9, "token_count": 140, "has_code": false, "section_hierarchy": ["Creating a LayoutTensor"], "metadata": {"chunk_id": "layout-tensors-009", "document_id": "layout-tensors", "position": 9, "token_count": 140, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Creating a LayoutTensor"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#creating-a-layouttensor"}}
{"chunk_id": "layout-tensors-010", "document_id": "layout-tensors", "content": "### Creating a `LayoutTensor` on the CPU\n\nWhile `LayoutTensor` is often used on the GPU, you can also use it to create\ntensors for use on the CPU.\n\nTo create a `LayoutTensor` for use on the CPU, you need a `Layout` and a block of\nmemory to store the tensor data. A common way to allocate memory for a\n`LayoutTensor` is to use an\n[`InlineArray`](/mojo/stdlib/collections/inline_array/InlineArray/):\n\n```mojo\nalias rows = 8\nalias columns = 16\nalias layout = Layout.row_major(rows, columns)\nvar storage = InlineArray[Float32, rows * columns](uninitialized=True)\nvar tensor = LayoutTensor[DType.float32, layout](storage).fill(0)", "position": 10, "token_count": 191, "has_code": true, "section_hierarchy": ["Creating a `LayoutTensor` on the CPU"], "metadata": {"chunk_id": "layout-tensors-010", "document_id": "layout-tensors", "position": 10, "token_count": 191, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Creating a `LayoutTensor` on the CPU"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#creating-a-layouttensor-on-the-cpu"}}
{"chunk_id": "layout-tensors-011", "document_id": "layout-tensors", "content": "```\n\n`InlineArray` is a statically-sized, stack-allocated array, so it's a fast and\nefficient way to allocate storage for most kinds of `LayoutTensor`. There are\ntarget-dependent limits on how much memory can be allocated this way, however.\n\nYou can also create a `LayoutTensor` using an `UnsafePointer`. This may be\npreferable for very large tensors.\n\n```mojo\nalias rows = 1024\nalias columns = 1024\nalias buf_size = rows * columns\nalias layout = Layout.row_major(rows, columns)\nvar ptr = alloc[Float32](buf_size)\nmemset(ptr, 0, buf_size)\nvar tensor = LayoutTensor[DType.float32, layout](ptr)", "position": 11, "token_count": 178, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-011", "document_id": "layout-tensors", "position": 11, "token_count": 178, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
{"chunk_id": "layout-tensors-012", "document_id": "layout-tensors", "content": "```\n\nNote that this example uses [`memset()`](/mojo/stdlib/memory/memory/memset/)\ninstead of the `LayoutTensor` `fill()` method. The `fill()` method performs\nelementwise initialization of the tensor, so it may be slow for large tensors.", "position": 12, "token_count": 77, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-012", "document_id": "layout-tensors", "position": 12, "token_count": 77, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
{"chunk_id": "layout-tensors-013", "document_id": "layout-tensors", "content": "### Creating a `LayoutTensor` on the GPU\n\nWhen creating a `LayoutTensor` for use on the GPU, you need to consider which\nmemory space the tensor data will be stored in:\n\n* GPU global memory can only be allocated from the host (CPU), as a\n `DeviceBuffer`.\n* GPU shared or local memory can be statically allocated on the GPU.", "position": 13, "token_count": 87, "has_code": false, "section_hierarchy": ["Creating a `LayoutTensor` on the GPU"], "metadata": {"chunk_id": "layout-tensors-013", "document_id": "layout-tensors", "position": 13, "token_count": 87, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Creating a `LayoutTensor` on the GPU"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#creating-a-layouttensor-on-the-gpu"}}
{"chunk_id": "layout-tensors-014", "document_id": "layout-tensors", "content": "#### Creating a `LayoutTensor` in global memory\n\nYou must allocate global memory from the host side, by allocating a\n[`DeviceBuffer`](/mojo/stdlib/gpu/host/device_context/DeviceBuffer/). You can\neither construct a `LayoutTensor` using this memory on the host side, before\ninvoking a GPU kernel, or you can construct a LayoutTensor inside the kernel\nitself:\n\n* On the CPU, you can construct a `LayoutTensor` using a `DeviceBuffer` as its\n storage. Although you can create this tensor on the CPU and pass it in to a\n kernel function, you can't directly modify its values on the CPU, since the\n memory is on the GPU.\n* On the GPU: When a `DeviceBuffer` is passed in to\n `enqueue_function_checked()`, the kernel receives a corresponding\n `UnsafePointer` in place of the `DeviceBuffer.` The kernel can then create a\n `LayoutTensor` using the `pointer`.", "position": 14, "token_count": 231, "has_code": false, "section_hierarchy": ["Creating a `LayoutTensor` in global memory"], "metadata": {"chunk_id": "layout-tensors-014", "document_id": "layout-tensors", "position": 14, "token_count": 231, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Creating a `LayoutTensor` in global memory"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#creating-a-layouttensor-in-global-memory"}}
{"chunk_id": "layout-tensors-015", "document_id": "layout-tensors", "content": "In both cases, if you want to initialize data for the tensor from the CPU, you\ncan call\n[`enqueue_copy()`](/mojo/stdlib/gpu/host/device_context/DeviceContext#enqueue_copy)\nor\n[`enqueue_memset()`](/mojo/stdlib/gpu/host/device_context/DeviceContext#enqueue_memset)\non the buffer prior to invoking the kernel. The following example shows\ninitializing a `LayoutTensor` from the CPU and passing it to a GPU kernel.\n\n```mojo\ndef initialize_tensor_from_cpu_example():\n alias dtype = DType.float32\n alias rows = 32\n alias cols = 8\n alias block_size = 8\n alias row_blocks = rows // block_size\n alias col_blocks = cols // block_size\n alias input_layout = Layout.row_major(rows, cols)\n alias size: Int = rows * cols", "position": 15, "token_count": 228, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-015", "document_id": "layout-tensors", "position": 15, "token_count": 228, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
{"chunk_id": "layout-tensors-016", "document_id": "layout-tensors", "content": "fn kernel(tensor: LayoutTensor[dtype, input_layout, MutAnyOrigin]):\n if global_idx.y < UInt(tensor.shape[0]()) and global_idx.x < UInt(\n tensor.shape[1]()\n ):\n tensor[global_idx.y, global_idx.x] = (\n tensor[global_idx.y, global_idx.x] + 1\n )\n\n try:\n var ctx = DeviceContext()\n var host_buf = ctx.enqueue_create_host_buffer[dtype](size)\n var dev_buf = ctx.enqueue_create_buffer[dtype](size)\n ctx.synchronize()\n\n var expected_values = List[Scalar[dtype]](capacity=size)", "position": 16, "token_count": 192, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-016", "document_id": "layout-tensors", "position": 16, "token_count": 192, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
{"chunk_id": "layout-tensors-017", "document_id": "layout-tensors", "content": "var expected_values = List[Scalar[dtype]](capacity=size)\n\n for i in range(size):\n host_buf[i] = Scalar[dtype](i)\n expected_values[i] = Scalar[dtype](i + 1)\n ctx.enqueue_copy(dev_buf, host_buf)\n var tensor = LayoutTensor[dtype, input_layout](dev_buf)\n\n ctx.enqueue_function_checked[kernel, kernel](\n tensor,\n grid_dim=(col_blocks, row_blocks),\n block_dim=(block_size, block_size),\n )\n ctx.enqueue_copy(host_buf, dev_buf)\n ctx.synchronize()", "position": 17, "token_count": 178, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-017", "document_id": "layout-tensors", "position": 17, "token_count": 178, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
{"chunk_id": "layout-tensors-018", "document_id": "layout-tensors", "content": "for i in range(rows * cols):\n if host_buf[i] != expected_values[i]:\n raise Error(\n String(\"Error at position {} expected {} got {}\").format(\n i, expected_values[i], host_buf[i]\n )\n )\n except error:\n print(error)", "position": 18, "token_count": 77, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-018", "document_id": "layout-tensors", "position": 18, "token_count": 77, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
{"chunk_id": "layout-tensors-019", "document_id": "layout-tensors", "content": "```", "position": 19, "token_count": 5, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-019", "document_id": "layout-tensors", "position": 19, "token_count": 5, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
{"chunk_id": "layout-tensors-020", "document_id": "layout-tensors", "content": "#### Creating a `LayoutTensor` in shared or local memory\n\nTo create a tensor on the GPU in shared memory or local memory, use the\n`LayoutTensor.stack_allocation()` static method to create a tensor with backing\nmemory in the appropriate memory space.\n\nBoth shared and local memory are very limited resources, so a common pattern\nis to copy a small tile of a larger tensor into shared memory or local memory to\nreduce memory access time.\n\n```mojo\nalias tile_layout = Layout.row_major(16, 16)\n\nvar shared_tile = LayoutTensor[\n dtype,\n tile_layout,\n MutAnyOrigin,\n address_space = AddressSpace.SHARED,\n].stack_allocation()", "position": 20, "token_count": 154, "has_code": true, "section_hierarchy": ["Creating a `LayoutTensor` in shared or local memory"], "metadata": {"chunk_id": "layout-tensors-020", "document_id": "layout-tensors", "position": 20, "token_count": 154, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Creating a `LayoutTensor` in shared or local memory"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#creating-a-layouttensor-in-shared-or-local-memory"}}
{"chunk_id": "layout-tensors-021", "document_id": "layout-tensors", "content": "```\n\nIn the case of shared memory, all threads in a thread block see the same\nallocation. For local memory, each thread gets a separate allocation.\n\nThere's no way to explicitly allocate _register_ memory. However, the\ncompiler can promote some local memory allocations to registers. To enable\nthis optimization, keep the size of the tensor small, and all indexing into the\ntensor static—for example, using `@parameter for` loops.\n\n[note]\nThe name `stack_allocation()` is misleading. It is a *static* allocation,\nmeaning the allocation is processed at compile time. The allocation is like a\nC/C++ stack allocation in that its lifetime ends when the function in which it\nwas allocated returns. This API may be subject to change in the near future.", "position": 21, "token_count": 168, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-021", "document_id": "layout-tensors", "position": 21, "token_count": 168, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
{"chunk_id": "layout-tensors-022", "document_id": "layout-tensors", "content": "## Tiling tensors\n\nA fundamental pattern for using a layout tensor is to divide the tensor into\nsmaller tiles to achieve better data locality and cache efficiency. In a GPU\nkernel you may want to select a tile that corresponds to the size of a thread\nblock. For example, given a 2D thread block of 16x16 threads, you could use a\n16x16 tile (with each thread handling one element in the tile) or a 64x16 tile\n(with each thread handling 4 elements from the tensor).\n\nTiles are most commonly 1D or 2D. For element-wise calculations, where the\noutput value for a given tensor element depends on only one input value, 1D\ntiles are easy to reason about. For calculations that involve neighboring\nelements, 2D tiles can help maintain data locality. For example, matrix\nmultiplication or 2D convolution operations usually use 2D tiles.\n\n`LayoutTensor` provides a `tile()` method for extracting a single tile. You can\nalso iterate through tiles using the `LayoutTensorIter` type.", "position": 22, "token_count": 215, "has_code": false, "section_hierarchy": ["Tiling tensors"], "metadata": {"chunk_id": "layout-tensors-022", "document_id": "layout-tensors", "position": 22, "token_count": 215, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Tiling tensors"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#tiling-tensors"}}
{"chunk_id": "layout-tensors-023", "document_id": "layout-tensors", "content": "`LayoutTensor` provides a `tile()` method for extracting a single tile. You can\nalso iterate through tiles using the `LayoutTensorIter` type.\n\nWhen tiling a tensor that isn't an exact multiple of the tile size, you can\ncreate the tensor as a *masked tensor* (with the optional `masked` parameter set\nto True). When tiling a masked tensor, the tile operations will return partial\ntiles at the edges of the tensor. These tiles will be smaller than the requested\ntile size. You can use the `tensor.dim(axis)` method to query the tile\ndimensions at runtime.", "position": 23, "token_count": 134, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-023", "document_id": "layout-tensors", "position": 23, "token_count": 134, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
{"chunk_id": "layout-tensors-024", "document_id": "layout-tensors", "content": "### Extracting a tile\n\nThe\n[`LayoutTensor.tile()`](/mojo/kernels/layout/layout_tensor/LayoutTensor/#tile)\nmethod extracts a tile with a given size at a given set of coordinates:\n\n```mojo\nalias rows = 2\nalias columns = 4\nalias tile_size = 32\nalias tile_layout = Layout.row_major(tile_size, tile_size)\nalias tiler_layout = Layout.row_major(rows, columns)\nalias tiled_layout = blocked_product(tile_layout, tiler_layout)\nvar storage = InlineArray[Float32, tiled_layout.size()](uninitialized=True)\nfor i in range(tiled_layout.size()):\n storage[i] = i\nvar tensor = LayoutTensor[DType.float32, tiled_layout](storage)\nvar tile = tensor.tile[tile_size, tile_size](0, 1)", "position": 24, "token_count": 213, "has_code": true, "section_hierarchy": ["Extracting a tile"], "metadata": {"chunk_id": "layout-tensors-024", "document_id": "layout-tensors", "position": 24, "token_count": 213, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Extracting a tile"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#extracting-a-tile"}}
{"chunk_id": "layout-tensors-025", "document_id": "layout-tensors", "content": "```\n\nThis code creates a 64x128 tensor with 32x32 tiles, and extracts the tile\nat row 0, column 1, as shown in Figure 3.\n\n<figure>\n\n![](../images/layout/tensors/layout-tensor-tile.png#light)\n![](../images/layout/tensors/layout-tensor-tile-dark.png#dark)\n\n<figcaption><b>Figure 3.</b> Extracting a tile from a tensor</figcaption>\n\n</figure>\n\nNote that the coordinates are specified in *tiles*.\n\nThe layout of the extracted tile depends on the layout of the parent tensor. For\nexample, if the parent tensor has a column-major layout, the extract tile has a\ncolumn-major layout.", "position": 25, "token_count": 174, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-025", "document_id": "layout-tensors", "position": 25, "token_count": 174, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
{"chunk_id": "layout-tensors-026", "document_id": "layout-tensors", "content": "The layout of the extracted tile depends on the layout of the parent tensor. For\nexample, if the parent tensor has a column-major layout, the extract tile has a\ncolumn-major layout.\n\nIf you're extracting a tile from a tensor with a tiled layout, the extracted\ntile must match the tile boundaries of the parent tensor. For example, if the\nparent tensor is composed of 8x8 row-major tiles, a tile size of 8x8 yields an\nextracted tile with an 8x8 row-major layout.\n\n[note]\nTrying to extract a tile that's not an even multiple of the parent tile size\nusually results in an error.\n\nIf you need to know the type of the tile (to declare a variable, for example),\nyou can use the `TileType` alias, with the same tile size parameters.", "position": 26, "token_count": 172, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-026", "document_id": "layout-tensors", "position": 26, "token_count": 172, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
{"chunk_id": "layout-tensors-027", "document_id": "layout-tensors", "content": "If you need to know the type of the tile (to declare a variable, for example),\nyou can use the `TileType` alias, with the same tile size parameters.\n\n```mojo\nvar my_tile: tensor.TileType[tile_size, tile_size]\nfor i in range (rows):\n for j in range(columns):\n my_tile = tensor.tile[tile_size, tile_size](i, j)\n # ... do something with the tile ...", "position": 27, "token_count": 110, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-027", "document_id": "layout-tensors", "position": 27, "token_count": 110, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
{"chunk_id": "layout-tensors-028", "document_id": "layout-tensors", "content": "```", "position": 28, "token_count": 5, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-028", "document_id": "layout-tensors", "position": 28, "token_count": 5, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
{"chunk_id": "layout-tensors-029", "document_id": "layout-tensors", "content": "### Tiled iterators\n\nThe [`LayoutTensorIter`](/mojo/kernels/layout/layout_tensor/LayoutTensorIter/)\nstruct provides a way to iterate through a block of memory, generating a layout\ntensor for each position. There are two ways to use `LayoutTensorIter`:\n\n* Starting with a memory buffer, you can generate a series of tiles.\n* Given an existing layout tensor, you can extract a set of tiles along a given\n axis.", "position": 29, "token_count": 107, "has_code": false, "section_hierarchy": ["Tiled iterators"], "metadata": {"chunk_id": "layout-tensors-029", "document_id": "layout-tensors", "position": 29, "token_count": 107, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Tiled iterators"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#tiled-iterators"}}
{"chunk_id": "layout-tensors-030", "document_id": "layout-tensors", "content": "#### Tiling a memory buffer\n\nWhen you start with a memory buffer, `LayoutTensorIter` iterates through the\nmemory one tile at a time. This essentially treats the memory as a flat array of\ntiles.\n\n```mojo\nalias buf_size = 128\nvar storage = InlineArray[Int16, buf_size](uninitialized=True)\nfor i in range(buf_size):\n storage[i] = i\nalias tile_layout = Layout.row_major(4, 4)\nvar iter = LayoutTensorIter[\n DType.int16,\n tile_layout,\n MutAnyOrigin\n](storage.unsafe_ptr(), buf_size)\n\nfor i in range(ceildiv(buf_size, tile_layout.size())):\n var tile = iter[]\n # ... do something with tile\n iter += 1", "position": 30, "token_count": 200, "has_code": true, "section_hierarchy": ["Tiling a memory buffer"], "metadata": {"chunk_id": "layout-tensors-030", "document_id": "layout-tensors", "position": 30, "token_count": 200, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Tiling a memory buffer"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#tiling-a-memory-buffer"}}
{"chunk_id": "layout-tensors-031", "document_id": "layout-tensors", "content": "```\n\nThe iterator constructor takes all of the parameters you'd use to construct a\n`LayoutTensor`—a `DType`, layout, and an origin—and as arguments it takes a\npointer and the size of the memory buffer.\n\nNote that the iterator doesn't work like a standard iterator, and you can't use\nit directly in a `for` statement like you would use a collection. Instead, you\ncan use either the dereference operator (`iter[]`) or the `get()` method to\nretrieve a `LayoutTensor` representing the tile at the current position.\n\nYou can advance the iterator by incrementing it, as shown above. The iterator\nalso supports `next()` and `next_unsafe()` methods, which return a copy of the\niterator incremented by a specified offset (default 1). This means you can also\nuse a pattern like this:\n\n```mojo\nfor i in range(num_tiles):\n current_tile = iter.next(i)[]\n …", "position": 31, "token_count": 236, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-031", "document_id": "layout-tensors", "position": 31, "token_count": 236, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
{"chunk_id": "layout-tensors-032", "document_id": "layout-tensors", "content": "```\n\n`LayoutTensorIter` also has an optional boolean `circular` parameter. A\n`LayoutTensorIter `created with `circular=True` treats the memory buffer as\ncircular; when it hits the end of the buffer, it starts over again at the\nbeginning.", "position": 32, "token_count": 60, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-032", "document_id": "layout-tensors", "position": 32, "token_count": 60, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
{"chunk_id": "layout-tensors-033", "document_id": "layout-tensors", "content": "### Tiling a `LayoutTensor`\n\nTo iterate over an existing tensor, call the\n[`tiled_iterator()`](/mojo/kernels/layout/layout_tensor/LayoutTensor/#tiled_iterator)\nmethod, which produces a `LayoutTensorIter`:\n\n```mojo", "position": 33, "token_count": 74, "has_code": true, "section_hierarchy": ["Tiling a `LayoutTensor`"], "metadata": {"chunk_id": "layout-tensors-033", "document_id": "layout-tensors", "position": 33, "token_count": 74, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["Tiling a `LayoutTensor`"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#tiling-a-layouttensor"}}
{"chunk_id": "layout-tensors-034", "document_id": "layout-tensors", "content": "# given a tensor of size rows x cols\nalias num_row_tiles = ceildiv(rows, tile_size)\nalias num_col_tiles = ceildiv(cols, tile_size)\n\nfor i in range(num_row_tiles):\n var iter = tensor.tiled_iterator[tile_size, tile_size, axis=1](i, 0)\n\n for _ in range(num_col_tiles):\n var tile = iter[]\n # ... do something with the tile\n iter += 1\n```", "position": 34, "token_count": 130, "has_code": true, "section_hierarchy": ["given a tensor of size rows x cols"], "metadata": {"chunk_id": "layout-tensors-034", "document_id": "layout-tensors", "position": 34, "token_count": 130, "has_code": true, "overlap_with_previous": true, "section_hierarchy": ["given a tensor of size rows x cols"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#given-a-tensor-of-size-rows-x-cols"}}
{"chunk_id": "layout-tensors-035", "document_id": "layout-tensors", "content": "## Vectorizing tensors\n\nWhen working with tensors, it's frequently efficient to access more than one\nvalue at a time. For example, having a single GPU thread calculate multiple\noutput values (\"thread coarsening\") can frequently improve performance.\nLikewise, when copying data from one memory space to another, it's often helpful\nfor each thread to copy a SIMD vector worth of values, instead of a single\nvalue. Many GPUs have vectorized copy instructions that can make copying more\nefficient.\n\nTo choose the optimum vector size, you need to know the vector operations\nsupported for your current GPU for the data type you're working with. (For\nexample, if you're working with 4 byte values and the GPU supports 16 byte copy\noperations, you can use a vector width of 4.) The `LayoutTensor` copy operations\nsupport copy sizes up to 16 bytes.\n\nThe [`vectorize()`](/mojo/kernels/layout/layout_tensor/LayoutTensor/#vectorize)\nmethod creates a new view of the tensor where each element of the tensor is a\nvector of values.", "position": 35, "token_count": 235, "has_code": false, "section_hierarchy": ["Vectorizing tensors"], "metadata": {"chunk_id": "layout-tensors-035", "document_id": "layout-tensors", "position": 35, "token_count": 235, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Vectorizing tensors"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#vectorizing-tensors"}}
{"chunk_id": "layout-tensors-036", "document_id": "layout-tensors", "content": "var vectorized_tensor = tensor.vectorize[1, 4]()\n\nThe vectorized tensor is a view of the original tensor, pointing to the same\ndata. The underlying number of scalar values remains the same, but the tensor\nlayout and element layout changes, as shown in Figure 4.\n\n<figure>\n\n![](../images/layout/tensors/vectorized-tensor.png#light)\n![](../images/layout/tensors/vectorized-tensor-dark.png#dark)\n\n<figcaption><b>Figure 4.</b> Vectorizing a tensor</figcaption>\n\n</figure>", "position": 36, "token_count": 145, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-036", "document_id": "layout-tensors", "position": 36, "token_count": 145, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
{"chunk_id": "layout-tensors-037", "document_id": "layout-tensors", "content": "## Partitioning a tensor across threads\n\nWhen working with tensors on the GPU, it's sometimes desirable to distribute the\nelements of a tensor across the threads in a thread block. The\n[`distribute()`](/mojo/kernels/layout/layout_tensor/LayoutTensor/#distribute)\nmethod takes a thread layout and a thread ID and returns a thread-specific\n*fragment* of the tensor.\n\nThe thread layout is tiled across the tensor. The *N*th thread receives a\nfragment consisting of the *N*th value from each tile. For example, Figure 5\nshows how `distribute()` forms fragments given a 4x4, row-major tensor and a\n2x2, column-major thread layout:\n\n<figure>\n\n![](../images/layout/tensors/distribute-layout.png#light)\n![](../images/layout/tensors/distribute-layout-dark.png#dark)\n\n<figcaption><b>Figure 5.</b> Partitioning a tensor into fragments</figcaption>", "position": 37, "token_count": 234, "has_code": false, "section_hierarchy": ["Partitioning a tensor across threads"], "metadata": {"chunk_id": "layout-tensors-037", "document_id": "layout-tensors", "position": 37, "token_count": 234, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Partitioning a tensor across threads"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#partitioning-a-tensor-across-threads"}}
{"chunk_id": "layout-tensors-038", "document_id": "layout-tensors", "content": "<figcaption><b>Figure 5.</b> Partitioning a tensor into fragments</figcaption>\n\n</figure>\n\nIn Figure 5, the numbers in the data layout represent offsets into storage, as\nusual. The numbers in the thread layout represent thread IDs.\n\nThe example in Figure 5 uses a small thread layout for illustration purposes. In\npractice, it's usually optimal to use a thread layout that's the same size as\nthe warp size of your GPU, so the work is divided across all available threads.\nFor example, the following code vectorizes and partitions a tensor over a full\nwarp worth of threads:\n\n```mojo\nalias thread_layout = Layout.row_major(WARP_SIZE // simd_size, simd_size)\nvar fragment = tile.vectorize[1, simd_size]().distribute[thread_layout](lane_id())", "position": 38, "token_count": 197, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-038", "document_id": "layout-tensors", "position": 38, "token_count": 197, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
{"chunk_id": "layout-tensors-039", "document_id": "layout-tensors", "content": "```\n\nGiven a 16x16 tile size, a warp size of 32 and a `simd_size` of 4, this code\nproduces a 16x4 tensor of 1x4 vectors. The thread layout is an 8x4 row major\nlayout.", "position": 39, "token_count": 55, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-039", "document_id": "layout-tensors", "position": 39, "token_count": 55, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
{"chunk_id": "layout-tensors-040", "document_id": "layout-tensors", "content": "## Copying tensors\n\nThe `layout-tensor` package provides a large set of utilities for copying\ntensors. A number of these are specialized for copying between various GPU\nmemory spaces. All of the layout tensor copy methods respect the layouts—so you\ncan transform a tensor by copying it to a tensor with a different layout.\n\n`LayoutTensor` itself provides two methods for copying tensor data:\n\n* [`copy_from()`](/mojo/kernels/layout/layout_tensor/LayoutTensor/#copy_from)\n copies data from a source tensor to the current tensor, which may be in a\n different memory space.\n* [`copy_from_async()`](/mojo/kernels/layout/layout_tensor/LayoutTensor/#copy_from_async)\n is an optimized copy mechanism for asynchronously copying from GPU global\n memory to shared memory.", "position": 40, "token_count": 198, "has_code": false, "section_hierarchy": ["Copying tensors"], "metadata": {"chunk_id": "layout-tensors-040", "document_id": "layout-tensors", "position": 40, "token_count": 198, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Copying tensors"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#copying-tensors"}}
{"chunk_id": "layout-tensors-041", "document_id": "layout-tensors", "content": "Both of these methods copy the entire source tensor. To divide the copying work\namong multiple threads, you need to use `distribute()` to create thread-local\ntensor fragments, as described in\n[Partitioning a tensor across threads](#partitioning-a-tensor-across-threads).\n\nThe following code sample demonstrates using both copy methods to copy data to\nand from shared memory.\n\n```mojo\nfn copy_from_async_example():\n alias dtype = DType.float32\n alias rows = 128\n alias cols = 128\n alias block_size = 16\n alias num_row_blocks = rows // block_size\n alias num_col_blocks = cols // block_size\n alias input_layout = Layout.row_major(rows, cols)\n alias simd_width = 4", "position": 41, "token_count": 175, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-041", "document_id": "layout-tensors", "position": 41, "token_count": 175, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
{"chunk_id": "layout-tensors-042", "document_id": "layout-tensors", "content": "fn kernel(tensor: LayoutTensor[dtype, input_layout, MutAnyOrigin]):\n # extract a tile from the input tensor.\n var global_tile = tensor.tile[block_size, block_size](\n Int(block_idx.y), Int(block_idx.x)\n )\n alias tile_layout = Layout.row_major(block_size, block_size)\n var shared_tile = LayoutTensor[\n dtype,\n tile_layout,\n MutAnyOrigin,\n address_space = AddressSpace.SHARED,\n ].stack_allocation()\n\n # Create thread layouts for copying\n alias thread_layout = Layout.row_major(\n WARP_SIZE // simd_width, simd_width\n )\n var global_fragment = global_tile.vectorize[\n 1, simd_width\n ]().distribute[thread_layout](lane_id())\n var shared_fragment = shared_tile.vectorize[\n 1, simd_width\n ]().distribute[thread_layout](lane_id())", "position": 42, "token_count": 234, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-042", "document_id": "layout-tensors", "position": 42, "token_count": 234, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
{"chunk_id": "layout-tensors-043", "document_id": "layout-tensors", "content": "shared_fragment.copy_from_async(global_fragment)\n @parameter\n if is_nvidia_gpu():\n async_copy_wait_all()\n barrier()\n\n # Put some data into the shared tile that we can verify on the host.\n if global_idx.y < rows and global_idx.x < cols:\n shared_tile[thread_idx.y, thread_idx.x] =\n shared_tile[thread_idx.y, thread_idx.x] + 1\n\n global_fragment.copy_from(shared_fragment)", "position": 43, "token_count": 133, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-043", "document_id": "layout-tensors", "position": 43, "token_count": 133, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
{"chunk_id": "layout-tensors-044", "document_id": "layout-tensors", "content": "try:\n var ctx = DeviceContext()\n var host_buf = ctx.enqueue_create_host_buffer[dtype](rows * cols)\n var dev_buf = ctx.enqueue_create_buffer[dtype](rows * cols)\n for i in range(rows * cols):\n host_buf[i] = i\n var tensor = LayoutTensor[dtype, input_layout](dev_buf)\n ctx.enqueue_copy(dev_buf, host_buf)\n ctx.enqueue_function_checked[kernel, kernel](\n tensor,\n grid_dim=(num_row_blocks, num_col_blocks),\n block_dim=(block_size, block_size),\n )\n ctx.enqueue_copy(host_buf, dev_buf)", "position": 44, "token_count": 198, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-044", "document_id": "layout-tensors", "position": 44, "token_count": 198, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
{"chunk_id": "layout-tensors-045", "document_id": "layout-tensors", "content": "block_dim=(block_size, block_size),\n )\n ctx.enqueue_copy(host_buf, dev_buf)\n ctx.synchronize()\n for i in range(rows * cols):\n if host_buf[i] != i + 1:\n raise Error(\n String(\"Unexpected value \", host_buf[i], \" at position \", i)\n )\n except error:\n print(error)", "position": 45, "token_count": 103, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-045", "document_id": "layout-tensors", "position": 45, "token_count": 103, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
{"chunk_id": "layout-tensors-046", "document_id": "layout-tensors", "content": "```", "position": 46, "token_count": 5, "has_code": true, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-046", "document_id": "layout-tensors", "position": 46, "token_count": 5, "has_code": true, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
{"chunk_id": "layout-tensors-047", "document_id": "layout-tensors", "content": "### Thread-aware copy functions\n\nThe [`layout_tensor` package](/mojo/kernels/layout/layout_tensor/) also includes\na number of specialized copy functions for different scenarios, such as copying\nfrom shared memory to local memory. These functions are all *thread-aware*:\ninstead of passing in tensor fragments, you pass in a thread layout which the\nfunction uses to partition the work.\n\nAs with the `copy_from()` and `copy_from_async()` methods, use the `vectorize()`\nmethod prior to copying to take advantage of vectorized copy operations.\n\nMany of the thread-aware copy functions have very specific requirements for the\nshape of the copied tensor and thread layout, based on the specific GPU and data\ntype in use.", "position": 47, "token_count": 167, "has_code": false, "section_hierarchy": ["Thread-aware copy functions"], "metadata": {"chunk_id": "layout-tensors-047", "document_id": "layout-tensors", "position": 47, "token_count": 167, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Thread-aware copy functions"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#thread-aware-copy-functions"}}
{"chunk_id": "layout-tensors-048", "document_id": "layout-tensors", "content": "## Summary\n\nIn this document, we've explored the fundamental concepts and practical usage of\n`LayoutTensor`. At its core, `LayoutTensor` provides\na powerful abstraction for working with multi-dimensional data.\nBy combining a layout (which defines memory organization), a data type, and a\nmemory pointer, `LayoutTensor` enables flexible and efficient data manipulation\nwithout unnecessary copying of the underlying data.\n\nWe covered several essential tensor operations that form the\nfoundation of working with `LayoutTensor`, including creating tensors,\naccessing tensor elements, and copying data between tensors.\n\nWe also covered key patterns for optimizing data access:\n\n- Tiling tensors for data locality. Accessing tensors one tile at a time can\n improve cache efficiency. On the GPU, tiling can allow the threads of a\n thread block to share high-speed access to a subset of a tensor.\n- Vectorizing tensors for more efficient data loads and stores.\n- Partitioning or distributing tensors into thread-local fragments for\n processing.\n\nThese patterns provide the building blocks for writing efficient kernels in Mojo\nwhile maintaining clean, readable code.", "position": 48, "token_count": 234, "has_code": false, "section_hierarchy": ["Summary"], "metadata": {"chunk_id": "layout-tensors-048", "document_id": "layout-tensors", "position": 48, "token_count": 234, "has_code": false, "overlap_with_previous": true, "section_hierarchy": ["Summary"], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors#summary"}}
{"chunk_id": "layout-tensors-049", "document_id": "layout-tensors", "content": "These patterns provide the building blocks for writing efficient kernels in Mojo\nwhile maintaining clean, readable code.\n\nTo see some practical examples of `LayoutTensor` in use, see [Optimize custom\nops for GPUs with Mojo ](/max/tutorials/custom-ops-matmul/).", "position": 49, "token_count": 67, "has_code": false, "section_hierarchy": [], "metadata": {"chunk_id": "layout-tensors-049", "document_id": "layout-tensors", "position": 49, "token_count": 67, "has_code": false, "overlap_with_previous": true, "section_hierarchy": [], "file_path": "layout/tensors.mdx", "url": "https://docs.modular.com/mojo/manual/layout/tensors", "title": "Using LayoutTensor", "category": null, "tags": [], "section_url": "https://docs.modular.com/mojo/manual/layout/tensors"}}
